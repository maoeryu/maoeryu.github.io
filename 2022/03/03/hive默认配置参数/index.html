<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maoeryu.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","width":200,"display":"post","padding":7,"offset":5},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="hive版本为2.1.1,默认配置在bin安装包conf下.">
<meta property="og:type" content="article">
<meta property="og:title" content="hive默认配置参数">
<meta property="og:url" content="https://maoeryu.github.io/2022/03/03/hive%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/index.html">
<meta property="og:site_name" content="FlyingPig">
<meta property="og:description" content="hive版本为2.1.1,默认配置在bin安装包conf下.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-03-02T16:00:00.000Z">
<meta property="article:modified_time" content="2022-11-15T09:28:34.557Z">
<meta property="article:author" content="maoeryu">
<meta property="article:tag" content="hive">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://maoeryu.github.io/2022/03/03/hive%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>hive默认配置参数 | FlyingPig</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlyingPig</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#datanucleus"><span class="nav-number">1.</span> <span class="nav-text">datanucleus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#common"><span class="nav-number">2.</span> <span class="nav-text">common</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cbo"><span class="nav-number">3.</span> <span class="nav-text">cbo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cli"><span class="nav-number">4.</span> <span class="nav-text">cli</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compactor"><span class="nav-number">5.</span> <span class="nav-text">compactor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exec"><span class="nav-number">6.</span> <span class="nav-text">exec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase"><span class="nav-number">7.</span> <span class="nav-text">hbase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hwi"><span class="nav-number">8.</span> <span class="nav-text">hwi</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#index"><span class="nav-number">9.</span> <span class="nav-text">index</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#llap"><span class="nav-number">10.</span> <span class="nav-text">llap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lock"><span class="nav-number">11.</span> <span class="nav-text">lock</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#log"><span class="nav-number">12.</span> <span class="nav-text">log</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#map"><span class="nav-number">13.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#merge"><span class="nav-number">14.</span> <span class="nav-text">merge</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#metastore"><span class="nav-number">15.</span> <span class="nav-text">metastore</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimize"><span class="nav-number">16.</span> <span class="nav-text">optimize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#orc"><span class="nav-number">17.</span> <span class="nav-text">orc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#query"><span class="nav-number">18.</span> <span class="nav-text">query</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#script"><span class="nav-number">19.</span> <span class="nav-text">script</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#security"><span class="nav-number">20.</span> <span class="nav-text">security</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#server2"><span class="nav-number">21.</span> <span class="nav-text">server2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#service"><span class="nav-number">22.</span> <span class="nav-text">service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark"><span class="nav-number">23.</span> <span class="nav-text">spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stats"><span class="nav-number">24.</span> <span class="nav-text">stats</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#test"><span class="nav-number">25.</span> <span class="nav-text">test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tez"><span class="nav-number">26.</span> <span class="nav-text">tez</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#txn"><span class="nav-number">27.</span> <span class="nav-text">txn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vectorized"><span class="nav-number">28.</span> <span class="nav-text">vectorized</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zookeeper"><span class="nav-number">29.</span> <span class="nav-text">zookeeper</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#jdo"><span class="nav-number">30.</span> <span class="nav-text">jdo</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">maoeryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">221</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://maoeryu.github.io/2022/03/03/hive%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="maoeryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlyingPig">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          hive默认配置参数
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-03 00:00:00" itemprop="dateCreated datePublished" datetime="2022-03-03T00:00:00+08:00">2022-03-03</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-11-15 17:28:34" itemprop="dateModified" datetime="2022-11-15T17:28:34+08:00">2022-11-15</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>hive版本为2.1.1,默认配置在bin安装包conf下.</p>
<span id="more"></span>
<h3 id="datanucleus"><a href="#datanucleus" class="headerlink" title="datanucleus"></a>datanucleus</h3><p>datanucleus.cache.level2<br>FALSE<br>使用 2 级缓存.如果元数据独立于 Hive 元存储服务器更改,请关闭此功能</p>
<p>datanucleus.cache.level2.type<br>none<br>无</p>
<p>datanucleus.connectionPoolingType<br>BONECP<br>为 datanucleus 指定连接池库</p>
<p>datanucleus.identifierFactory<br>datanucleus1<br>生成表/列名称等时使用的标识符工厂的名称.&#39;datanucleus1&#39;用于向后兼容 DataNucleus v1</p>
<p>datanucleus.plugin.pluginRegistryBundleCheck<br>LOG<br>定义发现插件包并重复时会发生什么 <code>[EXCEPTION|LOG|NONE]</code></p>
<p>datanucleus.rdbms.initializeColumnInfo<br>NONE<br>DataNucleus 的 initializeColumnInfo 设置;至少在 Postgres 上设置为 NONE.</p>
<p>datanucleus.rdbms.useLegacyNativeValueStrategy<br>TRUE<br>无</p>
<p>datanucleus.schema.autoCreateAll<br>FALSE<br>如果不存在,则自动在启动时创建必要的架构.在创建一次后将其设置为 false.要启用自动创建,还要设置 hive.metastore.schema.verification=false.不建议将自动创建用于生产用例,而是运行 schematool 命令.</p>
<p>datanucleus.schema.validateColumns<br>FALSE<br>根据代码验证现有模式.如果要验证现有架构,请打开此选项</p>
<p>datanucleus.schema.validateConstraints<br>FALSE<br>根据代码验证现有模式.如果要验证现有架构,请打开此选项</p>
<p>datanucleus.schema.validateTables<br>FALSE<br>根据代码验证现有模式.如果要验证现有架构,请打开此选项</p>
<p>datanucleus.storeManagerType<br>rdbms<br>元数据存储类型</p>
<p>datanucleus.transactionIsolation<br>read-committed<br>身份生成的默认事务隔离级别.</p>
<h3 id="common"><a href="#common" class="headerlink" title="common"></a>common</h3><p>fs.har.impl<br>org.apache.hadoop.hive.shims.HiveHarFileSystem<br>访问 Hadoop 档案的实现.请注意,这不适用于低于 0.20 的 Hadoop 版本</p>
<p>hive.added.archives.path<br>未配置<br>这是一个内部参数.</p>
<p>hive.added.files.path<br>未配置<br>这是一个内部参数.</p>
<p>hive.added.jars.path<br>未配置<br>这是一个内部参数.</p>
<p>hive.alias<br>未配置<br>无</p>
<p>hive.allow.udf.load.on.demand<br>FALSE<br>是否启用按需从 Metastore 加载 UDF;这主要与 HS2 相关,并且是 Hive 1.2 之前的默认行为.默认关闭.</p>
<p>hive.analyze.stmt.collect.partlevel.stats<br>TRUE<br>分析表 T 计算列的统计信息.即使没有指定部分规范,此类查询也应计算分区表的分区级别统计信息.</p>
<p><font color="#dd0000">hive.archive.enabled</font><br>FALSE<br>是否允许归档操作</p>
<p>hive.async.log.enabled<br>TRUE<br>是否开启 Log4j2 的异步日志.异步日志记录可以显着提高性能,因为日志记录将在使用 LMAX 中断队列缓冲日志消息的单独线程中处理.请参阅 <a target="_blank" rel="noopener" href="https://logging.apache.org/log4j/2.x/manual/async.html">https://logging.apache.org/log4j/2.x/manual/async.html</a> 以获得好处和缺点.</p>
<p><font color="#dd0000">hive.auto.convert.join</font><br>TRUE<br>Hive 是否开启了基于输入文件大小将 common join 转为 mapjoin 的优化</p>
<p>hive.auto.convert.join.noconditionaltask<br>TRUE<br>Hive是否开启根据输入文件大小将common join转为mapjoin的优化.如果开启此参数,n-way join的n-1个表/分区的size之和小于指定大小,join直接转换为mapjoin(没有条件任务).</p>
<p>hive.auto.convert.join.noconditionaltask.size<br>10000000<br>如果 hive.auto.convert.join.noconditionaltask 关闭,则此参数不生效.但是,如果打开,并且 n-way join 的 n-1 个表/分区的大小之和小于这个大小,join直接转换为mapjoin(没有条件任务).默认为 10MB</p>
<p>hive.auto.convert.join.use.nonstaged<br>FALSE<br>对于条件连接,如果来自小别名的输入流可以直接应用于连接算子而不需要过滤或投影,则别名不需要通过映射本地任务在分布式缓存中预先暂存.目前,这不适用于向量化或 tez 执行引擎.</p>
<p>hive.auto.convert.sortmerge.join<br>FALSE<br>如果连接的表通过了排序合并连接的条件,连接是否会自动转换为排序合并连接.</p>
<p>hive.auto.convert.sortmerge.join.bigtable.selection.policy<br>org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ<br>选择大表自动转换为排序合并连接的策略.默认情况下,分区最大的表被分配为大表.所有政策是:</p>
<p>hive.auto.convert.sortmerge.join.to.mapjoin<br>FALSE<br>如果 hive.auto.convert.sortmerge.join 设置为 true,并且将 join 转换为 sort-merge join,则此参数决定是否应尝试将每个表视为大表,并且应尝试有效地尝试 map-join . 这将创建一个有 n+1 个子节点的条件任务,用于 n 路连接(每个表有 1 个子节点作为大表),备份任务将是排序合并连接.在某些情况下,如果对输出进行分桶和排序没有优势,则 map-join 会比 sort-merge join 更快.例如,如果一个非常大的排序和分桶表只有很少的文件(比如 10 个文件)正在与一个非常小的排序器和分桶表连接只有很少的文件(10 个文件),排序合并连接将只使用 10 个映射器,并且如果完整的小表可以容纳在内存中,那么简单的 map-only join 可能会更快,并且可以执行 map-join.</p>
<p>hive.auto.progress.timeout<br>0s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.脚本/UDTF 操作符.永远设置为 0.</p>
<p>hive.autogen.columnalias.prefix.includefuncname<br>FALSE<br>是否在 Hive 自动生成的列别名中包含函数名称.</p>
<p>hive.autogen.columnalias.prefix.label<br><code>_c</code><br>自动生成列别名时用作前缀的字符串.默认情况下,前缀标签将附加列位置编号以形成列别名.如果在没有显式别名的选择子句中使用聚合函数,则会发生自动生成.</p>
<p><font color="#dd0000">hive.aux.jars.path</font><br>未配置<br>包含用户定义函数和 serdes 实现的插件 jar 的位置.</p>
<p>hive.binary.record.max.length<br>1000<br>从二进制流中读取并将每个 hive.binary.record.max.length 字节视为一条记录.流结束之前的最后一条记录可以少于 hive.binary.record.max.length 字节</p>
<p>hive.cache.expr.evaluation<br>TRUE<br>如果为真,则引用两次或多次的确定性表达式的评估结果将被缓存.例如,在像 &#39;.. where key + 10 = 100 or key + 10 = 0&#39; 这样的过滤条件中,表达式 &#39;key + 10&#39; 将被评估/缓存一次并重复用于以下表达式(&#39;key + 10 = 0&#39;).目前,这仅适用于选择或过滤运算符中的表达式.</p>
<h3 id="cbo"><a href="#cbo" class="headerlink" title="cbo"></a>cbo</h3><p>hive.cbo.cnf.maxnodes<br>-1<br>转换为合取范式 (CNF) 时,如果表达式超过此阈值,则失败;阈值以节点数(叶子和内部节点)表示.-1 不设置阈值.</p>
<p>hive.cbo.costmodel.cpu<br>0.000001<br>比较的默认成本</p>
<p>hive.cbo.costmodel.extended<br>FALSE<br>用于控制启用基于 CPU/IO 和基数的扩展成本模型的标志.否则,成本模型基于基数.</p>
<p>hive.cbo.costmodel.hdfs.read<br>1.5<br>从 HDFS 读取一个字节的默认成本;表示为本地 FS 读取成本的倍数</p>
<p>hive.cbo.costmodel.hdfs.write<br>10<br>将字节写入 HDFS 的默认成本;表示为本地 FS 写入成本的倍数</p>
<p>hive.cbo.costmodel.local.fs.read<br>4<br>从本地 FS 读取一个字节的默认成本;表示为网络成本的倍数</p>
<p>hive.cbo.costmodel.local.fs.write<br>4<br>将字节写入本地 FS 的默认成本;表示为网络成本的倍数</p>
<p>hive.cbo.costmodel.network<br>150<br>通过网络传输字节的默认成本;表示为 CPU 成本的倍数</p>
<p>hive.cbo.enable<br>TRUE<br>使用 Calcite 框架控制启用基于成本的优化的标志.</p>
<p>hive.cbo.returnpath.hiveop<br>FALSE<br>控制方解石计划到蜂巢运算符转换的标志</p>
<h3 id="cli"><a href="#cli" class="headerlink" title="cli"></a>cli</h3><p>hive.cli.errors.ignore<br>FALSE<br>无</p>
<p>hive.cli.pretty.output.num.cols<br>-1<br>格式化由 DESCRIBE PRETTY table_name 命令生成的输出时要使用的列数.如果此属性的值为 -1,则 Hive 将使用自动检测到的终端宽度.</p>
<p><font color="#dd0000">hive.cli.print.current.db</font><br>FALSE<br>是否在 Hive 提示中包含当前数据库.</p>
<p><font color="#dd0000">hive.cli.print.header</font><br>FALSE<br>是否打印查询输出中列的名称.</p>
<p>hive.cli.prompt<br>hive<br>命令行提示配置值.此配置值中可以使用其他 hiveconf.变量替换只会在 Hive CLI 启动时调用.</p>
<p>hive.cli.tez.session.async<br>TRUE<br>使用 Tez 运行 CLI 时是否在后台启动 Tez 会话,允许 CLI 更早可用.</p>
<p>hive.client.stats.counters<br>未配置<br>hive.client.stats.publishers 应该感兴趣的计数器子集(当人们想要限制他们的发布时).应该使用非显示名称</p>
<p>hive.client.stats.publishers<br>未配置<br>要在每个作业的计数器上调用的统计信息发布者的逗号分隔列表.客户端统计发布者被指定为实现 org.apache.hadoop.hive.ql.stats.ClientStatsPublisher 接口的 Java 类的名称.</p>
<p>hive.cluster.delegation.token.store.class<br>org.apache.hadoop.hive.thrift.MemoryTokenStore<br>委托令牌存储实现.为负载均衡集群设置为 org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.</p>
<p>hive.cluster.delegation.token.store.zookeeper.acl<br>未配置<br>令牌存储条目的 ACL.逗号分隔的 ACL 条目列表.例如:sasl:hive/host1@MY.DOMAIN:cdrwa,sasl:hive/host2@MY.DOMAIN:cdrwa 默认为 hiveserver2/metastore 进程用户的所有权限.</p>
<p>hive.cluster.delegation.token.store.zookeeper.connectString<br>未配置<br>ZooKeeper 令牌存储连接字符串.您可以通过不设置此参数来重新使用 hive.zookeeper.quorum 中设置的配置值.</p>
<p>hive.cluster.delegation.token.store.zookeeper.znode<br>/hivedelegation<br>令牌存储数据的根路径.请注意,HiveServer2 和 MetaStore 都使用它来存储委托 Token.为每个目录创建一个目录.最终目录名称将附加服务器名称(HIVESERVER2,METASTORE).</p>
<h3 id="compactor"><a href="#compactor" class="headerlink" title="compactor"></a>compactor</h3><p>hive.compactor.abortedtxn.threshold<br>1000<br>涉及将触发主要压缩的给定表或分区的中止事务数.</p>
<p>hive.compactor.check.interval<br>300s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.检查之间的时间以秒为单位如果需要压缩任何表或分区.这应该保持较高,因为每次检查压缩都需要对 NameNode 进行多次调用.减小此值将减少为需要压缩的表或分区启动压缩所需的时间.但是,检查是否需要压缩需要对自上次主要压缩以来已在其上完成事务的每个表或分区多次调用 NameNode.所以减小这个值会增加 NameNode 上的负载.</p>
<p>hive.compactor.cleaner.run.interval<br>5000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.清洁线程运行之间的时间</p>
<p>hive.compactor.delta.num.threshold<br>10<br>表或分区中将触发次要压缩的增量目录数.</p>
<p>hive.compactor.delta.pct.threshold<br>0.1<br>增量文件相对于将触发主要压缩的基础的百分比(小数)大小.(1.0 = 100%,所以默认 0.1 = 10%.)</p>
<p>hive.compactor.history.reaper.interval<br>2m<br>期望一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.确定压缩历史收割机运行的频率</p>
<p>hive.compactor.history.retention.attempted<br>2<br>期望值介于 0 和 100 之间.确定给定表/分区的压缩历史记录中将保留多少尝试压缩记录.</p>
<p>hive.compactor.history.retention.failed<br>3<br>期望值介于 0 和 100 之间.确定给定表/分区的压缩历史记录中将保留多少失败的压缩记录.</p>
<p>hive.compactor.history.retention.succeeded<br>3<br>期望值介于 0 和 100 之间.确定给定表/分区的压缩历史记录中将保留多少成功的压缩记录.</p>
<p>hive.compactor.initiator.failed.compacts.threshold<br>2<br>期望值在 1 到 20 之间.连续压缩失败的次数(每个表/分区),之后将不再安排自动压缩.请注意,这必须小于 hive.compactor.history.retention.failed.</p>
<p>hive.compactor.initiator.on<br>FALSE<br>是否在此 Metastore 实例上运行启动器和清理线程.在 Thrift Metastore 服务的一个实例上将此设置为 true,作为打开 Hive 事务的一部分.有关开启事务所需参数的完整列表,请参阅 hive.txn.manager.</p>
<p>hive.compactor.job.queue<br>未配置<br>用于指定将提交 Compaction 作业的 Hadoop 队列的名称.设置为空字符串让 Hadoop 选择队列.</p>
<p>hive.compactor.max.num.delta<br>500<br>压缩器将尝试在单个作业中处理的最大增量文件数.</p>
<p>hive.compactor.worker.threads<br>0<br>在此 Metastore 实例上运行多少个压缩器工作线程.作为打开 Hive 事务的一部分,在 Thrift 元存储服务的一个或多个实例上将此设置为正数.有关打开事务所需参数的完整列表,请参阅 hive.txn.manager.Worker 线程生成 MapReduce 作业以执行压缩.他们自己不进行压缩.一旦确定需要压缩,增加工作线程的数量将减少压缩表或分区所需的时间.它还将增加 Hadoop 集群的后台负载,因为更多的 MapReduce 作业将在后台运行.</p>
<p>hive.compactor.worker.timeout<br>86400s<br>需要一个时间值,单位为 (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 sec.压缩后的时间以秒为单位作业将被宣布失败并且压缩重新排队.</p>
<p>hive.compat<br>0.12<br>通过设置所需的向后兼容性级别来启用(可配置的)弃用行为.设置为 0.12:</p>
<p><font color="#dd0000">hive.compute.query.using.stats</font><br>FALSE<br>当设置为 true 时,Hive 将纯粹使用存储在 Metastore 中的统计信息来回答一些查询,例如 count(1).对于基本统计数据收集,请将配置 hive.stats.autogather 设置为 true.对于更高级的统计数据收集,需要运行分析表查询.</p>
<p>hive.compute.splits.in.am<br>TRUE<br>是在本地还是在 AM 中生成拆分(仅限 tez)</p>
<p>hive.conf.hidden.list<br>javax.jdo.option.ConnectionPassword,hive.server2.keystore.password<br>逗号分隔的配置选项列表,普通用户不应读取,如密码</p>
<p>hive.conf.internal.variable.list<br>hive.added.files.path,hive.added.jars.path,hive.added.archives.path<br>逗号分隔的变量列表,这些变量在内部使用且不可配置.</p>
<p>hive.conf.restricted.list<br>hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role,hive.server2.xsrf.filter.enabled<br>逗号分隔的配置选项列表,在运行时不可变</p>
<p>hive.conf.validation<br>TRUE<br>启用已注册 Hive 配置的类型检查</p>
<p>hive.convert.join.bucket.mapjoin.tez<br>FALSE<br>使用 tez 作为执行引擎时,hive 中的 join 是否可以自动转换为 bucket map 的 join.</p>
<p>hive.count.open.txns.interval<br>1s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.检查之间的时间以秒为单位开放交易.</p>
<p>hive.counters.group.name<br>HIVE<br>内部 Hive 变量的计数器组的名称(CREATED_FILE/FATAL_ERROR 等)</p>
<p>hive.ddl.createtablelike.properties.whitelist<br>未配置<br>执行 Create Table Like 时要复制的表属性.</p>
<p>hive.ddl.output.format<br>未配置<br>用于 DDL 输出的数据格式.&quot;文本&quot;(用于人类可读文本)或&quot;json&quot;(用于 json 对象)之一.</p>
<p>hive.debug.localtask<br>FALSE<br>无</p>
<p>hive.decode.partition.name<br>FALSE<br>是否在查询结果中显示不带引号的分区名称.</p>
<p><font color="#dd0000">hive.default.fileformat</font><br>TextFile<br>期望<code>[textfile, sequencefile, rcfile, orc]</code>之一. CREATE TABLE 语句的默认文件格式.用户可以通过 <code>CREATE TABLE ... STORED AS [FORMAT]</code> 显式覆盖它</p>
<p>hive.default.fileformat.managed<br>none<br>期望<code> [none, textfile, sequencefile, rcfile, orc]</code> 之一.仅适用于托管表的 CREATE TABLE 语句的默认文件格式.将使用 hive.default.fileformat 指定的格式创建外部表.保留此 null 将导致对所有表使用 hive.default.fileformat.</p>
<p>hive.default.rcfile.serde<br>org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe<br>默认 SerDe Hive 将用于 RCFile 格式</p>
<p><font color="#dd0000">hive.default.serde</font><br>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>默认 SerDe Hive 将用于未指定 SerDe 的存储格式.</p>
<p>hive.direct.sql.max.elements.in.clause<br>1000<br>IN 子句中值的最大数量.一旦超过,它将被分成多个 OR 分隔的 IN 子句.</p>
<p>hive.direct.sql.max.elements.values.clause<br>1000<br>INSERT 语句的 VALUES 子句中值的最大数量.</p>
<p>hive.direct.sql.max.query.length<br>100<br>查询字符串的最大大小(以 KB 为单位).</p>
<p>hive.display.partition.cols.separately<br>TRUE<br>在较旧的 Hive 版本(0.10 和更早版本)中,在描述表中显示列时,分区列或非分区列之间没有区别.从 0.12 开始,它们分别显示.如果需要,此标志将使您获得旧行为.请参阅 HIVE-6689 补丁中的测试用例.</p>
<p>hive.downloaded.resources.dir<br><code>$&#123;system:java.io.tmpdir&#125;/$&#123;hive.session.id&#125;_resources</code><br>远程文件系统中添加资源的临时本地目录.</p>
<p>hive.driver.parallel.compilation<br>FALSE<br>是否在 HiveServer2 上的会话之间启用并行编译.默认值为假.</p>
<p>hive.enforce.bucketmapjoin<br>FALSE<br>如果用户要求bucketed map-side join,但无法执行,查询是否应该失败?例如,如果正在连接的表中的桶不是彼此的倍数,则无法执行桶式 map-side join,如果 hive.enforce.bucketmapjoin 设置为 true,则查询将失败.</p>
<p>hive.enforce.sortmergebucketmapjoin<br>FALSE<br>如果用户要求 sort-merge bucketed map-side join,但无法执行,查询是否会失败?</p>
<p>hive.entity.capture.transform<br>FALSE<br>用于捕获查询中引用的转换 URI 的编译器</p>
<p>hive.entity.separator<br>@<br>用于构造表和分区名称的分隔符.例如,dbname@tablename@partitionname</p>
<p>hive.error.on.empty.partition<br>FALSE<br>如果动态分区插入产生空结果是否抛出异常.</p>
<h3 id="exec"><a href="#exec" class="headerlink" title="exec"></a>exec</h3><p>hive.exec.check.crossproducts<br>TRUE<br>检查计划是否包含交叉产品.如果有,则向会话的控制台输出警告.</p>
<p><font color="#dd0000">hive.exec.compress.intermediate</font><br>FALSE<br>这控制 Hive 在多个 map-reduce 作业之间生成的中间文件是否被压缩.压缩编解码器和其他选项由 Hadoop 配置变量 mapred.output.compress* 确定</p>
<p><font color="#dd0000">hive.exec.compress.output</font><br>FALSE<br>这控制是否压缩查询的最终输出(到本地/HDFS 文件或 Hive 表).压缩编解码器和其他选项由 Hadoop 配置变量 mapred.output.compress* 确定</p>
<p>hive.exec.concatenate.check.index<br>TRUE<br>如果将其设置为 true,则 Hive 在对具有索引的表/分区执行&quot;alter table tbl_name [partSpec] concatenate&quot;时将引发错误.用户想要将此设置为 true 的原因是因为它可以帮助用户避免处理所有索引删除/重新创建/重建工作.这对于具有数千个分区的表非常有用.</p>
<p>hive.exec.copyfile.maxsize<br>33554432<br>Hive 用于在目录之间执行单个 HDFS 副本的最大文件大小(以 Mb 为单位).分布式副本 (distcp) 将用于更大的文件,以便可以更快地完成副本.</p>
<p>hive.exec.counters.pull.interval<br>1000<br>轮询 JobTracker 以获取正在运行的作业的计数器的间隔.它越小,jobtracker 上的负载就越大,它越高,捕获的粒度就越小.</p>
<p>hive.exec.default.partition.name<br><code>__HIVE_DEFAULT_PARTITION__</code><br>如果动态分区列值为空/空字符串或任何其他无法转义的值,则默认分区名称.此值不得包含 HDFS URI 中使用的任何特殊字符(例如,&#39;:&#39;/&#39;%&#39;/&#39;/ &#39; 等).用户必须知道动态分区值不应包含此值以避免混淆.</p>
<p>hive.exec.driver.run.hooks<br>未配置<br>实现 HiveDriverRunHook 的钩子的逗号分隔列表.将在 Driver.run 的开头和结尾运行,这些将按照指定的顺序运行.</p>
<p>hive.exec.drop.ignorenonexistent<br>TRUE<br>如果 DROP TABLE/VIEW/Index/Function 指定了不存在的表/视图/索引/函数不报错</p>
<p><font color="#dd0000">hive.exec.dynamic.partition</font><br>TRUE<br>是否允许在 DML/DDL 中进行动态分区.</p>
<p><font color="#dd0000">hive.exec.dynamic.partition.mode</font><br>strict<br>在严格模式下,用户必须至少指定一个静态分区,以防用户不小心覆盖了所有分区.在非严格模式下,所有分区都允许是动态的.</p>
<p>hive.exec.failure.hooks<br>未配置<br>要为每个语句调用的失败钩子的逗号分隔列表.一个 on-failure 挂钩被指定为实现 org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext 接口的 Java 类的名称.</p>
<p>hive.exec.infer.bucket.sort<br>FALSE<br>如果设置了此项,则在写入分区时,元数据将包括写入数据的分桶/排序属性(如果有的话,这不会覆盖从表继承的元数据)</p>
<p>hive.exec.infer.bucket.sort.num.buckets.power.two<br>FALSE<br>如果设置了此项,则在为写入最终输出文件的 map reduce 任务设置 reducer 的数量时,它将选择一个 2 的幂的数字,除非用户使用 mapred.reduce 指定要使用的 reducer 的数量.任务.reducer 的数量可以设置为 2 的幂,后面跟着一个合并任务,意思是防止任何东西被推断出来.如果 hive.exec.infer.bucket.sort 设置为 true:Advantages:如果未设置,分区的桶数似乎是任意的,这意味着例如用于优化连接的映射器数量将非常低.有了这个集合,由于用于任何分区的桶数是 2 的幂,用于优化连接的映射器的数量将是被连接的任何分区使用的最少桶数.缺点:</p>
<p>hive.exec.job.debug.capture.stacktraces<br>TRUE<br>从每个失败作业的采样失败任务的任务日志中解析的堆栈跟踪是否应存储在 SessionState 中</p>
<p>hive.exec.job.debug.timeout<br>30000<br>无</p>
<p><font color="#dd0000">hive.exec.local.scratchdir</font><br><code>$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;</code><br>Hive 作业的本地暂存空间</p>
<p>hive.exec.log4j.file<br>未配置<br>用于执行模式(子命令)的 Hive log4j 配置文件.如果未设置该属性,则将使用类路径上的 hive-exec-log4j2.properties 初始化日志记录.如果设置了该属性,则该值必须是有效的 URI (java.net.URI,例如&quot;file:///tmp/my-logging.xml&quot;),然后您可以从中提取 URL 并将其传递给 PropertyConfigurator.configure(URL).</p>
<p>hive.exec.max.created.files<br>100000<br>MapReduce 作业中所有映射器/缩减器创建的 HDFS 文件的最大数量.</p>
<p>hive.exec.max.dynamic.partitions<br>1000<br>总共允许创建的最大动态分区数.</p>
<p><font color="#dd0000">hive.exec.max.dynamic.partitions.pernode</font><br>100<br>每个 mapper/reducer 节点允许创建的最大动态分区数.</p>
<p><font color="#dd0000">hive.exec.mode.local.auto</font><br>FALSE<br>让 Hive 自动判断是否以本地模式运行</p>
<p><font color="#dd0000">hive.exec.mode.local.auto.input.files.max</font><br>4<br>当 hive.exec.mode.local.auto 为 true 时,本地模式的任务数应少于此值.</p>
<p><font color="#dd0000">hive.exec.mode.local.auto.inputbytes.max</font><br>134217728<br>当 hive.exec.mode.local.auto 为 true 时,对于本地模式,输入字节数应小于此值.</p>
<p>hive.exec.orc.base.delta.ratio<br>8<br>基于 STRIPE_SIZE 和 BUFFER_SIZE 的基本写入器和增量写入器的比率.</p>
<p>hive.exec.orc.block.padding.tolerance<br>0.05<br>将块填充的容差定义为条带大小的小数部分(例如,默认值 0.05 是条带大小的 5%).对于 64Mb ORC 条带和 256Mb HDFS 块的默认值,5% 的默认块填充容差将为 256Mb 块内的填充保留最大 3.2Mb.在这种情况下,如果块内的可用大小超过 3.2Mb,则将插入一个新的较小的条带以适应该空间.这将确保写入的条带不会跨越块边界并导致节点本地任务中的远程读取.</p>
<p>hive.exec.orc.compression.strategy<br>SPEED<br>期望 <code>[速度,压缩]</code> 之一.定义写入数据时要使用的压缩策略.这会更改更高级别压缩编解码器(如 ZLIB)的压缩级别.</p>
<p>hive.exec.orc.default.block.padding<br>TRUE<br>定义默认块填充,将条带填充到 HDFS 块边界.</p>
<p><font color="#dd0000">hive.exec.orc.default.block.size</font><br>268435456<br>定义 ORC 文件的默认文件系统块大小.</p>
<p>hive.exec.orc.default.buffer.size<br>262144<br>定义默认的 ORC 缓冲区大小,以字节为单位.</p>
<p>hive.exec.orc.default.compress<br>ZLIB<br>为 ORC 文件定义默认压缩编解码器</p>
<p>hive.exec.orc.default.row.index.stride<br>10000<br>以行数定义默认的 ORC 索引步幅.(步幅是索引条目表示的行数.)</p>
<p>hive.exec.orc.default.stripe.size<br>67108864<br>定义默认的 ORC 条带大小,以字节为单位.</p>
<p>hive.exec.orc.dictionary.key.size.threshold<br>0.8<br>如果字典中的键数大于非空行总数的这一部分,请关闭字典编码.使用 1 始终使用字典编码.</p>
<p>hive.exec.orc.encoding.strategy<br>SPEED<br>期望 <code>[速度,压缩]</code> 之一.定义写入数据时要使用的编码策略.改变它只会影响整数的轻量级编码.此标志不会更改更高级别压缩编解码器(如 ZLIB)的压缩级别.</p>
<p>hive.exec.orc.memory.pool<br>0.5<br>ORC 文件写入器可以使用的堆的最大比例</p>
<p>hive.exec.orc.skip.corrupt.data<br>FALSE<br>如果 ORC reader 遇到损坏的数据,该值将用于确定是跳过损坏的数据还是抛出异常.默认行为是抛出异常.</p>
<p>hive.exec.orc.split.strategy<br>HYBRID<br>期望 <code>[hybrid, bi, etl]</code> 之一.这不是用户级配置.当要求在拆分生成而不是查询执行(拆分生成不读取或缓存文件页脚)上花费更少的时间时,使用 BI 策略.当可以接受在拆分生成中花费更多时间(拆分生成读取和缓存文件页脚)时,使用 ETL 策略.HYBRID 基于启发式在上述策略之间进行选择.</p>
<p>hive.exec.orc.write.format<br>未配置<br>定义要写入的文件的版本.可能的值为 0.11 和 0.12.如果未定义此参数,ORC 将使用 Hive 0.12 中引入的运行长度编码 (RLE).0.11 以外的任何值都会导致 0.12 编码.</p>
<p>hive.exec.orc.zerocopy<br>FALSE<br>对 ORC 使用 zerocopy 读取.(这需要 Hadoop 2.3 或更高版本.)</p>
<p><font color="#dd0000">hive.exec.parallel</font><br>FALSE<br>是否并行执行作业</p>
<p>hive.exec.parallel.thread.number<br>8<br>最多可以并行执行多少个作业</p>
<p>hive.exec.perf.logger<br>org.apache.hadoop.hive.ql.log.PerfLogger<br>负责记录客户端性能指标的类.必须是 org.apache.hadoop.hive.ql.log.PerfLogger 的子类</p>
<p>hive.exec.plan<br>未配置<br>无</p>
<p>hive.exec.post.hooks<br>未配置<br>要为每个语句调用的执行后挂钩的逗号分隔列表.执行后挂钩被指定为实现 org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext 接口的 Java 类的名称.</p>
<p>hive.exec.pre.hooks<br>未配置<br>要为每个语句调用的预执行挂钩的逗号分隔列表.预执行挂钩被指定为实现 org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext 接口的 Java 类的名称.</p>
<p>hive.exec.query.redactor.hooks<br>未配置<br>要为每个查询调用的以逗号分隔的钩子列表,这些钩子可以在将查询放入 job.xml 文件之前对其进行转换.必须是从 org.apache.hadoop.hive.ql.hooks.Redactor 抽象类扩展而来的 Java 类.</p>
<p>hive.exec.rcfile.use.explicit.header<br>TRUE<br>如果设置了 RCFiles 的标头将只是 RCF.如果未设置,则标头将从序列文件中借用,例如 SEQ- 后跟输入和输出 RCFile 格式.</p>
<p>hive.exec.rcfile.use.sync.cache<br>TRUE<br>无</p>
<p><font color="#dd0000">hive.exec.reducers.bytes.per.reducer</font><br>256000000<br>每个reducer的大小.默认为256Mb,即如果输入大小为1G,它将使用4个reducer.</p>
<p><font color="#dd0000">hive.exec.reducers.max</font><br>1009<br>将使用最大数量的减速器.如果配置参数 mapred.reduce.tasks 中指定的一个为负数,Hive 会在自动确定 reducer 数量时使用这个作为最大 reducer 数量.</p>
<p>hive.exec.rowoffset<br>FALSE<br>是否提供行偏移虚拟列</p>
<p>hive.exec.schema.evolution<br>TRUE<br>使用模式演化将自描述文件格式的数据转换为读者所需的模式.</p>
<p><font color="#dd0000">hive.exec.scratchdir</font><br>/tmp/hive<br>使用 write all (733) 权限创建的 Hive 作业的 HDFS 根暂存目录.对于每个连接的用户,一个 HDFS 暂存目录:<code>$&#123;hive.exec.scratchdir&#125;/&lt;username&gt;</code> 使用 <code>$&#123;hive.scratch.dir.permission&#125; </code>创建.</p>
<p>hive.exec.script.allow.partial.consumption<br>FALSE<br>启用后,此选项允许用户脚本成功退出,而不会消耗标准输入中的所有数据.</p>
<p>hive.exec.script.maxerrsize<br>100000<br>允许脚本向标准错误发出的最大字节数(每个 map-reduce 任务).这可以防止失控脚本将日志分区填充到容量</p>
<p>hive.exec.script.trust<br>FALSE<br>无</p>
<p>hive.exec.script.wrapper<br>未配置<br>无</p>
<p>hive.exec.show.job.failure.debug.info<br>TRUE<br>如果作业失败,是否在 CLI 中提供指向失败最多的任务的链接,以及调试提示(如果适用).</p>
<p><font color="#dd0000">hive.exec.stagingdir</font><br>.hive-staging<br>将在表位置内创建的目录名称,以支持 HDFS 加密.这将替换<code> $&#123;hive.exec.scratchdir&#125;</code> 的查询结果,只读表除外.在所有情况下,<code>$&#123;hive.exec.scratchdir&#125;</code> 仍用于其他临时文件,例如作业计划.</p>
<p>hive.exec.submit.local.task.via.child<br>TRUE<br>确定本地任务(通常是 mapjoin 哈希表生成阶段)是否在单独的 JVM 中运行(真正推荐).避免产生新 JVM 的开销,但可能导致内存不足问题.</p>
<p>hive.exec.submitviachild<br>FALSE<br>无</p>
<p>hive.exec.tasklog.debug.timeout<br>20000<br>无</p>
<p>hive.exec.temporary.table.storage<br>default<br>期望<code> [memory, ssd, default]</code> 之一.定义临时表的存储策略.在 memory/ssd 和 default 之间进行选择</p>
<p><font color="#dd0000">hive.execution.engine</font><br>mr<br>期望 <code>[mr, tez, spark]</code> 之一.选择执行引擎.选项有:mr(Map reduce,默认)/tez/spark.虽然 MR 由于历史原因仍然是默认引擎,但它本身就是一个历史引擎,在 Hive 2 行中已弃用.它可能会在没有进一步警告的情况下被删除.</p>
<p>hive.execution.mode<br>container<br>期望<code> [container, llap]</code> 之一.选择查询片段是在容器中还是在 llap 中运行</p>
<p>hive.exim.strict.repl.tables<br>TRUE<br>确定&quot;常规&quot;(非复制)导出转储是否可以导入到作为复制目标的表的参数.如果设置了此参数,则常规导入将检查目标表(如果存在)是否设置了&quot;repl.last.id&quot;.如果是这样,它将失败.</p>
<p>hive.exim.uri.scheme.whitelist<br>hdfs,pfile<br>用于导入和导出的可接受 URI 方案的逗号分隔列表.</p>
<p>hive.explain.dependency.append.tasktype<br>FALSE<br>无</p>
<p>hive.explain.user<br>TRUE<br>是否在用户级别显示解释结果.启用后,将在用户级别记录查询的 EXPLAIN 输出.</p>
<p>hive.fetch.output.serde<br>org.apache.hadoop.hive.serde2.DelimitedJSONSerDe<br>FetchTask 用于序列化提取输出的 SerDe.</p>
<p>hive.fetch.task.aggr<br>FALSE<br>没有 group-by 子句的聚合查询(例如,<code>select count(*) from src</code>)在单个 reduce 任务中执行最终聚合.如果设置为 true,Hive 会委托最终聚合阶段来获取任务,这可能会减少查询时间.</p>
<p><font color="#dd0000">hive.fetch.task.conversion</font><br>more<br>期望 <code>[none, minimum, more]</code> 之一.一些选择查询可以转换为单个 FETCH 任务,最大限度地减少延迟.目前查询应该是单一来源的,没有任何子查询,并且不应该有任何聚合或不同(这会导致 RS),横向视图和joins.0.none : 禁用 hive.fetch.task.conversion 1. minimum : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (支持 TABLESAMPLE 和虚拟列)</p>
<p>hive.fetch.task.conversion.threshold<br>1073741824<br>应用 hive.fetch.task.conversion 的输入阈值.如果目标表是本机的,则输入长度通过文件长度的总和计算.如果它不是本机的,则表的存储处理程序可以选择实现 org.apache.hadoop.hive.ql.metadata.InputEstimator 接口.</p>
<p>hive.file.max.footer<br>100<br>页脚用户可以为表格文件定义的最大行数</p>
<p>hive.fileformat.check<br>TRUE<br>加载数据文件时是否检查文件格式</p>
<p>hive.groupby.limit.extrastep<br>TRUE<br>此参数决定 Hive 是否应该创建新的 MR 作业以对最终输出进行排序</p>
<p>hive.groupby.mapaggr.checkinterval<br>100000<br>执行分组键/聚合类大小之后的行数</p>
<p>hive.groupby.orderby.position.alias<br>FALSE<br>是否启用在 Group By 或 Order By 中使用列位置别名</p>
<p><font color="#dd0000">hive.groupby.skewindata</font><br>FALSE<br>数据中是否存在倾斜以优化 group by 查询</p>
<p>hive.hadoop.classpath<br>未配置<br>对于 Windows 操作系统,我们需要在使用&quot;-hiveconf hive.hadoop.classpath=%HIVE_LIB%&quot;启动 HiveServer2 时传递 HIVE_HADOOP_CLASSPATH Java 参数.</p>
<p>hive.hash.table.inflation.factor<br>2<br>哈希表的磁盘/内存表示之间的预期膨胀因子</p>
<p>hive.hashtable.initialCapacity<br>100000<br>如果没有统计信息,或者如果 hive.hashtable.key.count.adjustment 设置为 0,则 mapjoin 哈希表的初始容量</p>
<p>hive.hashtable.key.count.adjustment<br>1<br>调整从表和列统计中导出的 mapjoin 哈希表大小;估计的键数除以这个值.如果值为 0,则不使用统计信息,而是使用 hive.hashtable.initialCapacity.</p>
<p>hive.hashtable.loadfactor<br>0.75<br>无</p>
<h3 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h3><p>hive.hbase.generatehfiles<br>FALSE<br>当 HBaseStorageHandler 应该生成 hfiles 而不是对在线表进行操作时为真.</p>
<p>hive.hbase.snapshot.name<br>未配置<br>要使用的 HBase 表快照名称.</p>
<p>hive.hbase.snapshot.restoredir<br>/tmp<br>恢复 HBase 表快照的目录.</p>
<p><font color="#dd0000">hive.hbase.wal.enabled</font><br>TRUE<br>是否应强制写入 HBase 到预写日志.禁用此功能可提高 HBase 写入性能,但有可能在崩溃时丢失写入.</p>
<p>hive.heartbeat.interval<br>1000<br>在此间隔之后发送心跳 - 由 mapjoin 和过滤器操作员使用</p>
<p>hive.hmshandler.force.reload.conf<br>FALSE<br>是否在下一个访问数据存储的元存储查询之前强制重新加载 HMSHandler 配置(包括连接 URL).一旦重新加载,此值将重置为 false.仅用于测试.</p>
<p>hive.hmshandler.retry.attempts<br>10<br>如果出现连接错误,重试 HMSHandler 调用的次数.</p>
<p>hive.hmshandler.retry.interval<br>2000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.HMSHandler 重试尝试之间的时间失败.</p>
<h3 id="hwi"><a href="#hwi" class="headerlink" title="hwi"></a>hwi</h3><p><font color="#dd0000">hive.hwi.listen.host</font><br>0.0.0.0<br>这是 Hive Web 界面将侦听的主机地址</p>
<p>hive.hwi.listen.port<br>9999<br>这是 Hive Web 界面将侦听的端口</p>
<p>hive.hwi.war.file<br>${env:HWI_WAR_FILE}<br>这将设置 HWI 战争文件的路径,相对于 ${HIVE_HOME}.</p>
<p>hive.ignore.mapjoin.hint<br>TRUE<br>忽略 mapjoin 提示</p>
<h3 id="index"><a href="#index" class="headerlink" title="index"></a>index</h3><p>hive.index.blockfilter.file<br>未配置<br>内部变量</p>
<p>hive.index.compact.binary.search<br>TRUE<br>在可能的情况下,是否使用二分查找来查找索引表中与过滤器匹配的条目</p>
<p>hive.index.compact.file<br>未配置<br>内部变量</p>
<p>hive.index.compact.file.ignore.hdfs<br>FALSE<br>当为 true 时,存储在索引文件中的 HDFS 位置将在运行时被忽略.如果数据被移动或集群名称被更改,索引数据应该仍然可用.</p>
<p>hive.index.compact.query.max.entries<br>10000000<br>在使用紧凑索引的查询期间要读取的最大索引条目数.负值相当于无穷大.</p>
<p>hive.index.compact.query.max.size<br>10737418240<br>使用紧凑索引的查询可以读取的最大字节数.负值相当于无穷大.</p>
<p><font color="#dd0000">hive.input.format</font><br>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat<br>默认输入格式.如果遇到 CombineHiveInputFormat 问题,请将其设置为 HiveInputFormat.</p>
<p>hive.insert.into.external.tables<br>TRUE<br>是否允许插入外部表</p>
<p>hive.insert.into.multilevel.dirs<br>FALSE<br>在哪里插入多级目录,例如&quot;从表中插入目录&#39;/HIVEFT25686/chinna/&#39;&quot;</p>
<p>hive.int.timestamp.conversion.in.seconds<br>FALSE<br>Boolean/tinyint/smallint/int/bigint 值在时间戳转换期间被解释为毫秒.将此标志设置为 true 以将值解释为秒以与 float/double 一致.</p>
<p>hive.intermediate.compression.codec<br>未配置<br>无</p>
<p>hive.intermediate.compression.type<br>未配置<br>无</p>
<p>hive.io.exception.handlers<br>未配置<br>io 异常处理程序类名称的列表.这用于构造一个列表异常处理程序来处理记录读取器抛出的异常</p>
<p>hive.io.rcfile.column.number.conf<br>0<br>无</p>
<p>hive.io.rcfile.record.buffer.size<br>4194304<br>无</p>
<p>hive.io.rcfile.record.interval<br>2147483647<br>无</p>
<p>hive.io.rcfile.tolerate.corruptions<br>FALSE<br>无</p>
<p>hive.jar.directory<br>未配置<br>这是 tez 模式下的位置 hive 将寻找以查找站点范围安装的 hive 实例.</p>
<p>hive.jar.path<br>未配置<br>在单独的 jvm 中提交作业时使用的 hive_cli.jar 的位置.</p>
<p>hive.jobname.length<br>50<br>最大作业名长度</p>
<p>hive.join.cache.size<br>25000<br>连接表(流表除外)中有多少行应缓存在内存中.</p>
<p>hive.join.emit.interval<br>1000<br>在发出连接结果之前,最右边的连接操作数 Hive 应该缓冲多少行.</p>
<p>hive.lazysimple.extended_boolean_literal<br>FALSE<br>LazySimpleSerde 使用此属性来确定除了 &#39;TRUE&#39; 和 &#39;FALSE&#39; 之外,它是否将 &#39;T&#39;/&#39;t&#39;/&#39;F&#39;/&#39;f&#39;/&#39;1&#39; 和 &#39;0&#39; 视为扩展的合法布尔文字. 默认为 false,这意味着只有 &#39;TRUE&#39; 和 &#39;FALSE&#39; 被视为合法的布尔文字.</p>
<p><font color="#dd0000">hive.limit.optimize.enable</font><br>FALSE<br>是否启用优化以首先尝试使用较小的数据子集进行简单 LIMIT.</p>
<p>hive.limit.optimize.fetch.max<br>50000<br>简单 LIMIT 的较小数据子集允许的最大行数,如果它是获取查询.插入查询不受此限制的限制.</p>
<p>hive.limit.optimize.limit.file<br>10<br>当为简单的 LIMIT 尝试较小的数据子集时,我们可以采样的最大文件数.</p>
<p>hive.limit.pushdown.memory.usage<br>0.1<br>期望值介于 0.0f 和 1.0f 之间.用于在 Reducesink 运算符中缓冲行以进行限制下推优化的可用内存分数.</p>
<p>hive.limit.query.max.table.partition<br>-1<br>这控制了每个分区表可以扫描多少个分区.默认值&quot;-1&quot;表示没有限制.</p>
<p>hive.limit.row.max.size<br>100000<br>当为简单的 LIMIT 尝试较小的数据子集时,我们需要保证每行至少具有多少大小.</p>
<h3 id="llap"><a href="#llap" class="headerlink" title="llap"></a>llap</h3><p>hive.llap.allow.permanent.fns<br>TRUE<br>LLAP 决策者是否应允许永久 UDF.</p>
<p>hive.llap.am.liveness.connection.sleep.between.retries.ms<br>2000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.等待重试连接时的睡眠持续时间来自一般保持活动线程的守护程序的 AM 故障(毫秒).</p>
<p>hive.llap.am.liveness.connection.timeout.ms<br>10000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.等待连接的时间在认为 AM 已死之前,来自 LLAP 守护进程的 AM 故障.</p>
<p>hive.llap.auto.allow.uber<br>FALSE<br>是否允许规划器在 AM 中运行顶点.</p>
<p>hive.llap.auto.auth<br>FALSE<br>是否设置 Hadoop 配置以在 LLAP Web 应用程序中启用身份验证.</p>
<p>hive.llap.auto.enforce.stats<br>TRUE<br>在考虑顶点之前强制 col stats 可用</p>
<p>hive.llap.auto.enforce.tree<br>TRUE<br>在考虑顶点之前强制所有父母都在 llap</p>
<p>hive.llap.auto.enforce.vectorized<br>TRUE<br>在考虑顶点之前强制输入向量化</p>
<p>hive.llap.auto.max.input.size<br>10737418240<br>在考虑顶点之前检查输入大小(-1 禁用检查)</p>
<p>hive.llap.auto.max.output.size<br>1073741824<br>在考虑顶点之前检查输出大小(-1 禁用检查)</p>
<p>hive.llap.cache.allow.synthetic.fileid<br>FALSE<br>如果没有真实的文件 ID,LLAP 缓存是否应该使用合成文件 ID.HDFS/Isilon 等系统提供唯一的文件/inode ID.在其他 FS(例如本地 FS)上,默认情况下缓存不会工作,因为 LLAP 无法唯一跟踪文件;启用此设置允许 LLAP 根据路径/大小和修改时间生成文件 ID,这几乎可以确定文件的唯一性.但是,如果您使用没有文件 ID 的 FS 并大量重写文件(或偏执),您可能希望避免此设置.</p>
<p>hive.llap.client.consistent.splits<br>FALSE<br>是否设置拆分位置以匹配运行 llap 守护程序的节点,而不是使用拆分本身提供的位置</p>
<p>hive.llap.daemon.acl<br><code>*</code><br>LLAP 守护程序的 ACL.</p>
<p>hive.llap.daemon.acl.blocked<br>未配置<br>LLAP 守护程序的拒绝 ACL.</p>
<p>hive.llap.daemon.am.liveness.heartbeat.interval.ms<br>10000ms<br>需要一个时间值,单位为 (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 msec.Tez AM-LLAP 心跳间隔 (毫秒).这需要低于任务超时间隔,否则尽可能高以避免不必要的流量.</p>
<p>hive.llap.daemon.communicator.num.threads<br>10<br>在 Tez AM 中的 LLAP 任务通信器中使用的线程数.</p>
<p>hive.llap.daemon.container.id<br>未配置<br>正在运行的 LlapDaemon 的 ContainerId.用于发布到注册表</p>
<p>hive.llap.daemon.delegation.token.lifetime<br>14d<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.LLAP 委托令牌生命周期,以秒为单位如果指定没有单位.</p>
<p>hive.llap.daemon.download.permanent.fns<br>FALSE<br>LLAP 守护程序是否应本地化永久 UDF 的资源.</p>
<p>hive.llap.daemon.keytab.file<br>未配置<br>包含 LLAP 守护程序的服务主体的 Kerberos Keytab 文件的路径.</p>
<p>hive.llap.daemon.memory.per.instance.mb<br>4096<br>用于 LLAP 内的执行程序的内存总量(以兆字节为单位).</p>
<p>hive.llap.daemon.num.executors<br>4<br>在 LLAP 守护进程中使用的执行器数量;本质上,可以并行执行的任务数.</p>
<p>hive.llap.daemon.num.file.cleaner.threads<br>1<br>LLAP 中的文件清理器线程数.</p>
<p>hive.llap.daemon.output.service.port<br>15003<br>LLAP daemon 输出服务端口</p>
<p>hive.llap.daemon.output.service.send.buffer.size<br>131072<br>LLAP 守护程序输出服务使用的发送缓冲区大小</p>
<p>hive.llap.daemon.queue.name<br>未配置<br>llap 滑块应用程序将在其中运行的队列名称.在 LlapServiceDriver 和 package.py 中使用</p>
<p>hive.llap.daemon.rpc.num.handlers<br>5<br>LLAP 守护程序的 RPC 处理程序的数量.</p>
<p>hive.llap.daemon.rpc.port<br>15001<br>LLAP 守护程序 RPC 端口.</p>
<p>hive.llap.daemon.service.hosts<br>未配置<br>明确指定用于 LLAP 调度的主机.对测试很有用.默认情况下,使用 YARN 注册表.</p>
<p>hive.llap.daemon.service.principal<br>未配置<br>LLAP 守护程序的服务主体的名称.</p>
<p>hive.llap.daemon.service.refresh.interval.sec<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.LLAP YARN 注册表服务列表刷新延迟, 片刻之间.</p>
<p>hive.llap.daemon.shuffle.dir.watcher.enabled<br>FALSE<br>待办事项</p>
<p>hive.llap.daemon.task.preemption.metrics.intervals<br>30,60,300<br>以逗号分隔的整数集,表示百分位延迟指标的所需翻转间隔(以秒为单位).由 LLAP 守护程序任务调度程序度量用于终止任务(由于抢占)和即将被抢占的任务浪费的有用时间.</p>
<p>hive.llap.daemon.task.scheduler.enable.preemption<br>TRUE<br>LLAP 调度程序中的可完成任务是否应该抢占不可完成的正在运行的任务(例如等待输入的减速器).</p>
<p>hive.llap.daemon.task.scheduler.wait.queue.size<br>10<br>LLAP 调度程序最大队列大小.</p>
<p>hive.llap.daemon.vcpus.per.instance<br>4<br>用于 LLAP 内的执行程序的 vcpus 总数.</p>
<p>hive.llap.daemon.wait.queue.comparator.class.name<br>org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator<br>用于 LLAP 调度程序优先级队列的优先级比较器.内置选项是 org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator 和 .....FirstInFirstOutComparator</p>
<p>hive.llap.daemon.web.port<br>15002<br>LLAP 守护程序 Web UI 端口.</p>
<p>hive.llap.daemon.web.ssl<br>FALSE<br>LLAP 守护程序 Web UI 是否应使用 SSL.</p>
<p>hive.llap.daemon.work.dirs<br>未配置<br>守护进程的工作目录.需要为安全集群设置,因为 LLAP 可能无法访问默认的 YARN 工作目录.如果未设置,则使用 yarn.nodemanager.local-dirs</p>
<p>hive.llap.daemon.yarn.container.mb<br>-1<br>llap 服务器纱线容器大小(以 MB 为单位).在 LlapServiceDriver 和 package.py 中使用</p>
<p>hive.llap.daemon.yarn.shuffle.port<br>15551<br>用于 LLAP 守护进程托管的 shuffle 的 YARN shuffle 端口.</p>
<p>hive.llap.enable.grace.join.in.llap<br>FALSE<br>如果应允许宽限连接在 llap 中运行,则覆盖.</p>
<p>hive.llap.execution.mode<br>none<br>期望<code>[auto, none, all, map]</code>之一.选择查询片段是在容器中还是在 llap 中运行</p>
<p>hive.llap.file.cleanup.delay.seconds<br>300s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.在清理之前延迟多长时间LLAP 中的查询文件(以秒为单位,用于调试).</p>
<p>hive.llap.io.allocator.alloc.max<br>16Mb<br>需要一个带有单位的字节大小值(字节/kb/mb/gb/tb/pb 为空白).来自 LLAP 伙伴分配器的最大分配可能.对于 ORC,应与预期的最大 ORC 压缩缓冲区大小一样大.必须是 2 的幂.</p>
<p>hive.llap.io.allocator.alloc.min<br>16Kb<br>期望一个带有单位的字节大小值(字节/kb/mb/gb/tb/pb 为空白).来自 LLAP 伙伴分配器的最小分配可能.下面的分配被填充到最小分配.对于 ORC,通常应该与预期的压缩缓冲区大小相同,或者是 2 的次幂.必须是 2 的幂.</p>
<p>hive.llap.io.allocator.arena.count<br>8<br>LLAP 低级缓存的 Arena 计数;缓存将以 (size/arena_count) 字节为单位分配.这个大小必须是 &lt;= 1Gb 并且 &gt;= 最大分配;如果不是这种情况,将使用调整后的大小.建议使用 2 的幂.</p>
<p>hive.llap.io.allocator.direct<br>TRUE<br>ORC 低级缓存是否应该使用直接分配.</p>
<p>hive.llap.io.allocator.mmap<br>FALSE<br>ORC 低级缓存是否应使用内存映射分配(直接 I/O).建议与 NVDIMM (DAX) 或 NVMe 闪存存储一起使用.</p>
<p>hive.llap.io.allocator.mmap.path<br>/tmp<br>需要本地文件系统上的可写目录.用于将 NVDIMM/NVMe 闪存存储映射到 ORC 低级缓存的目录位置.</p>
<p>hive.llap.io.decoding.metrics.percentiles.intervals<br>30<br>以逗号分隔的整数集,表示 LLAP 守护程序 IO 解码 time.hive.llap.queue.metrics.percentiles.intervals 上百分位延迟指标的所需翻转间隔(以秒为单位)</p>
<p>hive.llap.io.enabled<br>未配置<br>是否启用 LLAP IO 层.</p>
<p>hive.llap.io.lrfu.lambda<br>0.01<br>Lambda 用于 ORC 低级缓存 LRFU 缓存策略.必须在 <code>[0, 1]</code> 中.0 使 LRFU 表现得像 LFU,1 使它表现得像 LRU,两者之间的值相应地平衡.</p>
<p>hive.llap.io.memory.mode<br>cache<br>期望 <code>[cache, none]</code>.LLAP IO 内存使用量之一;&#39;cache&#39;(默认)使用带有自定义堆外分配器的数据和元数据缓存,&#39;none&#39; 两者都不使用(这种模式可能会导致显着的性能下降)</p>
<p>hive.llap.io.memory.size<br>1Gb<br>需要一个带有单位的字节大小值(字节/kb/mb/gb/tb/pb 为空白).IO 分配器或 ORC 低级缓存的最大大小.</p>
<p>hive.llap.io.orc.time.counters<br>TRUE<br>是否开启 LLAP IO 层的时间计数器(在 HDFS 中花费的时间等)</p>
<p>hive.llap.io.threadpool.size<br>10<br>指定用于低级 IO 线程池的线程数.</p>
<p>hive.llap.io.use.fileid.path<br>TRUE<br>LLAP 是否应使用基于 fileId(inode)的路径来确保文件覆盖情况的更好一致性.这在 HDFS 上受支持.</p>
<p>hive.llap.io.use.lrfu<br>TRUE<br>ORC 低级缓存是否应该使用 LRFU 缓存策略而不是默认 (FIFO).</p>
<p>hive.llap.management.acl<br><code>*</code><br>用于 LLAP 守护程序管理的 ACL.</p>
<p>hive.llap.management.acl.blocked<br>未配置<br>用于 LLAP 守护程序管理的拒绝 ACL.</p>
<p>hive.llap.management.rpc.port<br>15004<br>用于 LLAP 守护程序管理服务的 RPC 端口.</p>
<p>hive.llap.object.cache.enabled<br>TRUE<br>在 llap 中缓存对象(计划/哈希表等)</p>
<p>hive.llap.orc.gap.cache<br>TRUE<br>ORC 的 LLAP 缓存是否应该记住 ORC 压缩缓冲区读取估计中的间隙,以避免重新读取一次读取并因为不需要而丢弃的数据.这仅对在 HIVE-9660 之前编写的 ORC 文件是必需的.</p>
<p>hive.llap.remote.token.requires.signing<br>TRUE<br>期望 <code>[false, except_llap_owner, true]</code> 之一.从 LLAP 管理 API 返回的令牌是否需要片段签名.默认情况下为 True;可以禁用以允许 CLI 通过将其设置为 true 或&quot;except_llap_owner&quot;来从安全集群中的 LLAP 获取令牌(后者将此类令牌返回给所有人,但用户 LLAP 集群正在对其进行身份验证).</p>
<p>hive.llap.skip.compile.udf.check<br>FALSE<br>在决定是否在 LLAP 中执行任务时是否跳过对非内置 UDF 的编译时检查.跳过检查允许从 LLAP 中的预本地化 jar 执行 UDF;如果 jar 没有预先本地化,UDF 将无法加载.</p>
<p>hive.llap.task.communicator.connection.sleep.between.retries.ms<br>2000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.睡眠持续时间(以毫秒为单位)从 Tez AM 获取到 LLAP 守护程序的连接时,在重试错误之前等待.</p>
<p>hive.llap.task.communicator.connection.timeout.ms<br>16000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.连接超时(以毫秒为单位)之前来自 Tez AM 的 LLAP 守护程序失败.</p>
<p>hive.llap.task.scheduler.locality.delay<br>0ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.时间应介于 - 1 毫秒(含)和 9223372036854775807 毫秒(含).在将包含位置信息的请求分配到所请求的位置以外的位置之前等待的时间量.设置为 -1 表示无限延迟,设置为 0 表示无延迟.</p>
<p>hive.llap.task.scheduler.node.disable.backoff.factor<br>1.5<br>由于某些故障,节点的连续黑名单上的退避因子.黑名单时间从最小超时开始,然后根据此退避因子上升到最大超时.</p>
<p>hive.llap.task.scheduler.node.reenable.max.timeout.ms<br>10000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.节点将重新启用调度,以毫秒为单位.如果故障持续存在,这可以通过指数回退来修改.</p>
<p>hive.llap.task.scheduler.node.reenable.min.timeout.ms<br>200ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.之前禁用的最短时间节点将重新启用调度,以毫秒为单位.如果故障持续存在,这可以通过指数回退来修改.</p>
<p>hive.llap.task.scheduler.num.schedulable.tasks.per.node<br>0<br>AM TaskScheduler 将尝试为每个节点分配的任务数.0 表示这应该从注册表中获取.-1 表示无限容量;正值表示特定的界限.</p>
<p>hive.llap.task.scheduler.timeout.seconds<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.失败前等待的时间集群中没有运行(活动)的 llap 守护程序时的查询.</p>
<p>hive.llap.validate.acls<br>TRUE<br>LLAP 是否应该在某些情况下拒绝许可 ACL(例如,它自己的管理协议或 ZK 路径),类似于 ssh 如何拒绝具有不良访问权限的密钥.</p>
<p>hive.llap.zk.registry.namespace<br>未配置<br>在基于 LLAP ZooKeeper 的注册表中,覆盖 ZK 路径命名空间.请注意,使用它会使路径管理(例如设置正确的 ACL)成为您的责任.</p>
<p>hive.llap.zk.registry.user<br>未配置<br>在基于 LLAP ZooKeeper 的注册表中,指定 Zookeeper 路径中的用户名.这应该是 hive 用户或运行 LLAP 守护程序的任何用户.</p>
<p>hive.llap.zk.sm.connectionString<br>未配置<br>ZooKeeper SecretManager 的 ZooKeeper 连接字符串.</p>
<p>hive.llap.zk.sm.keytab.file<br>未配置<br>Kerberos Keytab 文件的路径,其中包含用于与 ZooKeeper 通信以获取 ZooKeeper SecretManager 的主体.</p>
<p>hive.llap.zk.sm.principal<br>未配置<br>用于与 ZooKeeper 通信以获取 ZooKeeper SecretManager 的主体名称.</p>
<h3 id="lock"><a href="#lock" class="headerlink" title="lock"></a>lock</h3><p>hive.localize.resource.num.wait.attempts<br>5<br>在 hive-tez 中等待本地化资源的尝试次数.</p>
<p>hive.localize.resource.wait.interval<br>5000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.等待另一个线程的时间为 hive-tez 本地化相同的资源.</p>
<p>hive.lock.manager<br>org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager<br>无</p>
<p>hive.lock.mapred.only.operation<br>FALSE<br>此参数用于控制是否只锁定需要执行至少一个映射作业的查询.</p>
<p>hive.lock.numretries<br>100<br>您想尝试获得所有锁的次数</p>
<p>hive.lock.sleep.between.retries<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.时间应在 0 之间sec (exclusive) 和 9223372036854775807 sec (exclusive). 各种重试之间的最大休眠时间</p>
<p>hive.lockmgr.zookeeper.default.partition.name<br><code>__HIVE_DEFAULT_ZOOKEEPER_PARTITION__</code><br>无</p>
<h3 id="log"><a href="#log" class="headerlink" title="log"></a>log</h3><p>hive.log.every.n.records<br>0<br>期望值大于 0.如果值大于 0,则以大小为 n 的固定间隔而不是指数方式记录.</p>
<p>hive.log.explain.output<br>FALSE<br>是否为每个查询记录解释输出.启用后,将在 INFO log4j 日志级别记录查询的 EXPLAIN EXTENDED 输出.</p>
<p>hive.log.trace.id<br>未配置<br>上游客户端可用于跟踪相应日志的日志跟踪 ID.截断为 64 个字符.默认使用自动生成的会话 ID.</p>
<p>hive.log4j.file<br>未配置<br>Hive log4j 配置文件.如果未设置该属性,则将使用在类路径上找到的 hive-log4j2.properties 初始化日志记录.如果设置了该属性,则该值必须是有效的 URI(java.net.URI,例如&quot; file:///tmp/my-logging.xml&quot;),然后您可以从中提取 URL 并将其传递给 PropertyConfigurator.configure(URL).</p>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p><font color="#dd0000">hive.map.aggr</font><br>TRUE<br>是否在 Hive Group By 查询中使用地图端聚合</p>
<p>hive.map.aggr.hash.force.flush.memory.threshold<br>0.9<br>map端组聚合hash表要使用的最大内存,如果内存使用率高于这个数,则强制刷新数据</p>
<p>hive.map.aggr.hash.min.reduction<br>0.5<br>如果哈希表大小与输入行之间的比率大于此数字,则将关闭哈希聚合.设置为 1 以确保永远不会关闭哈希聚合.</p>
<p>hive.map.aggr.hash.percentmemory<br>0.5<br>映射端组聚合哈希表要使用的总内存的一部分</p>
<p>hive.map.groupby.sorted<br>TRUE<br>如果表的bucket/sorting属性与grouping key完全匹配,是否在mapper中使用BucketizedHiveInputFormat进行group by.唯一的缺点是它将映射器的数量限制为文件的数量.</p>
<p>hive.mapjoin.bucket.cache.size<br>100<br>无</p>
<p>hive.mapjoin.check.memory.rows<br>100000<br>该数字表示在处理了多少行之后需要检查内存使用情况</p>
<p>hive.mapjoin.followby.gby.localtask.max.memory.usage<br>0.55<br>这个数字意味着当这个 map join 后面跟着 group by 时,本地任务可以占用多少内存来将 key/value 保存到内存中的哈希表中.如果本地任务的内存使用量超过这个数字,本地任务将自行中止.这意味着小表的数据太大而无法保存在内存中.</p>
<p>hive.mapjoin.followby.map.aggr.hash.percentmemory<br>0.3<br>map 端组聚合哈希表要使用的总内存的一部分,当这个 group by 后跟 map join</p>
<p>hive.mapjoin.hybridgrace.bloomfilter<br>TRUE<br>是否在混合宽限哈希连接中使用 BloomFilter 以最大程度地减少不必要的溢出.</p>
<p>hive.mapjoin.hybridgrace.hashtable<br>TRUE<br>是否使用 hybridgrace hash join 作为 mapjoin 的 join 方法.只有特兹.</p>
<p>hive.mapjoin.hybridgrace.memcheckfrequency<br>1024<br>对于混合宽限哈希联接,我们检查内存是否已满的频率(相隔多少行).这个数字应该是 2 的幂.</p>
<p>hive.mapjoin.hybridgrace.minnumpartitions<br>16<br>对于Hybrid grace hash join,要创建的最小分区数.</p>
<p>hive.mapjoin.hybridgrace.minwbsize<br>524288<br>对于混合graceHash join,优化哈希表使用的最小写入缓冲区大小.默认为 512 KB.</p>
<p>hive.mapjoin.localtask.max.memory.usage<br>0.9<br>这个数字表示本地任务可以占用多少内存来将键/值保存到内存中的哈希表中.如果本地任务的内存使用量超过这个数字,本地任务将自行中止.这意味着数据小表太大而无法保存在内存中.</p>
<p>hive.mapjoin.optimized.hashtable<br>TRUE<br>Hive 是否应该为 MapJoin 使用内存优化哈希表.仅适用于 Tez 和 Spark,因为内存优化哈希表无法序列化.</p>
<p>hive.mapjoin.optimized.hashtable.probe.percent<br>0.5<br>优化哈希表的探测空间百分比</p>
<p>hive.mapjoin.optimized.hashtable.wbsize<br>8388608<br>优化的哈希表(请参阅 hive.mapjoin.optimized.hashtable)使用缓冲区链来存储数据.这是一种缓冲区大小.如果它更大,HT 可能会稍微快一些,但是对于小连接,不必要的内存将被分配然后修剪.</p>
<p>hive.mapjoin.smalltable.filesize<br>25000000<br>小表的输入文件大小阈值;如果文件大小小于这个阈值,它会尝试将common join转换为map join</p>
<p>hive.mapper.cannot.span.multiple.partitions<br>FALSE<br>无</p>
<p><font color="#dd0000">hive.mapred.local.mem</font><br>0<br>本地模式下的 mapper/reducer 内存</p>
<p>hive.mapred.mode<br>nonstrict<br>已弃用;改用 <code>hive.strict.checks.*</code> 设置.</p>
<p><font color="#dd0000">hive.mapred.partitioner</font><br>org.apache.hadoop.hive.ql.io.DefaultHivePartitioner<br>无</p>
<p>hive.mapred.reduce.tasks.speculative.execution<br>TRUE<br>是否应该打开减速器的推测执行.</p>
<p>hive.max.open.txns<br>100000<br>打开交易的最大数量.如果当前未结交易达到此限制,则未来的未结交易请求将被拒绝,直到此数量低于限制.</p>
<h3 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h3><p><font color="#dd0000">hive.merge.mapfiles</font><br>TRUE<br>在仅地图作业结束时合并小文件</p>
<p><font color="#dd0000">hive.merge.mapredfiles</font><br>FALSE<br>在 map-reduce 作业结束时合并小文件</p>
<p>hive.merge.orcfile.stripe.level<br>TRUE<br>当 hive.merge.mapfiles/hive.merge.mapredfiles 或 hive.merge.tezfiles 在写入 ORC 文件格式的表时启用,启用此配置将对小型 ORC 文件进行条带级快速合并.请注意,启用此配置不会遵守填充容差配置 (hive.exec.orc.block.padding.tolerance).</p>
<p>hive.merge.rcfile.block.level<br>TRUE<br>无</p>
<p>hive.merge.size.per.task<br>256000000<br>作业结束时合并文件的大小</p>
<p>hive.merge.smallfiles.avgsize<br>16000000<br>当作业的平均输出文件大小小于此数字时,Hive 将启动一个附加的 map-reduce 作业以将输出文件合并为更大的文件.如果 hive.merge.mapfiles 为 true,则仅对 map-only 作业执行此操作;如果 hive.merge.mapredfiles 为 true,则对 map-reduce 作业执行此操作.</p>
<p>hive.merge.sparkfiles<br>FALSE<br>在 Spark DAG 转换结束时合并小文件</p>
<p>hive.merge.tezfiles<br>FALSE<br>在 Tez DAG 末尾合并小文件</p>
<h3 id="metastore"><a href="#metastore" class="headerlink" title="metastore"></a>metastore</h3><p>hive.metadata.export.location<br>未配置<br>当与 org.apache.hadoop.hive.ql.parse.MetaDataExportListener 事件前监听器一起使用时,它是元数据将被导出到的位置.默认值为空字符串,这会导致元数据被导出到 HDFS 上当前用户的主目录.</p>
<p>hive.metadata.move.exported.metadata.to.trash<br>TRUE<br>当与 org.apache.hadoop.hive.ql.parse.MetaDataExportListener 事件前侦听器一起使用时,此设置确定导出的元数据是否随后将与删除的表数据一起移动到用户的垃圾目录.这确保元数据将与删除的表数据一起被清理.</p>
<p>hive.metastore.aggregate.stats.cache.clean.until<br>0.8<br>清理线程会一直清理,直到缓存达到这个 % full size.</p>
<p>hive.metastore.aggregate.stats.cache.enabled<br>TRUE<br>是否启用聚合统计缓存.</p>
<p>hive.metastore.aggregate.stats.cache.fpp<br>0.01<br>每个聚合统计缓存节点中使用的布隆过滤器的最大误报概率(默认 1%).</p>
<p>hive.metastore.aggregate.stats.cache.max.full<br>0.9<br>缓存清理线程启动后的最大缓存满百分比.</p>
<p>hive.metastore.aggregate.stats.cache.max.partitions<br>10000<br>每个缓存节点聚合的最大分区数.</p>
<p>hive.metastore.aggregate.stats.cache.max.reader.wait<br>1000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.读者将等待的毫秒数在放弃之前获取读锁.</p>
<p>hive.metastore.aggregate.stats.cache.max.variance<br>0.01<br>缓存节点和我们的请求之间的分区数的最大可容忍差异(默认 1%).</p>
<p>hive.metastore.aggregate.stats.cache.max.writer.wait<br>5000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.写入器将等待的毫秒数在放弃之前获取写锁.</p>
<p>hive.metastore.aggregate.stats.cache.size<br>10000<br>我们将放置在 Metastore 聚合统计信息缓存中的聚合统计信息节点的最大数量.</p>
<p>hive.metastore.aggregate.stats.cache.ttl<br>600s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.缓存节点的秒数在它们变得陈旧之前在缓存中处于活动状态.</p>
<p>hive.metastore.archive.intermediate.archived<br><code>_INTERMEDIATE_ARCHIVED</code><br>无</p>
<p>hive.metastore.archive.intermediate.extracted<br><code>_INTERMEDIATE_EXTRACTED</code><br>无</p>
<p>hive.metastore.archive.intermediate.original<br><code>_INTERMEDIATE_ORIGINAL</code><br>用于归档的中间目录后缀.只要避免碰撞,它们是什么并不重要</p>
<p>hive.metastore.authorization.storage.checks<br>FALSE<br>Metastore 是否应该针对底层存储(通常是 hdfs)对 drop-partition 等操作进行授权检查(如果相关用户无权删除存储上的相应目录,则不允许 drop-partition).</p>
<p>hive.metastore.batch.retrieve.max<br>300<br>一次可以从 Metastore 中检索到的对象(表/分区)的最大数量.数字越大,Hive Metastore 服务器所需的往返次数越少,但也可能导致客户端的内存需求更高.</p>
<p>hive.metastore.batch.retrieve.table.partition.max<br>1000<br>Metastore 在一批中内部检索的最大对象数.</p>
<p>hive.metastore.cache.pinobjtypes<br>Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order<br>应固定在缓存中的逗号分隔元存储对象类型列表</p>
<p><font color="#dd0000">hive.metastore.client.connect.retry.delay</font><br>1s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.客户端的秒数在连续的连接尝试之间等待</p>
<p>hive.metastore.client.drop.partitions.using.expressions<br>TRUE<br>选择使用 HCatClient 删除分区是将分区谓词推送到元存储,还是迭代删除分区</p>
<p>hive.metastore.client.socket.lifetime<br>0s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.MetaStore 客户端套接字生命周期以秒为单位.超过此时间后,客户端会在下一次 MetaStore 操作时重新连接.值为 0 表示连接具有无限的生命周期.</p>
<p>hive.metastore.client.socket.timeout<br>600s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.MetaStore 客户端套接字超时(以秒为单位)</p>
<p><font color="#dd0000">hive.metastore.connect.retries</font><br>3<br>打开与 Metastore 的连接时的重试次数</p>
<p>hive.metastore.dbaccess.ssl.properties<br>未配置<br>当 JDO 连接 URL 启用 SSL 访问时,元存储访问数据库的逗号分隔 SSL 属性.例如 javax.net.ssl.trustStore=/tmp/truststore,javax.net.ssl.trustStorePassword=pwd.</p>
<p>hive.metastore.direct.sql.batch.size<br>0<br>在直接 SQL 中从基础数据库中检索分区和其他对象的批量大小.对于 Oracle 和 MSSQL 等一些 DB,存在硬编码或基于 perf 的限制,因此必须这样做.对于可以处理查询的数据库,这不是必需的,并且可能会影响性能.-1 表示不分批,0 表示自动分批.</p>
<p><font color="#dd0000">hive.metastore.disallow.incompatible.col.type.changes</font><br>TRUE<br>如果为 true(默认为 false),则不允许将列的类型(例如 STRING)更改为不兼容的类型(例如 MAP)的 ALTER TABLE 操作.RCFile 默认 SerDe (ColumnarSerDe) 以数据类型可以的方式序列化值从字符串转换为任何类型.映射也被序列化为字符串,也可以作为字符串读取.但是,对于任何二进制序列化,这都是不正确的.在随后尝试访问旧分区时,阻止 ALTER TABLE 可防止 ClassCastExceptions.INT/STRING/BIGINT 等基本类型相互兼容且不会被阻止.有关更多详细信息,请参阅 HIVE-4409.</p>
<p>hive.metastore.dml.events<br>FALSE<br>如果为 true,将要求元存储为 DML 操作触发事件</p>
<p>hive.metastore.ds.connection.url.hook<br>未配置<br>用于检索 JDO 连接 URL 的挂钩的名称.如果为空,则使用 javax.jdo.option.ConnectionURL 中的值</p>
<p>hive.metastore.end.function.listeners<br>未配置<br>Metastore 函数末尾的逗号分隔侦听器列表.</p>
<p>hive.metastore.event.clean.freq<br>0s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.计时器任务运行到的频率清除 Metastore 中的过期事件.</p>
<p>hive.metastore.event.db.listener.timetolive<br>86400s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.在此之后事件将被删除从数据库监听队列</p>
<p>hive.metastore.event.expiry.duration<br>0s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.事件从事件到期的持续时间桌子</p>
<p>hive.metastore.event.listeners<br>未配置<br>无</p>
<p>hive.metastore.execute.setugi<br>TRUE<br>在不安全模式下,将此属性设置为 true 将导致 Metastore 使用客户端报告的用户和组权限执行 DFS 操作.请注意,必须在客户端和服务器端都设置此属性.进一步注意它的最大努力.如果客户端将其设置为 true 而服务器将其设置为 false,则客户端设置将被忽略.</p>
<p>hive.metastore.expression.proxy<br>org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore<br>无</p>
<p>hive.metastore.failure.retries<br>1<br>Thrift Metastore 调用失败时的重试次数</p>
<p>hive.metastore.fastpath<br>FALSE<br>用于避免 Metastore 中的所有代理和对象副本.请注意,如果设置了此项,则必须使用本地元存储(hive.metastore.uris 必须为空)否则将导致未定义且很可能出现不希望的行为</p>
<p>hive.metastore.filter.hook<br>org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl<br>Metastore 钩子类,用于过滤元数据读取结果.如果 hive.security.authorization.manageris 设置为 HiveAuthorizerFactory 的实例,则忽略此值.</p>
<p>hive.metastore.fs.handler.class<br>org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl<br>无</p>
<p>hive.metastore.fshandler.threads<br>20<br>为 fs 操作的 Metastore 处理程序分配的线程数.</p>
<p>hive.metastore.hbase.aggr.stats.cache.entries<br>10000<br>在内存中缓存多少个统计对象</p>
<p>hive.metastore.hbase.aggr.stats.hbase.ttl<br>604800s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.创建后的 HBase 缓存.在此之前,它们可能会因更新或分区删除而失效.默认为一周.</p>
<p>hive.metastore.hbase.aggr.stats.invalidator.frequency<br>5s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.stats 缓存多久扫描一次HBase 条目并查找过期条目</p>
<p>hive.metastore.hbase.aggr.stats.memory.ttl<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.统计对象存在的秒数从 HBase 读取它们后的内存.</p>
<p>hive.metastore.hbase.aggregate.stats.cache.size<br>10000<br>我们将放置在 hbase Metastore 聚合统计信息缓存中的聚合统计信息节点的最大数量.</p>
<p>hive.metastore.hbase.aggregate.stats.false.positive.probability<br>0.01<br>每个聚合统计缓存节点中使用的布隆过滤器的最大误报概率(默认 1%).</p>
<p>hive.metastore.hbase.aggregate.stats.max.partitions<br>10000<br>每个缓存节点聚合的最大分区数.</p>
<p>hive.metastore.hbase.aggregate.stats.max.variance<br>0.1<br>缓存节点和我们的请求之间的最大可容忍分区数差异(默认 10%).</p>
<p>hive.metastore.hbase.cache.clean.until<br>0.8<br>清理线程会一直清理,直到缓存达到这个 % full size.</p>
<p>hive.metastore.hbase.cache.max.full<br>0.9<br>缓存清理线程启动后的最大缓存满百分比.</p>
<p>hive.metastore.hbase.cache.max.reader.wait<br>1000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.读者将等待的毫秒数在放弃之前获取读锁.</p>
<p>hive.metastore.hbase.cache.max.writer.wait<br>5000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.写入器将等待的毫秒数在放弃之前获取写锁.</p>
<p>hive.metastore.hbase.cache.ttl<br>600s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.缓存节点的秒数在它们变得陈旧之前在缓存中处于活动状态.</p>
<p>hive.metastore.hbase.catalog.cache.size<br>50000<br>我们将放置在 hbase 元存储目录缓存中的最大对象数.这些对象将按我们需要缓存的类型进行划分.</p>
<p>hive.metastore.hbase.connection.class<br>org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection<br>用于连接 HBase 的类</p>
<p>hive.metastore.hbase.file.metadata.threads<br>1<br>用于在后台读取文件元数据以缓存它的线程数.</p>
<p>hive.metastore.init.hooks<br>未配置<br>在 HMSHandler 初始化开始时要调用的钩子的逗号分隔列表.一个 init 钩子被指定为扩展 org.apache.hadoop.hive.metastore.MetaStoreInitListener 的 Java 类的名称.</p>
<p>hive.metastore.initial.metadata.count.enabled<br>TRUE<br>在 Metastore 启动时为指标启用元数据计数.</p>
<p>hive.metastore.integral.jdo.pushdown<br>FALSE<br>允许对元存储中的完整分区列进行 JDO 查询下推.默认关闭.这提高了整列的 Metastore 性能,特别是在有大量分区的情况下.但是,它不能正确处理未标准化的整数值(例如,具有前导零,如 0012).如果 Metastore 直接 SQL 已启用并正常工作,则此优化也无关紧要.</p>
<p>hive.metastore.kerberos.keytab.file<br>未配置<br>包含 Metastore Thrift 服务器的服务主体的 Kerberos Keytab 文件的路径.</p>
<p>hive.metastore.kerberos.principal<br>hive-metastore/<code>_HOST@EXAMPLE.COM</code><br>Metastore Thrift 服务器的服务主体.特殊字符串 _HOST 将自动替换为正确的主机名.</p>
<p>hive.metastore.metrics.enabled<br>FALSE<br>在元存储上启用指标.</p>
<p>hive.metastore.orm.retrieveMapNullsAsEmptyStrings<br>FALSE<br>Thrift 不支持映射中的空值,因此从 ORM 检索到的映射中存在的任何空值都必须被修剪或转换为空字符串.一些支持数据库(例如 Oracle)将空字符串作为空值保存,因此如果我们希望反转该行为,我们应该设置此参数.对于其他人来说,修剪是正确的行为</p>
<p>hive.metastore.partition.inherit.table.properties<br>未配置<br>表属性中出现的逗号分隔键列表将被继承到新创建的分区.<code>*</code>意味着所有键都将被继承.</p>
<p>hive.metastore.partition.name.whitelist.pattern<br>未配置<br>分区名称将根据此正则表达式模式进行检查,如果不匹配则拒绝.</p>
<p>hive.metastore.port<br>9083<br>Hive Metastore 侦听器端口</p>
<p>hive.metastore.pre.event.listeners<br>未配置<br>Metastore 事件的逗号分隔侦听器列表.</p>
<p>hive.metastore.rawstore.impl<br>org.apache.hadoop.hive.metastore.ObjectStore<br>实现org.apache.hadoop.hive.metastore.rawstore接口的类名.该类用于存储和检索表/数据库等原始元数据对象</p>
<p>hive.metastore.sasl.enabled<br>FALSE<br>如果为 true,Metastore Thrift 接口将使用 SASL 保护.客户端必须使用 Kerberos 进行身份验证.</p>
<p><font color="#dd0000">hive.metastore.schema.verification</font><br>TRUE<br>强制 Metastore 模式版本一致性.True:验证存储的版本信息是否与 Hive jar 中的版本信息兼容.还要禁用自动模式迁移尝试.用户需要在 Hive 升级后手动迁移模式,以确保正确的元存储模式迁移.(默认).False:如果存储在 Metastore 中的版本信息与 Hive jar 中的版本信息不匹配,则发出警告.</p>
<p><font color="#dd0000">hive.metastore.schema.verification.record.version</font><br>FALSE<br>如果为 true,则当前 MS 版本记录在 VERSION 表中.如果禁用此功能并启用验证,则 MS 将无法使用.</p>
<p>hive.metastore.server.max.message.size<br>104857600<br>HMS 将接受的最大消息大小(以字节为单位).</p>
<p>hive.metastore.server.max.threads<br>1000<br>Thrift 服务器池中的最大工作线程数.</p>
<p>hive.metastore.server.min.threads<br>200<br>Thrift 服务器池中的最小工作线程数.</p>
<p>hive.metastore.server.tcp.keepalive<br>TRUE<br>是否为 Metastore 服务器启用 TCP keepalive.Keepalive 将防止半开连接的积累.</p>
<p>hive.metastore.stats.ndv.densityfunction<br>FALSE<br>是否使用密度函数根据分区的NDV估计整张表的NDV</p>
<p>hive.metastore.thrift.compact.protocol.enabled<br>FALSE<br>如果为真,Metastore Thrift 接口将使用 TCompactProtocol.当 false(默认)将使用 TBinaryProtocol.将其设置为 true 将破坏与运行 TBinaryProtocol 的旧客户端的兼容性.</p>
<p>hive.metastore.thrift.framed.transport.enabled<br>FALSE<br>如果为真,Metastore Thrift 接口将使用 TFramedTransport.如果为 false(默认),则使用标准 TTransport.</p>
<p>hive.metastore.token.signature<br>未配置<br>从当前用户的令牌中选择令牌时要匹配的委托令牌服务名称.</p>
<p>hive.metastore.try.direct.sql<br>TRUE<br>Hive 元存储是否应尝试对某些读取路径使用直接 SQL 查询而不是 DataNucleus.这可以在按数量级获取许多分区或列统计信息时提高 Metastore 性能;但是,不能保证适用于所有 RDBMS-es 和所有版本.在 SQL 失败的情况下,元存储将回退到 DataNucleus,因此即使 SQL 不适用于数据存储上的所有查询,它也是安全的.如果所有 SQL 查询都失败(例如,您的元存储由 MongoDB 支持),您可能希望禁用此功能以节省 try-and-fall-back 成本.</p>
<p>hive.metastore.try.direct.sql.ddl<br>TRUE<br>与 hive.metastore.try.direct.sql 相同,用于修改元存储数据的事务中的读取语句.由于 Postgres 中的非标准行为,如果直接 SQL 选择查询在事务中具有不正确的语法或类似内容,则整个事务将失败并且无法回退到 DataNucleus.如果在您的情况下发生这种情况,您应该在事务中禁用直接 SQL 的使用.</p>
<p>hive.metastore.txn.store.impl<br>org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler<br>实现 org.apache.hadoop.hive.metastore.txn.TxnStore 的类的名称.此类用于存储和检索事务和锁</p>
<p><font color="#dd0000">hive.metastore.uris</font><br>未配置<br>远程元存储的 Thrift URI.Metastore 客户端用于连接到远程 Metastore.</p>
<p>hive.metastore.warehouse.dir<br>/user/hive/warehouse<br>仓库默认数据库的位置</p>
<p><font color="#dd0000">hive.msck.path.validation</font><br>throw<br>期望 <code>[throw/skip/ignore]</code> 之一.msck 应采用类似于分区但包含不受支持的字符的 HDFS 目录的方法.&#39;throw&#39;(例外)是默认值;&#39;skip&#39; 将跳过无效目录并仍然修复其他目录;&#39;ignore&#39; 将跳过验证(遗留行为,在许多情况下会导致错误)</p>
<p>hive.multi.insert.move.tasks.share.dependencies<br>FALSE<br>如果在多插入查询结束时设置了表/分区(不是目录)的所有移动任务,则只有在满足所有这些移动任务的依赖关系后才会开始.优点:如果启用了并发,锁将只查询完成后释放,因此启用此配置后,生成表/分区的时间将更接近释放锁定的时间.缺点:如果未启用并发,禁用此配置,表由该查询生成并较早完成的 /partitions 将可用于更早的查询.由于仅在查询完成后才释放锁,因此如果启用了并发,则不适用.</p>
<p>hive.multigroupby.singlereducer<br>TRUE<br>是否通过查询优化多组以生成单个 M/R 作业计划.如果multi group by query 有common group by key,将优化生成单个M/R 作业.</p>
<p>hive.mv.files.thread<br>15<br>需要一个带有单位的字节大小值(字节/kb/mb/gb/tb/pb 为空白).大小应介于 0Pb(含)和 1Kb(含)之间.用于移动任务中的文件的线程数.将其设置为 0 以禁用多线程文件移动.MSCK 也使用此参数来检查表.</p>
<p>hive.new.job.grouping.set.cardinality<br>30<br>是否应启动新的 map-reduce 作业以对集合/汇总/多维数据集进行分组.对于类似查询:select a, b, c, count(1) from T group by a, b, c with rollup; 4 行被创建每行:(a,b,c),(a,b,null),(a,null,null),(null,null,null).如果 T 的基数,这可能导致跨越 map-reduce 边界的爆炸非常高,map-side 聚合做的不是很好.这个参数决定了 Hive 是否应该添加一个额外的 map-reduce 作业.如果分组集基数(上例中为 4)大于此值,则在假设原始分组依据将减少数据大小的情况下添加新的 MR 作业.</p>
<h3 id="optimize"><a href="#optimize" class="headerlink" title="optimize"></a>optimize</h3><p>hive.optimize.bucketingsorting<br>TRUE<br>不要为以下形式的查询创建用于执行分桶/排序的化简器:插入覆盖表 T2 从 T1 中选择<code> *</code>;其中 T1 和 T2 由相同的键分桶/排序到相同数量的桶中.</p>
<p>hive.optimize.bucketmapjoin<br>FALSE<br>是否尝试bucket mapjoin</p>
<p>hive.optimize.bucketmapjoin.sortedmerge<br>FALSE<br>是否尝试 sorted bucket merge map join</p>
<p>hive.optimize.constant.propagation<br>TRUE<br>是否启用常量传播优化器</p>
<p><font color="#dd0000">hive.optimize.correlation</font><br>FALSE<br>利用查询内的相关性.</p>
<p>hive.optimize.cte.materialize.threshold<br>-1<br>如果对 CTE 子句的引用数量超过此阈值,Hive 将在执行主查询块之前将其具体化.-1 将禁用此功能.</p>
<p>hive.optimize.distinct.rewrite<br>TRUE<br>在适用时,此优化会将不同的聚合从单阶段重写为多阶段聚合.这可能不是在所有情况下都是最佳的.理想情况下,是否触发它应该是基于成本的决定.在 Hive 为此正式确定成本模型之前,这是配置驱动的.</p>
<p>hive.optimize.dynamic.partition.hashjoin<br>FALSE<br>是否启用动态分区哈希连接优化.此设置还取决于启用 hive.auto.convert.join</p>
<p>hive.optimize.filter.stats.reduction<br>FALSE<br>是否使用列统计信息简化过滤器运算符中的比较表达式</p>
<p>hive.optimize.groupby<br>TRUE<br>是否通过分桶分区/表启用分桶分组.</p>
<p>hive.optimize.index.autoupdate<br>FALSE<br>是否自动更新过时的索引</p>
<p>hive.optimize.index.filter<br>FALSE<br>是否启用自动使用索引</p>
<p>hive.optimize.index.filter.compact.maxsize<br>-1<br>自动使用紧凑索引的输入的最大大小(以字节为单位).负数相当于无穷大.</p>
<p>hive.optimize.index.filter.compact.minsize<br>5368709120<br>自动使用紧凑索引的输入的最小大小(以字节为单位).</p>
<p>hive.optimize.index.groupby<br>FALSE<br>是否启用使用聚合索引优化分组查询.</p>
<p>hive.optimize.limittranspose<br>FALSE<br>是否通过左/右外连接或联合来推动限制.如果该值为 true 并且外部输入的大小已足够减小(如 hive.optimize.limittranspose.reduction 中指定),则将限制推送到外部输入或联合;为了保持语义正确,限制也保持在连接或联合之上.</p>
<p>hive.optimize.limittranspose.reductionpercentage<br>1<br>当 hive.optimize.limittranspose 为 true 时,此变量指定连接的外部输入或联合输入的大小的最小减少量,我们应该获得以应用规则.</p>
<p>hive.optimize.limittranspose.reductiontuples<br>0<br>当 hive.optimize.limittranspose 为 true 时,此变量指定连接的外部输入或联合输入的元组数量的最小减少量,您应该获得以应用规则.</p>
<p>hive.optimize.listbucketing<br>FALSE<br>启用列表分桶优化器.默认值为 false,因此我们默认禁用它.</p>
<p>hive.optimize.metadataonly<br>TRUE<br>无</p>
<p>hive.optimize.null.scan<br>TRUE<br>不要扫描保证不会生成任何行的关系</p>
<p>hive.optimize.partition.columns.separate<br>TRUE<br>从 IN 子句中提取分区列</p>
<p>hive.optimize.point.lookup<br>TRUE<br>是否将 Filter 运算符中的 OR 子句转换为 IN 子句</p>
<p>hive.optimize.point.lookup.min<br>31<br>转换为 IN 子句所需的最小 OR 子句数</p>
<p>hive.optimize.ppd<br>TRUE<br>是否开启谓词下推</p>
<p>hive.optimize.ppd.storage<br>TRUE<br>是否将谓词下推到存储处理程序</p>
<p>hive.optimize.ppd.windowing<br>TRUE<br>是否通过窗口启用谓词下推</p>
<p>hive.optimize.reducededuplication<br>TRUE<br>如果数据已经由需要再次使用的相同键聚类,则删除额外的 map-reduce 作业.这应始终设置为 true.由于它是一项新功能,因此已可配置.</p>
<p>hive.optimize.reducededuplication.min.reducer<br>4<br>减少重复数据删除通过将子 RS 的 key/parts/reducer-num 移动到父 RS 来合并两个 RS.这意味着如果子 RS 的 reducer-num 是固定的(order by 或强制分桶)并且很小,它可能会变得非常慢,单个 MR.如果 reducer 的数量小于指定值,优化将自动禁用.</p>
<p>hive.optimize.remove.identity.project<br>TRUE<br>从操作员树中删除身份项目</p>
<p>hive.optimize.sampling.orderby<br>FALSE<br>对 order-by 子句使用采样以进行并行执行.</p>
<p>hive.optimize.sampling.orderby.number<br>1000<br>要获得的样本总数.</p>
<p>hive.optimize.sampling.orderby.percent<br>0.1<br>期望值介于 0.0f 和 1.0f 之间.选择行的概率.</p>
<p>hive.optimize.skewjoin<br>FALSE<br>是否开启skew join优化.算法如下:在运行时,检测skew大的key.无需处理这些密钥,而是将它们临时存储在 HDFS 目录中.在后续的 map-reduce 作业中,处理那些倾斜的键.不需要为所有表倾斜相同的键,因此,后续的 map-reduce 作业(对于倾斜的键)会快得多,因为它将是一个 map-join.</p>
<p>hive.optimize.skewjoin.compiletime<br>FALSE<br>是否为联接中的表的倾斜键创建单独的计划.这基于存储在元数据中的倾斜键.在编译时,计划被分成不同的连接:一个用于倾斜键,另一个用于剩余键.然后,对上面生成的 2 个连接执行并集.因此,除非两个连接表中存在相同的倾斜键,否则倾斜键的连接将作为映射端连接执行.此参数与 hive.optimize.skewjoin 的主要区别在于此参数使用倾斜信息存储在metastore中,在编译时自己优化计划.如果元数据中没有skew信息,这个参数不会有任何影响.hive.optimize.skewjoin.compiletime和hive.optimize.skewjoin都应该设置为true .理想情况下,hive.optimize.</p>
<p>hive.optimize.sort.dynamic.partition<br>FALSE<br>启用动态分区后,列将全局排序.这样我们可以为 reducer 中的每个分区值只保持一个记录写入器打开,从而减少 reducer 的内存压力.</p>
<p>hive.optimize.union.remove<br>FALSE<br>是否删除联合并在联合和联合上方的文件接收器之间推送运算符.这避免了联合对输出的额外扫描.这对于联合查询是独立有用的,并且在 hive.optimize.skewjoin.compiletime 设置为 true 时特别有用,因为插入了一个额外的联合.如果 hive.merge.mapfiles 或 hive.merge.mapredfiles 是,则触发合并设置为 true.如果用户将 hive.merge.mapfiles 设置为 true 并将 hive.merge.mapredfiles 设置为 false,则想法是 reducer 的数量很少,因此文件数量无论如何都很少.但是,通过这种优化,我们可能会大幅增加文件数量.因此,我们积极合并.</p>
<h3 id="orc"><a href="#orc" class="headerlink" title="orc"></a>orc</h3><p>hive.orc.cache.stripe.details.size<br>10000<br>用于在客户端缓存有关兽人拆分的元信息的最大缓存大小.</p>
<p>hive.orc.cache.use.soft.references<br>FALSE<br>默认情况下,ORC 输入格式用于存储 orc 文件页脚的缓存对缓存对象使用硬引用.将此设置为 true 可以帮助避免内存压力下的内存不足问题(在某些情况下),但代价是整体查询性能略有不可预测性.</p>
<p>hive.orc.compute.splits.num.threads<br>10<br>orc 应该使用多少线程来并行创建拆分.</p>
<p>hive.orc.row.index.stride.dictionary.check<br>TRUE<br>如果启用字典检查将在第一行索引步长(默认 10000 行)之后发生,否则字典检查将在写入第一个条带之前发生.在这两种情况下,使用或不使用字典的决定将在此后保留.</p>
<p>hive.orc.splits.allow.synthetic.fileid<br>TRUE<br>允许在没有本机文件系统的文件系统上拆分合成文件 ID.</p>
<p>hive.orc.splits.directory.batch.ms<br>0<br>在 ORC 拆分生成期间,等待批处理输入目录以进行处理的时间(以毫秒为单位).0 表示单独处理目录.如果使用 Metastore 元数据缓存,这可能会增加 Metastore 调用的数量.</p>
<p>hive.orc.splits.include.file.footer<br>FALSE<br>如果打开,由 orc 生成的拆分将在文件中包含有关条带的元数据.该数据被远程读取(从客户端或 HS2 机器)并发送到所有任务.</p>
<p>hive.orc.splits.include.fileid<br>TRUE<br>在支持它的文件系统上的拆分中包含文件 ID.</p>
<p>hive.orc.splits.ms.footer.cache.enabled<br>FALSE<br>是否为 ORC 文件页脚启用在 Metastore 中使用文件元数据缓存.</p>
<p>hive.orc.splits.ms.footer.cache.ppd.enabled<br>TRUE<br>是否启用文件页脚缓存 PPD(hive.orc.splits.ms.footer.cache.enabled 也必须设置为 true 才能工作).</p>
<p>hive.order.columnalignment<br>TRUE<br>标志来控制我们是否要尝试在聚合或连接等运算符中对齐列,以便我们尝试减少洗牌阶段的数量</p>
<p>hive.outerjoin.supports.filters<br>TRUE<br>无</p>
<p>hive.output.file.extension<br>未配置<br>字符串用作输出文件的文件扩展名.如果未设置,则默认为文本文件的编解码器扩展名(例如&quot;.gz&quot;),否则没有扩展名.</p>
<p>hive.parquet.timestamp.skip.conversion<br>TRUE<br>Parquet 的当前 Hive 实现将时间戳存储为 UTC,此标志允许在从其他工具读取 parquet 文件时跳过转换</p>
<p>hive.ppd.recognizetransivity<br>TRUE<br>是否在 equijoin 条件下传递复制谓词过滤器.</p>
<p>hive.ppd.remove.duplicatefilters<br>TRUE<br>在查询优化过程中,过滤器可能会在算子树中下推.如果此配置为真,则只有下推过滤器保留在算子树中,并删除原始过滤器.如果此配置为 false,则原始过滤器也将保留在原始位置的算子树中.</p>
<p>hive.prewarm.enabled<br>FALSE<br>为 Tez/Spark 启用容器预热(仅限 Hadoop 2)</p>
<p>hive.prewarm.numcontainers<br>10<br>控制要为 Tez/Spark 预热的容器数量(仅限 Hadoop 2)</p>
<h3 id="query"><a href="#query" class="headerlink" title="query"></a>query</h3><p>hive.query.id<br>未配置<br>正在执行的查询的 ID(每个会话可能有多个)</p>
<p>hive.query.name<br>未配置<br>Tez 使用此命名来设置 dag 名称.该名称将依次出现在代表已完成工作的 Tez UI 上.</p>
<p>hive.query.result.fileformat<br>SequenceFile<br>期望 [textfile, sequencefile, rcfile, llap] 之一.用于存储查询结果的默认文件格式.</p>
<p>hive.query.string<br>未配置<br>正在执行的查询(每个会话可能有多个)</p>
<p>hive.querylog.enable.plan.progress<br>TRUE<br>是否在每次检查作业进度时记录计划的进度.这些日志写入 hive.querylog.location 指定的位置</p>
<p>hive.querylog.location<br><code>$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;</code><br>Hive 运行时结构化日志文件的位置</p>
<p>hive.querylog.plan.progress.interval<br>60000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.计划的进度.如果 mapper 或 reducer 的进度有整数百分比变化,则无论此值如何,都会记录进度.实际间隔将是(此值除以 hive.exec 的值)的上限. counters.pull.interval) 乘以 hive.exec.counters.pull.interval 的值,即如果它没有除以 hive.exec.counters.pull.interval 的值,则记录的频率将低于指定的频率.这仅当 hive.querylog.enable.plan.progress 设置为 true 时才有效.</p>
<p>hive.reloadable.aux.jars.path<br>未配置<br>可以通过执行 reload 命令来更新罐子.这些 jar 可以用作创建 UDF 或 SerDe 等辅助类.</p>
<p>hive.reorder.nway.joins<br>TRUE<br>在单 n 向连接中运行表的重新排序(即:选择流表)</p>
<p>hive.repl.task.factory<br>org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory<br>可用于覆盖将用于实例化 ReplicationTask 事件的 ReplicationTaskFactory 的参数.覆盖第三方 repl 插件</p>
<p>hive.resultset.use.unique.column.names<br>TRUE<br>如果需要,通过使用表别名限定列名使列名在结果集中唯一.表别名将添加到&quot;select *&quot;类型的查询的列名中,或者如果查询显式使用表别名&quot;select r1.x..&quot;.</p>
<p>hive.rework.mapredwork<br>FALSE<br>是否应该重做映射的工作.这是 SymlinkTextInputFormat 首次引入的,用于在编译时用真实路径替换符号链接文件.</p>
<p>hive.rpc.query.plan<br>FALSE<br>是通过本地资源还是 RPC 发送查询计划</p>
<p>hive.sample.seednumber<br>0<br>用于百分比抽样的数字.通过更改此数字,用户将更改采样数据的子集.</p>
<p>hive.scratch.dir.permission<br>700<br>创建的用户特定临时目录的权限.</p>
<p>hive.scratchdir.lock<br>FALSE<br>在 scratchdir 中保存一个锁定文件以防止被 cleardanglingscratchdir 删除</p>
<h3 id="script"><a href="#script" class="headerlink" title="script"></a>script</h3><p>hive.script.auto.progress<br>FALSE<br>Hive Transform/Map/Reduce 子句是否应自动向 TaskTracker 发送进度信息,以避免任务因不活动而被终止.当脚本输出到 stderr 时,Hive 会发送进度信息.此选项无需定期生成 stderr 消息,但用户应谨慎,因为这可能会防止脚本中的无限循环被 TaskTracker 杀死.</p>
<p>hive.script.operator.env.blacklist<br>hive.txn.valid.txns,hive.script.operator.env.blacklist<br>配置文件中的逗号分隔键列表在调用脚本运算符时不转换为环境变量</p>
<p>hive.script.operator.id.env.var<br>HIVE_SCRIPT_OPERATOR_ID<br>保存用户转换函数中唯一脚本运算符 ID 的环境变量的名称(用户在查询中指定的自定义映射器/缩减器)</p>
<p>hive.script.operator.truncate.env<br>FALSE<br>将脚本运算符中外部脚本的每个环境变量截断为 20KB(以适应系统限制)</p>
<p>hive.script.recordreader<br>org.apache.hadoop.hive.ql.exec.TextRecordReader<br>用于从用户脚本读取数据的默认记录读取器.</p>
<p>hive.script.recordwriter<br>org.apache.hadoop.hive.ql.exec.TextRecordWriter<br>用于将数据写入用户脚本的默认记录编写器.</p>
<p>hive.script.serde<br>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>用于将输入数据传输到用户脚本并从用户脚本读取输出数据的默认 SerDe.</p>
<h3 id="security"><a href="#security" class="headerlink" title="security"></a>security</h3><p>hive.security.authenticator.manager<br>org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator<br>Hive 客户端身份验证器管理器类名称.用户定义的身份验证器应该实现接口 org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.</p>
<p>hive.security.authorization.createtable.group.grants<br>未配置<br>每当创建表时自动授予某些组的权限.像&quot;groupX,groupY:select;groupZ:create&quot;这样的示例将授予 groupX 和 groupY 的选择权限,并在创建新表时授予 groupZ 的创建权限.</p>
<p>hive.security.authorization.createtable.owner.grants<br>未配置<br>每当创建表时自动授予所有者的权限.像&quot;select,drop&quot;这样的示例将授予表所有者选择和删除权限.请注意,默认情况下,表的创建者无法访问表(但请参阅 HIVE-8067).</p>
<p>hive.security.authorization.createtable.role.grants<br>未配置<br>每当创建表时自动授予某些角色的权限.像&quot;roleX,roleY:select;roleZ:create&quot;这样的示例将授予roleX和roleY选择权限,并在创建新表时授予roleZ创建权限.</p>
<p>hive.security.authorization.createtable.user.grants<br>未配置<br>每当创建表时自动授予某些用户的权限.像&quot;userX,userY:select;userZ:create&quot;这样的示例将授予 userX 和 userY 的选择权限,并在创建新表时授予 userZ 的创建权限.</p>
<p>hive.security.authorization.enabled<br>FALSE<br>启用或禁用 Hive 客户端授权</p>
<p>hive.security.authorization.manager<br>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory<br>Hive 客户端授权管理器类名称.用户定义的授权类应该实现接口 org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.</p>
<p>hive.security.authorization.sqlstd.confwhitelist<br>未配置<br>逗号分隔的 Java 正则表达式列表.当启用 SQL 标准授权时,用户可以修改与这些正则表达式匹配的配置参数.要获取默认值,请使用&quot;set <param>&quot; 命令.请注意,在白名单检查之后仍会执行 hive.conf.restricted.list 检查</p>
<p>hive.security.authorization.sqlstd.confwhitelist.append<br>未配置<br>逗号分隔的 Java 正则表达式列表,附加到 hive.security.authorization.sqlstd.confwhitelist 中的列表集.使用此列表而不是更新原始列表意味着您可以附加到 SQL 标准授权设置的默认值,而不是完全替换它.</p>
<p>hive.security.authorization.task.factory<br>org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl<br>授权DDL任务工厂实现</p>
<p>hive.security.command.whitelist<br>set,reset,dfs,add,list,delete,reload,compile<br>用户有权执行的非 SQL Hive 命令的逗号分隔列表</p>
<p>hive.security.metastore.authenticator.manager<br>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator<br>在元存储中用于身份验证的身份验证器管理器类名称.用户定义的身份验证器应实现接口 org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.</p>
<p>hive.security.metastore.authorization.auth.reads<br>TRUE<br>如果这是真的,元存储授权器授权对数据库/表的读取操作</p>
<p>hive.security.metastore.authorization.manager<br>org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider<br>要在 Metastore 中用于授权的授权管理器类的名称(逗号分隔).用户定义的授权类应实现接口 org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.所有授权管理器类必须成功授权 Metastore API 调用才能允许执行命令.</p>
<p>hive.semantic.analyzer.hook<br>未配置<br>无</p>
<p>hive.serdes.using.metastore.for.schema<br>org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe<br>SerDes 从 Metastore 中检索模式.这是一个内部参数.</p>
<h3 id="server2"><a href="#server2" class="headerlink" title="server2"></a>server2</h3><p>hive.server.read.socket.timeout<br>10s<br>需要一个时间值,单位为 (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 sec.HiveServer 关闭如果客户端没有响应,则连接.默认情况下,10 秒.</p>
<p>hive.server.tcp.keepalive<br>TRUE<br>是否为 Hive Server 启用 TCP keepalive.Keepalive 将防止半开连接的积累.</p>
<p>hive.server2.allow.user.substitution<br>TRUE<br>允许将备用用户指定为 HiveServer2 打开连接请求的一部分.</p>
<p>hive.server2.async.exec.async.compile<br>FALSE<br>是否启用异步编译异步查询.如果启用,则在编译完成之前查询是否会有任何结果集是未知的.</p>
<p>hive.server2.async.exec.keepalive.time<br>10s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.空闲 HiveServer2 异步线程的时间(来自线程池)将在终止之前等待新任务到达</p>
<p>hive.server2.async.exec.shutdown.timeout<br>10s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.HiveServer2 关闭将等待多长时间异步线程终止.</p>
<p>hive.server2.async.exec.threads<br>100<br>HiveServer2 的异步线程池中的线程数</p>
<p>hive.server2.async.exec.wait.queue.size<br>100<br>HiveServer2 中异步线程池的等待队列大小.达到此限制后,异步线程池将拒绝新请求.</p>
<p>hive.server2.authentication<br>NONE<br>期望 [nosasl, none, ldap, kerberos, pam, custom] 之一.客户端身份验证类型.NONE:无身份验证检查 LDAP:基于 LDAP/AD 的身份验证 KERBEROS:Kerberos/GSSAPI 身份验证 CUSTOM:自定义身份验证提供程序(与属性 hive 一起使用. server2.custom.authentication.class)PAM:可插入身份验证模块 NOSASL:原始传输</p>
<p>hive.server2.authentication.kerberos.keytab<br>未配置<br>服务器主体的 Kerberos 密钥表文件</p>
<p>hive.server2.authentication.kerberos.principal<br>未配置<br>Kerberos 服务器主体</p>
<p>hive.server2.authentication.ldap.baseDN<br>未配置<br>LDAP 基本 DN</p>
<p>hive.server2.authentication.ldap.customLDAPQuery<br>未配置<br>LDAP Atn 提供程序用于对 LDAP 服务器执行的完整 LDAP 查询.如果此查询返回空结果集,则 LDAP 提供程序将失败身份验证请求,如果用户是结果集的一部分,则成功.例如:(&amp;(objectClass= group)(objectClass=top)(instanceType=4)(cn=Domain*))(&amp;(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC= domain,DC=com)(memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com))))</p>
<p>hive.server2.authentication.ldap.Domain<br>未配置<br>无</p>
<p>hive.server2.authentication.ldap.groupClassKey<br>groupOfNames<br>要在 LDAP 组搜索中使用的组条目上的 LDAP 属性名称.例如:group/groupOfNames 或 groupOfUniqueNames.</p>
<p>hive.server2.authentication.ldap.groupDNPattern<br>未配置<br>用冒号分隔的模式列表,用于在此目录中查找组实体的 DN.在要替换实际组名的位置使用 %s.例如:CN=%s,CN=Groups,DC=subdomain,DC=域,DC = com.</p>
<p>hive.server2.authentication.ldap.groupFilter<br>未配置<br>LDAP 组名称的逗号分隔列表(短名称不是完整的 DN).例如:HiveAdmins/HadoopAdmins/Administrators</p>
<p>hive.server2.authentication.ldap.groupMembershipKey<br>member<br>引用组的用户条目上的 LDAP 属性名称,用户所属.例如:member/uniqueMember 或 memberUid</p>
<p>hive.server2.authentication.ldap.guidKey<br>uid<br>LDAP 属性名称,其值在此 LDAP 服务器中是唯一的.例如:uid 或 CN.</p>
<p>hive.server2.authentication.ldap.url<br>未配置<br>LDAP 连接 URL,此值可以包含用于 HA 的多个 LDAP 服务器实例的 URL,每个 LDAP URL 由空格字符分隔.URL 按照指定的顺序使用,直到连接成功.</p>
<p>hive.server2.authentication.ldap.userDNPattern<br>未配置<br>用冒号分隔的模式列表,用于在此目录中查找用户的 DN.在要替换实际组名的地方使用 %s.例如:CN=%s,CN=Users,DC=subdomain,DC=domain ,DC=com.</p>
<p>hive.server2.authentication.ldap.userFilter<br>未配置<br>LDAP 用户名的逗号分隔列表(只是短名称,而不是完整的 DN).例如:hiveuser/impalauser/hiveadmin/hadoopadmin</p>
<p>hive.server2.authentication.pam.services<br>未配置<br>auth type 为 PAM 时应使用的底层 pam 服务列表 /etc/pam.d 中必须存在同名文件</p>
<p>hive.server2.authentication.spnego.keytab<br>未配置<br>SPNego 主体的 keytab 文件,可选,典型值类似于 /etc/security/keytabs/spnego.service.keytab,当启用 Kerberos 安全性并使用 HTTP 传输模式时,HiveServer2 将使用此 keytab.只需设置如果要在身份验证中使用 SPNEGO.仅当指定了有效的 hive.server2.authentication.spnego.principal 和 hive.server2.authentication.spnego.keytab 时,才会接受 SPNEGO 身份验证.</p>
<p>hive.server2.authentication.spnego.principal<br>未配置<br>SPNego 服务主体,可选,典型值类似于 HTTP/<a href="mailto:&#95;&#x48;&#x4f;&#83;&#84;&#64;&#x45;&#x58;&#x41;&#77;&#80;&#76;&#x45;&#46;&#x43;&#79;&#77;">&#95;&#x48;&#x4f;&#83;&#84;&#64;&#x45;&#x58;&#x41;&#77;&#80;&#76;&#x45;&#46;&#x43;&#79;&#77;</a> 当启用 Kerberos 安全性并使用 HTTP 传输模式时,HiveServer2 将使用 SPNego 服务主体.仅当使用 SPNEGO 时才需要设置在认证中.</p>
<p>hive.server2.builtin.udf.blacklist<br>未配置<br>逗号分隔的 udfs 名称列表.查询中不允许使用这些 udf.udf 黑名单优先于 udf 白名单</p>
<p>hive.server2.builtin.udf.whitelist<br>未配置<br>查询中允许的内置 udf 名称的逗号分隔列表.空的白名单允许执行所有内置 udf.udf 黑名单优先于 udf 白名单</p>
<p>hive.server2.close.session.on.disconnect<br>TRUE<br>连接关闭时会话将关闭.将此设置为 false 以使会话超过其父连接.</p>
<p>hive.server2.compile.lock.timeout<br>0s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.请求将等待的秒数在放弃之前获取编译锁.将其设置为 0 会禁用超时.</p>
<p>hive.server2.custom.authentication.class<br>未配置<br>自定义身份验证类.当属性&quot;hive.server2.authentication&quot;设置为&quot;CUSTOM&quot;时使用.提供的类必须是接口 org.apache.hive.service.auth.PasswdAuthenticationProvider 的正确实现.HiveServer2 将调用其 Authenticate(user,passed) 方法对请求进行身份验证.实现可以选择实现 Hadoop 的 org.apache.hadoop.conf.Configurable 类来获取 Hive 的 Configuration 对象.</p>
<p>hive.server2.enable.doAs<br>TRUE<br>将此属性设置为 true 将使 HiveServer2 在用户调用它时执行 Hive 操作.</p>
<p>hive.server2.global.init.file.location<br>${env:HIVE_CONF_DIR}<br>HS2 全局初始化文件的位置或包含 .hiverc 文件的目录.如果设置了该属性,则该值必须是指向 init 文件或 init 文件所在目录的有效路径.</p>
<p>hive.server2.idle.operation.timeout<br>5d<br>期望一个时间值,单位为(d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 msec.未指定时将关闭操作在这段时间内访问,可以通过设置为零值禁用.使用正值,仅检查终端状态下的操作(完成,取消,关闭,错误).使用负值,检查所有操作不论状态.</p>
<p>hive.server2.idle.session.check.operation<br>TRUE<br>只有在没有活动,并且没有挂起的操作时,会话才会被认为是空闲的.此设置只有在会话空闲超时(hive.server2.idle.session.timeout)和检查(hive.server2.session. check.interval) 已启用.</p>
<p>hive.server2.idle.session.timeout<br>7d<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.未指定时将关闭会话在此持续时间内访问,可以通过设置为零或负值来禁用.</p>
<p>hive.server2.keystore.password<br>未配置<br>SSL 证书密钥库密码.</p>
<p>hive.server2.keystore.path<br>未配置<br>SSL 证书密钥库位置.</p>
<p>hive.server2.llap.concurrent.queries<br>-1<br>通过 llap 并行允许的查询数.负数意味着&quot;无限&quot;.</p>
<p>hive.server2.logging.operation.enabled<br>TRUE<br>当为 true 时,HS2 将保存操作日志并提供给客户端</p>
<p>hive.server2.logging.operation.level<br>EXECUTION<br>期望 <code>[none, execution, performance, verbose]</code> 之一.HS2 操作日志记录模式可用于客户端设置在会话级别.为此,hive.server2.logging.operation.enabled 应设置为 true.NONE:忽略任何日志记录 执行:记录任务的完成情况 性能:执行 + 性能日志 VERBOSE:所有日志</p>
<p>hive.server2.logging.operation.log.location<br><code>$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs</code><br>如果启用了日志记录功能,则存储操作日志的顶级目录</p>
<p>hive.server2.long.polling.timeout<br>5000ms<br>需要一个时间值,单位为 (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 msec.HiveServer2 在响应之前将等待的时间到使用长轮询的异步调用</p>
<p>hive.server2.map.fair.scheduler.queue<br>TRUE<br>如果配置了 YARN 公平调度器并且 HiveServer2 以非模拟模式运行,则此设置确定公平调度器队列映射的用户.如果设置为 true(默认),则登录用户确定提交作业的公平调度器队列,以便用户可以跟踪 map reduce 资源使用情况.如果设置为 false,则所有 Hive 作业都会进入&quot;hive&quot;用户的队列.</p>
<p>hive.server2.max.start.attempts<br>30<br>期望值大于 0.HiveServer2 在退出之前尝试启动的次数,在重试之间休眠 60 秒.默认值 30 将持续尝试 30 分钟.</p>
<p>hive.server2.metrics.enabled<br>FALSE<br>在 HiveServer2 上启用指标.</p>
<p>hive.server2.parallel.ops.in.session<br>TRUE<br>是否允许在一个会话中进行多个并行操作(例如 SQL 语句).</p>
<p>hive.server2.session.check.interval<br>6h<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.时间应大于或等于 3000 毫秒.会话/操作超时的检查间隔,可以通过设置为零或负值来禁用.</p>
<p>hive.server2.session.hook<br>未配置<br>无</p>
<p><font color="#dd0000">hive.server2.support.dynamic.service.discovery</font><br>FALSE<br>HiveServer2 是否支持其客户端的动态服务发现.为了支持这一点,HiveServer2 的每个实例当前都使用 ZooKeeper 来注册自己,当它启动时.JDBC/ODBC 客户端应在其连接字符串中使用 ZooKeeper 集合:hive.zookeeper.quorum.</p>
<p>hive.server2.table.type.mapping<br>CLASSIC<br>期望 <code>[classic, hive]</code> 之一.此设置反映 HiveServer2 将如何报告用于检索可用表和支持的表类型的 JDBC 和其他客户端实现的表类型 HIVE:公开 Hive 的本机表类型,如 MANAGED_TABLE/EXTERNAL_TABLE/VIRTUAL_VIEW CLASSIC:更多泛型类型,如 TABLE 和 VIEW</p>
<p>hive.server2.tez.default.queues<br>未配置<br>同名YARN队列对应的逗号分隔值列表.当HiveServer2以Tez模式启动时,需要设置此配置以使多个Tez会话在集群上并行运行.</p>
<p>hive.server2.tez.initialize.default.sessions<br>FALSE<br>此标志在 HiveServer2 中用于使用户能够使用 HiveServer2 而无需为 HiveServer2 开启 Tez.用户可能希望在没有会话池的情况下通过 Tez 运行查询.</p>
<p>hive.server2.tez.session.lifetime<br>162h<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为小时.启动的 Tez 会话的生命周期启用默认会话时由 HS2 设置.设置为 0 以禁用会话到期.</p>
<p>hive.server2.tez.session.lifetime.jitter<br>3h<br>期望一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为小时.Tez 会话生命周期的抖动;防止所有会话一次重新启动.</p>
<p>hive.server2.tez.sessions.init.threads<br>16<br>如果启用了 hive.server2.tez.initialize.default.sessions,则用于初始化默认会话的最大线程数.</p>
<p>hive.server2.tez.sessions.per.default.queue<br>1<br>一个正整数,确定应在&quot;hive.server2.tez.default.queues&quot;指定的每个队列上启动的 Tez 会话数.确定每个队列上的并行度.</p>
<p><font color="#dd0000">hive.server2.thrift.bind.host</font><br>未配置<br>绑定运行 HiveServer2 Thrift 服务的主机.</p>
<p>hive.server2.thrift.client.connect.retry.limit<br>1<br>打开与 HiveServe2 的连接时的重试次数</p>
<p>hive.server2.thrift.client.password<br>anonymous<br>用于节俭客户端的密码</p>
<p>hive.server2.thrift.client.retry.delay.seconds<br>1s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.HiveServer2 节俭的秒数客户端在连续的连接尝试之间等待.还指定在失败时重试 thrift 调用之间的等待时间</p>
<p>hive.server2.thrift.client.retry.limit<br>1<br>Thrift HiveServer2 调用失败时的重试次数</p>
<p>hive.server2.thrift.client.user<br>anonymous<br>用于节俭客户端的用户名</p>
<p>hive.server2.thrift.exponential.backoff.slot.length<br>100ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.Thrift 的二进制指数退避时隙时间客户端在登录 HiveServer2 期间重试,直到遇到 Thrift 客户端超时</p>
<p>hive.server2.thrift.http.cookie.auth.enabled<br>TRUE<br>当为 true 时,HiveServer2 在 HTTP 传输模式下,将使用基于 cookie 的身份验证机制.</p>
<p>hive.server2.thrift.http.cookie.domain<br>未配置<br>HS2 生成的 cookie 的域</p>
<p>hive.server2.thrift.http.cookie.is.httponly<br>TRUE<br>HS2 生成的 cookie 的 HttpOnly 属性.</p>
<p>hive.server2.thrift.http.cookie.is.secure<br>TRUE<br>HS2 生成的 cookie 的安全属性.</p>
<p>hive.server2.thrift.http.cookie.max.age<br>86400s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.服务器端的最大使用时间(以秒为单位) HS2 在 HTTP 模式下使用的 cookie.</p>
<p>hive.server2.thrift.http.cookie.path<br>未配置<br>HS2 生成的 cookie 的路径</p>
<p>hive.server2.thrift.http.max.idle.time<br>1800s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.连接的最大空闲时间处于 HTTP 模式时的服务器.</p>
<p>hive.server2.thrift.http.path<br>cliservice<br>在 HTTP 模式下 URL 端点的路径组件.</p>
<p>hive.server2.thrift.http.port<br>10001<br>当 hive.server2.transport.mode 为 &#39;http&#39; 时 HiveServer2 Thrift 接口的端口号.</p>
<p>hive.server2.thrift.http.request.header.size<br>6144<br>使用 HTTP 传输模式时的请求标头大小(以字节为单位).使用 Jetty 默认值.</p>
<p>hive.server2.thrift.http.response.header.size<br>6144<br>使用 HTTP 传输模式时的响应标头大小(以字节为单位).使用 Jetty 默认值.</p>
<p>hive.server2.thrift.http.worker.keepalive.time<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.空闲 http worker 的 Keepalive 时间线.当 worker 的数量超过 min worker 时,在此时间间隔后会杀死过多的线程.</p>
<p>hive.server2.thrift.login.timeout<br>20s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.Thrift 客户端在登录期间超时蜂巢服务器2</p>
<p>hive.server2.thrift.max.message.size<br>104857600<br>HS2 服务器将接受的最大消息大小(以字节为单位).</p>
<p>hive.server2.thrift.max.worker.threads<br>500<br>Thrift 工作线程的最大数量</p>
<p>hive.server2.thrift.min.worker.threads<br>5<br>Thrift 工作线程的最小数量</p>
<p>hive.server2.thrift.port<br>10000<br>当 hive.server2.transport.mode 为 &#39;binary&#39; 时 HiveServer2 Thrift 接口的端口号.</p>
<p>hive.server2.thrift.resultset.max.fetch.size<br>1000<br>服务器在一次 Fetch RPC 调用中向客户端发送的最大行数.</p>
<p>hive.server2.thrift.resultset.serialize.in.tasks<br>FALSE<br>我们是否应该在任务节点中序列化 JDBC ResultSet RPC 中使用的 Thrift 结构.如果是,我们使用 SequenceFile 和 ThriftJDBCBinarySerDe 来读取和写入最终结果.</p>
<p>hive.server2.thrift.sasl.qop<br>auth<br>期望 [auth, auth-int, auth-conf].Sasl QOP 值之一;将其设置为以下值之一,为 HiveServer2 与客户端的通信启用更高级别的保护.在大多数情况下,将 hadoop.rpc.protection 设置为比 HiveServer2 更高的级别没有意义.HiveServer2 忽略 hadoop.rpc.protection 支持 hive.server2.thrift.sasl.qop.&quot;auth&quot; - 仅身份验证(默认)&quot;auth-int&quot; - 身份验证和完整性保护 &quot;auth-conf&quot; - 身份验证以及完整性和机密性protection 仅当 HiveServer2 配置为使用 Kerberos 身份验证时才适用.</p>
<p>hive.server2.thrift.worker.keepalive.time<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.Keepalive 时间(以秒为单位)一个空闲的工作线程.当 worker 的数量超过 min worker 时,在此时间间隔后会杀死过多的线程.</p>
<p>hive.server2.transport.mode<br>binary<br>期望 HiveServer2 的 <code>[binary, http]</code>.Transport 模式之一.</p>
<p>hive.server2.use.SSL<br>FALSE<br>将此设置为 true 以在 HiveServer2 中使用 SSL 加密.</p>
<p><font color="#dd0000">hive.server2.webui.host</font><br>0.0.0.0<br>HiveServer2 WebUI 将侦听的主机地址</p>
<p>hive.server2.webui.keystore.password<br>未配置<br>HiveServer2 WebUI 的 SSL 证书密钥库密码.</p>
<p>hive.server2.webui.keystore.path<br>未配置<br>HiveServer2 WebUI 的 SSL 证书密钥库位置.</p>
<p>hive.server2.webui.max.historic.queries<br>25<br>在 HiverSever2 WebUI 中显示的过去查询的最大数量.</p>
<p>hive.server2.webui.max.threads<br>50<br>最大 HiveServer2 WebUI 线程数</p>
<p>hive.server2.webui.port<br>10002<br>HiveServer2 WebUI 将侦听的端口.这可以设置为 0 或负整数以禁用 Web UI</p>
<p>hive.server2.webui.spnego.keytab<br>未配置<br>包含 HiveServer2 WebUI SPNEGO 服务主体的 Kerberos Keytab 文件的路径.</p>
<p>hive.server2.webui.spnego.principal<br>HTTP/<code>_HOST@EXAMPLE.COM</code><br>HiveServer2 WebUI SPNEGO 服务主体.特殊字符串 <code>_HOST</code> 将自动替换为 hive.server2.webui.host 的值或正确的主机名.</p>
<p>hive.server2.webui.use.spnego<br>FALSE<br>如果为 true,则 HiveServer2 WebUI 将使用 SPNEGO 保护.客户端必须使用 Kerberos 进行身份验证.</p>
<p>hive.server2.webui.use.ssl<br>FALSE<br>将此设置为 true 以对 HiveServer2 WebUI 使用 SSL 加密.</p>
<p>hive.server2.xsrf.filter.enabled<br>FALSE<br>如果启用,如果 X-XSRF-HEADER 标头不存在,HiveServer2 将阻止通过 http 对其发出的任何请求</p>
<p><font color="#dd0000">hive.server2.zookeeper.namespace</font><br>hiveserver2<br>HiveServer2 在支持动态服务发现时使用的 ZooKeeper 中的父节点.</p>
<p>hive.server2.zookeeper.publish.configs<br>TRUE<br>我们是否应该将 HiveServer2 的配置发布到 ZooKeeper.</p>
<h3 id="service"><a href="#service" class="headerlink" title="service"></a>service</h3><p>hive.service.metrics.class<br>org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics<br>期望 <code>[org.apache.hadoop.hive.common.metrics.metrics2.codahalemetrics, org.apache.hadoop.hive.common.metrics.legacymetrics]</code>.Hive 度量子系统实现类之一.</p>
<p>hive.service.metrics.file.frequency<br>5s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.对于度量类 org.apache. hadoop.hive.common.metrics.metrics2.CodahaleMetrics JSON_FILE 报告器,更新 JSON 指标文件的频率.</p>
<p>hive.service.metrics.file.location<br>/tmp/report.json<br>对于度量类 org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics JSON_FILE 报告器,本地 JSON 度量文件的位置.该文件将在每个时间间隔被覆盖.</p>
<p>hive.service.metrics.hadoop2.component<br>hive<br>提供给 Hadoop2 Metrics 系统的组件名称.理想情况下,MetaStore 为&quot;hivemetastore&quot;,HiveServer2 为&quot;hiveserver2&quot;.</p>
<p>hive.service.metrics.hadoop2.frequency<br>30s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.对于度量类 org.apache. hadoop.hive.common.metrics.metrics2.CodahaleMetrics HADOOP2 报告者,更新 HADOOP2 指标系统的频率.</p>
<p>hive.service.metrics.reporter<br>JSON_FILE, JMX<br>指标类 org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics 的报告类型,以逗号分隔的 JMX/CONSOLE/JSON_FILE/HADOOP2 列表</p>
<p>hive.session.history.enabled<br>FALSE<br>是否记录 Hive 查询/查询计划/运行时统计信息等.</p>
<p>hive.session.id<br>未配置<br>无</p>
<p>hive.session.impl.classname<br>未配置<br>Hive 会话的自定义实现的类名</p>
<p>hive.session.impl.withugi.classname<br>未配置<br>使用 UGI 自定义实现 hive 会话的类名</p>
<p>hive.session.silent<br>FALSE<br>无</p>
<p>hive.skewjoin.key<br>100000<br>确定我们是否在 join 中得到一个倾斜键.如果我们在连接运算符中看到超过指定数量的具有相同键的行,我们认为该键是一个倾斜连接键.</p>
<p>hive.skewjoin.mapjoin.map.tasks<br>10000<br>确定用于倾斜连接的后续地图连接作业中使用的地图任务数量.它应与 hive.skewjoin.mapjoin.min.split 一起使用以执行细粒度控制.</p>
<p>hive.skewjoin.mapjoin.min.split<br>33554432<br>通过指定最小拆分大小来确定在后续映射连接作业中最多使用的映射任务数以进行倾斜连接.它应该与 hive.skewjoin.mapjoin.map.tasks 一起使用以执行细粒度控制.</p>
<p>hive.smbjoin.cache.rows<br>10000<br>每个 smb 连接表应在内存中缓存多少具有相同键值的行.</p>
<h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><p>hive.spark.client.channel.log.level<br>未配置<br>远程 Spark 驱动程序的通道日志记录级别.{DEBUG, ERROR, INFO, TRACE, WARN} 之一.</p>
<p>hive.spark.client.connect.timeout<br>1000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.远程 Spark 驱动程序连接超时返回 Hive 客户端.</p>
<p>hive.spark.client.future.timeout<br>60s<br>需要一个时间值,单位为 (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 sec.Hive 客户端请求的超时时间远程 Spark 驱动程序.</p>
<p>hive.spark.client.rpc.max.size<br>52428800<br>Hive 客户端和远程 Spark 驱动程序之间通信的最大消息大小(以字节为单位).默认为 50MB.</p>
<p>hive.spark.client.rpc.sasl.mechanisms<br>DIGEST-MD5<br>用于身份验证的 SASL 机制的名称.</p>
<p>hive.spark.client.rpc.server.address<br>未配置<br>HiverServer2 主机的服务器地址,用于 Hive 客户端和远程 Spark 驱动程序之间的通信.默认为空,这意味着地址将以与 hive.server2.thrift.bind.host 相同的方式确定.这仅在主机具有多个网络地址且与 hive.server2 不同的网络地址时才需要. thrift.bind.host 将被使用.</p>
<p>hive.spark.client.rpc.threads<br>8<br>远程 Spark 驱动程序的 RPC 事件循环的最大线程数.</p>
<p>hive.spark.client.secret.bits<br>256<br>生成的密钥中用于 Hive 客户端和远程 Spark 驱动程序之间通信的随机位数.向下舍入到最接近的 8 倍数.</p>
<p>hive.spark.client.server.connect.timeout<br>90000ms<br>需要一个时间值,单位为 (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec),如果未指定,则为 msec.Hive 客户端与客户端之间的握手超时远程 Spark 驱动程序.由两个进程检查.</p>
<p>hive.spark.dynamic.partition.pruning<br>FALSE<br>启用动态修剪后,分区键上的连接将通过写入临时 HDFS 文件来处理,稍后再读取以删除不必要的分区.</p>
<p>hive.spark.dynamic.partition.pruning.max.data.size<br>104857600<br>动态修剪中的最大总数据大小.</p>
<p>hive.spark.job.monitor.timeout<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.作业监视器获取 Spark 的超时工作状态.</p>
<p>hive.ssl.protocol.blacklist<br>SSLv2,SSLv3<br>为所有 Hive 服务器禁用的 SSL 版本</p>
<p>hive.stageid.rearrange<br>none<br>期望 <code>[none, idonly, traverse, execution]</code> 之一.</p>
<p>hive.start.cleanup.scratchdir<br>FALSE<br>启动 Hive 服务器时清理 Hive 暂存目录</p>
<h3 id="stats"><a href="#stats" class="headerlink" title="stats"></a>stats</h3><p>hive.stats.atomic<br>FALSE<br>是否仅在所有统计信息都可用时更新元存储统计信息</p>
<p>hive.stats.autogather<br>TRUE<br>在 INSERT OVERWRITE 命令期间自动收集统计信息(仅基本)的标志.</p>
<p>hive.stats.collect.scancols<br>FALSE<br>是否在 QueryPlan 中跟踪列访问.这有助于确定表的访问方式以及确定是否存在可以修剪的浪费列.</p>
<p>hive.stats.collect.tablekeys<br>FALSE<br>是否在 QueryPlan 中派生和维护表上的 join 和 group by 键.这对于确定如何访问表并确定是否应该对它们进行存储很有用.</p>
<p>hive.stats.column.autogather<br>FALSE<br>自动收集列统计信息的标志.</p>
<p>hive.stats.dbclass<br>fs<br>期望 <code>[custom, fs]</code> 中的模式之一.存储临时 Hive 统计信息的存储.在基于文件系统的统计信息收集 (&#39;fs&#39;) 中,每个任务将其收集的统计信息写入文件系统上的文件中,这些信息将在作业完成后汇总.支持的值是 StatsSetupConst.java 中定义的 fs(文件系统)和 custom.</p>
<p>hive.stats.default.aggregator<br>未配置<br>如果 hive.stats.dbclass 是自定义类型,则默认使用的 Java 类(实现 StatsAggregator 接口).</p>
<p>hive.stats.default.publisher<br>未配置<br>如果 hive.stats.dbclass 是自定义类型,则默认使用的 Java 类(实现 StatsPublisher 接口).</p>
<p>hive.stats.deserialization.factor<br>1<br>Hive/Tez 优化器估计流经每个算子的数据大小.在没有行数和数据大小等基本统计数据的情况下,文件大小用于估计行数和数据大小.由于表/分区中的文件是序列化的(并且可以选择压缩),因此无法可靠地确定行数和数据大小的估计值.这个因素乘以文件大小来解释序列化和压缩.</p>
<p>hive.stats.fetch.column.stats<br>FALSE<br>带有统计信息的运算符树的注释需要列统计信息.列统计信息是从元存储中获取的.当列数很高时,为每个需要的列获取列统计信息可能会很昂贵.此标志可用于禁用从 Metastore 获取列统计信息.</p>
<p>hive.stats.fetch.partition.stats<br>TRUE<br>使用统计信息注释算子树需要分区级别的基本统计信息,如行数/数据大小和文件大小.分区统计信息是从 Metastore 中获取的.当分区数量很大时,为每个需要的分区获取分区统计信息可能会很昂贵.此标志可用于禁用从 Metastore 获取分区统计信息.当禁用此标志时,Hive 将调用文件系统以获取文件大小,并将估计行模式中的行数.</p>
<p>hive.stats.filter.in.factor<br>1<br>目前假设列分布是均匀的.这可能导致某个运算符过滤的行数被高估/低估,这反过来又可能导致资源的过度供应或供应不足.该因子应用于过滤运算符中 IN 子句的基数估计.</p>
<p>hive.stats.gather.num.threads<br>10<br>partialscan/noscan analyze 命令对分区表使用的线程数.这仅适用于实现 StatsProvidingRecordReader(如 ORC)的文件格式.</p>
<p>hive.stats.join.factor<br>1.1<br>Hive/Tez 优化器估计流经每个算子的数据大小.JOIN 运算符使用列统计信息来估计流出它的行数,从而估计数据大小.在没有列统计信息的情况下,这个因素决定了流出 JOIN 运算符的行数.</p>
<p>hive.stats.list.num.entries<br>10<br>为了估计流经 Hive/Tez 中运算符的数据大小(用于 reducer 估计等),平均行大小乘以每个运算符出来的总行数.平均行大小是根据所有列的平均列大小计算的在行中.在没有列统计信息和可变长度复杂列(如列表)的情况下,可以使用此配置指定条目/值的平均数量.</p>
<p>hive.stats.map.num.entries<br>10<br>为了估计流经 Hive/Tez 中运算符的数据大小(用于 reducer 估计等),平均行大小乘以每个运算符出来的总行数.平均行大小是根据所有列的平均列大小计算的在行中.在没有列统计信息和可变长度复杂列(如 map)的情况下,可以使用此配置指定条目/值的平均数量.</p>
<p>hive.stats.max.variable.length<br>100<br>为了估计流经 Hive/Tez 中运算符的数据大小(用于 reducer 估计等),平均行大小乘以每个运算符出来的总行数.平均行大小是根据所有列的平均列大小计算的在行中.在没有列统计信息的情况下,对于可变长度列(如字符串/字节等),将使用此值.对于固定长度的列,使用它们对应的 Java 等效大小(float - 4 字节,double - 8 字节等).</p>
<p>hive.stats.ndv.error<br>20<br>以百分比表示的标准误差.提供准确性和计算成本之间的折衷.错误值越低,表示准确性越高,计算成本越高.</p>
<p>hive.stats.reliable<br>FALSE<br>查询是否会因为无法完全准确地收集统计信息而失败.如果设置为 true,则从/写入分区可能会失败,因为无法准确计算统计信息.</p>
<p>hive.strict.checks.cartesian.product<br>TRUE<br>启用严格的大型查询检查不允许以下情况:笛卡尔积(交叉连接).</p>
<p>hive.strict.checks.large.query<br>FALSE<br>启用严格的大型查询检查不允许以下操作:Orderby without limit.没有为针对分区表的查询选择分区.请注意,这些检查目前不考虑数据大小,只考虑查询模式.</p>
<p>hive.strict.checks.type.safety<br>TRUE<br>启用严格的类型安全检查不允许以下操作:比较 bigint 和字符串.比较 bigint 和双精度数.</p>
<p>hive.support.concurrency<br>FALSE<br>Hive 是否支持并发控制.使用 zookeeper Hive 锁管理器时必须启动并运行 ZooKeeper 实例</p>
<p>hive.support.quoted.identifiers<br>column<br>期望 [none, column] 之一.是否使用带引号的标识符.可以使用&quot;无&quot;或&quot;列&quot;.无:默认(过去)行为.暗示只有 alphaNumeric 和下划线是 identifiers.column 中的有效字符:暗示列名可以包含任何字符.</p>
<p>hive.support.special.characters.tablename<br>TRUE<br>此标志应设置为 true 以启用对表名中特殊字符的支持.设置为 false 时,仅支持 <code>[a-zA-Z_0-9]+.</code>目前唯一支持的特殊字符是&quot;/&quot;.此标志仅适用于引用的表名.默认值为 true.</p>
<p>hive.support.sql11.reserved.keywords<br>TRUE<br>此标志应设置为 true 以启用对 SQL2011 保留关键字的支持.默认值为 true.</p>
<p>hive.table.parameters.default<br>未配置<br>新创建表的默认属性值</p>
<h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>hive.test.currenttimestamp<br>未配置<br>测试的当前时间戳</p>
<p>hive.test.dummystats.aggregator<br>未配置<br>用于测试的内部变量</p>
<p>hive.test.dummystats.publisher<br>未配置<br>用于测试的内部变量</p>
<p>hive.test.fail.compaction<br>FALSE<br>仅用于测试.将导致 CompactorMR 失败.</p>
<p>hive.test.fail.heartbeater<br>FALSE<br>仅用于测试.将导致 Heartbeater 失败.</p>
<p>hive.test.mode<br>FALSE<br>Hive 是否在测试模式下运行.如果是,它会打开采样并为输出表名添加前缀.</p>
<p>hive.test.mode.nosamplelist<br>未配置<br>在测试模式下,指定不应用采样的逗号分隔表名</p>
<p>hive.test.mode.prefix<br>test_<br>在测试模式下,指定输出表的前缀</p>
<p>hive.test.mode.samplefreq<br>32<br>在测试模式下,指定不分桶的表的采样频率,例如以下查询:INSERT OVERWRITE TABLE dest SELECT col1 from src 将转换为 INSERT OVERWRITE TABLE test_dest SELECT col1 from src TABLESAMPLE (BUCKET 1 out of 32 on兰特(1))</p>
<p>hive.test.rollbacktxn<br>FALSE<br>仅用于测试.将标记每个 ACID 事务中止</p>
<h3 id="tez"><a href="#tez" class="headerlink" title="tez"></a>tez</h3><p>hive.tez.auto.reducer.parallelism<br>FALSE<br>打开 Tez 的自动减速器并行功能.启用后,Hive 仍将估计数据大小并设置并行度估计.Tez 将对源顶点的输出大小进行采样,并根据需要在运行时调整估计值.</p>
<p>hive.tez.bucket.pruning<br>FALSE<br>启用修剪后,桶列上的过滤器将通过根据包含的桶的位集过滤拆分来处理.这需要由 hive.optimize.ppd 和 hive.optimize.index.filters 生成的谓词.</p>
<p>hive.tez.bucket.pruning.compat<br>TRUE<br>启用修剪时,处理由于负哈希码而可能导致的插入损坏.这有时会使数据扫描成本加倍,但为了安全起见默认启用</p>
<p>hive.tez.container.max.java.heap.fraction<br>0.8<br>这是为了覆盖同名的 tez 设置</p>
<p>hive.tez.container.size<br>-1<br>默认情况下,Tez 将生成映射器大小的容器.这可以用来覆盖.</p>
<p>hive.tez.cpu.vcores<br>-1<br>默认情况下,Tez 会要求每个容器配置许多 cpus map-reduce.这可用于覆盖.</p>
<p>hive.tez.dynamic.partition.pruning<br>TRUE<br>启用动态修剪后,将通过将事件从处理顶点发送到 Tez 应用程序主节点来处理分区键上的连接.这些事件将用于修剪不必要的分区.</p>
<p>hive.tez.dynamic.partition.pruning.max.data.size<br>104857600<br>动态修剪中事件的最大总数据大小.</p>
<p>hive.tez.dynamic.partition.pruning.max.event.size<br>1048576<br>处理器在动态修剪中发送的最大事件大小.如果超过此大小,则不会进行修剪.</p>
<p>hive.tez.enable.memory.manager<br>TRUE<br>为 tez 启用内存管理器</p>
<p>hive.tez.exec.inplace.progress<br>TRUE<br>在终端中就地更新 tez 作业执行进度.</p>
<p>hive.tez.exec.print.summary<br>FALSE<br>显示 shell 执行的每个查询的执行步骤细分.</p>
<p>hive.tez.input.format<br>org.apache.hadoop.hive.ql.io.HiveInputFormat<br>tez 的默认输入格式.Tez 小组在上午分裂.</p>
<p>hive.tez.input.generate.consistent.splits<br>TRUE<br>在AM中生成分割时是否生成一致的分割位置</p>
<p>hive.tez.java.opts<br>未配置<br>默认情况下,Tez 将使用映射任务中的 Java 选项.这可以用来覆盖.</p>
<p>hive.tez.log.level<br>INFO<br>用于作为 DAG 一部分执行的任务的日志级别.仅当 hive.tez.java.opts 用于配置 Java 选项时使用.</p>
<p>hive.tez.max.partition.factor<br>2<br>当启用自动减速器并行性时,此因素将用于对 shuffle 边缘中的数据进行过度分区.</p>
<p>hive.tez.min.partition.factor<br>0.25<br>当启用自动减速器并行性时,此因子将用于对 tez 指定的减速器数量设置下限.</p>
<p>hive.tez.smb.number.waves<br>0.5<br>运行 SMB 加入的波次数.占集群的帐户.理想情况下应该是 1 波.</p>
<p>hive.tez.task.scale.memory.reserve.fraction<br>-1<br>Tez 将为处理器保留的自定义 JVM 内存部分</p>
<p>hive.tez.task.scale.memory.reserve.fraction.max<br>0.5<br>Tez 将为处理器保留的 JVM 内存的最大比例</p>
<p>hive.tez.task.scale.memory.reserve-fraction.min<br>0.3<br>这是为了覆盖 tez 设置 tez.task.scale.memory.reserve-fraction</p>
<h3 id="txn"><a href="#txn" class="headerlink" title="txn"></a>txn</h3><p>hive.timedout.txn.reaper.interval<br>180s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.时间间隔描述收割机的频率运行</p>
<p>hive.timedout.txn.reaper.start<br>100s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.第一个收割机运行后的时间延迟元存储启动</p>
<p>hive.transform.escape.input<br>FALSE<br>当特殊字符(换行符/回车符和制表符)传递给用户脚本时,这将添加一个选项来转义它们.如果 Hive 表可以包含包含特殊字符的数据,这将很有用.</p>
<p>hive.transpose.aggr.join<br>FALSE<br>通过 join 推送聚合</p>
<p>hive.txn.heartbeat.threadpool.size<br>5<br>用于检测信号的线程数.对于 Hive CLI,1 就足够了.对于 HiveServer2,我们需要一些</p>
<p>hive.txn.manager<br>org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager<br>设置为 org.apache.hadoop.hive.ql.lockmgr.DbTxnManager 作为打开 Hive 事务的一部分,这也需要对 hive.compactor.initiator.on/hive.compactor.worker.threads/hive.support.concurrency 进行适当设置(true)/<code>hive.enforce.bucketing</code> (true) 和 hive.exec.dynamic.partition.mode (nonstrict).默认的 DummyTxnManager 复制 Hive-0.13 之前的行为并且不提供任何事务.</p>
<p>hive.txn.manager.dump.lock.state.on.acquire.timeout<br>FALSE<br>将此设置为 true,以便在尝试获取资源锁时超时,锁管理器的当前状态将转储到日志文件.这是为了调试.另请参阅 hive.lock.numretries 和 hive.lock.sleep.between.retries.</p>
<p>hive.txn.max.open.batch<br>1000<br>一次调用 open_txns() 可以获取的最大事务数.这控制有多少事务流代理(例如 Flume 或 Storm)同时打开.然后,流代理将该数量的条目写入单个文件(每个 Flume 代理或 Storm bolt).因此,增加此值会减少流代理创建的增量文件的数量.但它也增加了 Hive 在任何给定时间必须跟踪的打开事务的数量,这可能会对读取性能产生负面影响.</p>
<p>hive.txn.retryable.sqlex.regex<br>未配置<br>逗号分隔的 SQL 状态/错误代码和可重试 SQLExceptions 的错误消息的正则表达式模式列表,适用于 Metastore DB.例如:Can&#39;t serialize.<code>*,40001$,^Deadlock,.*ORA-08176. *正则表达式将匹配的字符串具有以下形式,其中 ex 是 SQLException:ex.getMessage() + &quot; (SQLState=&quot; + ex.getSQLState() + &quot;, ErrorCode=&quot; + ex.getErrorCode( ) + &quot;)&quot;</code></p>
<p>hive.txn.timeout<br>300s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 sec.在此时间之后,事务被宣布中止如果客户端没有发送心跳.</p>
<p>hive.typecheck.on.insert<br>TRUE<br>此属性已扩展为控制是否检查/转换和规范化分区值以符合其在分区操作中的列类型,包括但不限于插入,例如 alter/describe 等.</p>
<p>hive.udtf.auto.progress<br>FALSE<br>Hive 是否应该在使用 UDTF 时自动将进度信息发送到 TaskTracker,以防止任务因不活动而被终止.用户应该小心,因为这可能会阻止 TaskTracker 杀死无限循环的任务.</p>
<p>hive.unlock.numretries<br>10<br>您想重试一次解锁的次数</p>
<p>hive.user.install.directory<br>/user/<br>如果 hive(仅在 tez 模式下)在&quot;hive.jar.directory&quot;中找不到可用的 hive jar,它会将 hive jar 上传到&quot;hive.user.install.directory/user.name&quot;并使用它来运行查询.</p>
<p>hive.users.in.admin.role<br>未配置<br>以逗号分隔的管理员角色用户列表以进行引导.更多用户可以稍后添加到管理员角色中.</p>
<p>hive.variable.substitute<br>TRUE<br>这可以使用 ${var} ${system:var} 和 ${env:var} 之类的语法进行替换.</p>
<p>hive.variable.substitute.depth<br>40<br>替换引擎将执行的最大替换.</p>
<h3 id="vectorized"><a href="#vectorized" class="headerlink" title="vectorized"></a>vectorized</h3><p>hive.vectorized.execution.enabled<br>FALSE<br>此标志应设置为 true 以启用查询执行的矢量化模式.默认值为 false.</p>
<p>hive.vectorized.execution.mapjoin.minmax.enabled<br>FALSE<br>此标志应设置为 true 以启用向量映射连接哈希表,以便使用 MapJoin 对整数连接查询使用最大/最大过滤.默认值为 false.</p>
<p>hive.vectorized.execution.mapjoin.native.enabled<br>TRUE<br>此标志应设置为 true 以启用使用 MapJoin 的查询的本机(即非传递)矢量化.默认值为 true.</p>
<p>hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled<br>FALSE<br>此标志应设置为 true 以启用在使用 MapJoin 的查询中使用本机快速向量映射连接哈希表.默认值为 false.</p>
<p>hive.vectorized.execution.mapjoin.native.multikey.only.enabled<br>FALSE<br>此标志应设置为 true,以在使用 MapJoin 的查询中将本机向量映射连接哈希表的使用限制为 MultiKey.默认值为 false.</p>
<p>hive.vectorized.execution.mapjoin.overflow.repeated.threshold<br>-1<br>向量映射连接哈希表中匹配的小表行数,其中我们使用溢出向量化行批处理中的重复字段优化来使用 MapJoin 进行连接查询.值 -1 表示确实使用连接结果优化.否则,阈值可以是 0 到最大整数.</p>
<p>hive.vectorized.execution.reduce.enabled<br>TRUE<br>此标志应设置为 true 以启用查询执行的 reduce 端的矢量化模式.默认值为 true.</p>
<p>hive.vectorized.execution.reduce.groupby.enabled<br>TRUE<br>此标志应设置为 true 以启用 reduce 端 GROUP BY 查询执行的矢量化模式.默认值为 true.</p>
<p>hive.vectorized.execution.reducesink.new.enabled<br>TRUE<br>此标志应设置为 true 以启用使用 ReduceSink.i 的查询的新矢量化.默认值为 true.</p>
<p>hive.vectorized.groupby.checkinterval<br>100000<br>在重新计算平均条目大小之前,通过聚合散列添加到组中的条目数.</p>
<p>hive.vectorized.groupby.flush.percent<br>0.1<br>超过内存阈值时,按聚合哈希刷新的组中条目的百分比.</p>
<p>hive.vectorized.groupby.maxentries<br>1000000<br>聚合哈希表中向量组中的最大条目数.超过此值将触发与内存压力条件无关的刷新.</p>
<p>hive.vectorized.use.row.serde.deserialize<br>FALSE<br>此标志应设置为 true 以启用使用行反序列化的矢量化.默认值为 false.</p>
<p>hive.vectorized.use.vector.serde.deserialize<br>FALSE<br>此标志应设置为 true 以启用使用矢量反序列化的矢量化行.默认值为 false.</p>
<p>hive.vectorized.use.vectorized.input.format<br>TRUE<br>此标志应设置为 true 以启用支持向量化输入文件格式的 SerDe 向量化.默认值为 true.</p>
<p>hive.warehouse.subdir.inherit.perms<br>TRUE<br>如果应该使用从 dfs umask 派生的权限而不是继承仓库或数据库目录的权限来创建表目录,则将此设置为 false.</p>
<p>hive.writeset.reaper.interval<br>60s<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.WriteSet reaper 运行的频率</p>
<h3 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h3><p>hive.zookeeper.clean.extra.nodes<br>FALSE<br>在会话结束时清理额外的节点.</p>
<p><font color="#dd0000">hive.zookeeper.client.port</font><br>2181<br>要与之通信的 ZooKeeper 服务器的端口.如果 hive.zookeeper.quorum 中指定的 Zookeeper 服务器列表不包含端口号,则使用此值.</p>
<p>hive.zookeeper.connection.basesleeptime<br>1000ms<br>需要一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.初始时间量(以毫秒为单位) ) 在使用 ExponentialBackoffRetry 策略连接到 ZooKeeper 服务器时在重试之间等待.</p>
<p>hive.zookeeper.connection.max.retries<br>3<br>连接到 ZooKeeper 服务器时重试的最大次数.</p>
<p><font color="#dd0000">hive.zookeeper.namespace</font><br>hive_zookeeper_namespace<br>创建所有 ZooKeeper 节点的父节点.</p>
<p><font color="#dd0000">hive.zookeeper.quorum</font><br>未配置<br>要与之交谈的 ZooKeeper 服务器列表.这是需要的:1.读/写锁 - 当 hive.lock.manager 设置为 org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager 时,2.当 HiveServer2 通过 Zookeeper.3 支持服务发现时.如果使用 zookeeper 存储,则用于委托令牌存储,如果未设置 hive.cluster.delegation.token.store.zookeeper.connectString 4. LLAP 守护进程注册表服务</p>
<p><font color="#dd0000">hive.zookeeper.session.timeout</font><br>1200000ms<br>期望一个时间值,单位为(d/day/h/hour/m/min/s/sec/ms/msec/us/usec/ns/nsec),如果未指定,则为 msec.ZooKeeper 客户端的会话超时(以毫秒为单位) ).客户端断开连接,如果超时没有发送心跳,所有的锁都会被释放.</p>
<h3 id="jdo"><a href="#jdo" class="headerlink" title="jdo"></a>jdo</h3><p><font color="#dd0000">javax.jdo.option.ConnectionDriverName</font><br>org.apache.derby.jdbc.EmbeddedDriver<br>JDBC 元存储的驱动程序类名</p>
<p><font color="#dd0000">javax.jdo.option.ConnectionPassword</font><br>mine<br>用于 Metastore 数据库的密码</p>
<p><font color="#dd0000">javax.jdo.option.ConnectionURL</font><br>jdbc:derby:;databaseName=metastore_db;create=true<br>JDBC 元存储的 JDBC 连接字符串.要使用 SSL 加密/验证连接,请在连接 URL 中提供特定于数据库的 SSL 标志.例如,对于 postgres 数据库,jdbc:postgresql://myhost/db?ssl=true.</p>
<p><font color="#dd0000">javax.jdo.option.ConnectionUserName</font><br>APP<br>用于元存储数据库的用户名</p>
<p>javax.jdo.option.DetachAllOnCommit<br>TRUE<br>从会话中分离所有对象,以便在事务提交后可以使用它们</p>
<p>javax.jdo.option.Multithreaded<br>TRUE<br>如果多个线程同时通过 JDO 访问 Metastore,则将此设置为 true.</p>
<p>javax.jdo.option.NonTransactionalRead<br>TRUE<br>在事务之外读取</p>
<p>javax.jdo.PersistenceManagerFactoryClass<br>org.datanucleus.api.jdo.JDOPersistenceManagerFactory<br>实现 jdo 持久性的类</p>
<p>parquet.memory.pool.ratio<br>0.5<br>Parquet 文件编写器在一个任务中可以使用的最大堆分数.这是为了避免任务中的 OutOfMemory 错误.使用 Parquet 1.6.0 及更高版本.此配置参数在 Parquet 中定义,因此它不以&quot;hive.&quot;开头.</p>
<p>stream.stderr.reporter.enabled<br>TRUE<br>启用流式作业的状态和计数器消息的消费.</p>
<p>stream.stderr.reporter.prefix<br>reporter:<br>使用此前缀记录到标准错误的流式作业可以记录计数器或状态信息.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/hive/" rel="tag"># hive</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/03/03/mysql%E5%AE%89%E8%A3%85/" rel="prev" title="mysql安装">
                  <i class="fa fa-chevron-left"></i> mysql安装
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/03/03/hbase%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/" rel="next" title="hbase默认配置参数">
                  hbase默认配置参数 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">maoeryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  





</body>
</html>
