<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maoeryu.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","width":200,"display":"post","padding":7,"offset":5},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="DataStream概览预定义的 Source 和 Sink一些比较基本的 Source 和 Sink 已经内置在 Flink 里.预定义 data sources 支持从文件&#x2F;目录&#x2F;socket,以及 collections 和 iterators 中读取数据.预定义 data sinks 支持把数据写入文件&#x2F;标准输出(stdout)&#x2F;标准错误输出(stderr)和 socket.">
<meta property="og:type" content="article">
<meta property="og:title" content="flink connectors">
<meta property="og:url" content="https://maoeryu.github.io/2022/09/13/flink%20connectors/index.html">
<meta property="og:site_name" content="FlyingPig">
<meta property="og:description" content="DataStream概览预定义的 Source 和 Sink一些比较基本的 Source 和 Sink 已经内置在 Flink 里.预定义 data sources 支持从文件&#x2F;目录&#x2F;socket,以及 collections 和 iterators 中读取数据.预定义 data sinks 支持把数据写入文件&#x2F;标准输出(stdout)&#x2F;标准错误输出(stderr)和 socket.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl117.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl118.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl119.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl120.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl121.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl122.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl123.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl124.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl125.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl126.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl127.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl128.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl129.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl130.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl131.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl132.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl133.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl134.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl135.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl136.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl137.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl138.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl139.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl140.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl141.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl142.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl143.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl144.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl145.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl146.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl147.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl148.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl149.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl150.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl151.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl152.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl153.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl154.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl155.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl156.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl157.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl158.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl159.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl160.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl161.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl162.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl163.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl164.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl165.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl166.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl167.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl168.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl169.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl170.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl171.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl172.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl173.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl174.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl175.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl176.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl177.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl178.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl179.png">
<meta property="og:image" content="https://maoeryu.github.io/images/flgl180.png">
<meta property="article:published_time" content="2022-09-12T16:00:00.000Z">
<meta property="article:modified_time" content="2022-12-02T06:11:02.555Z">
<meta property="article:author" content="maoeryu">
<meta property="article:tag" content="flink">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maoeryu.github.io/images/flgl117.png">


<link rel="canonical" href="https://maoeryu.github.io/2022/09/13/flink%20connectors/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>flink connectors | FlyingPig</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlyingPig</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#DataStream"><span class="nav-number">1.</span> <span class="nav-text">DataStream</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%A7%88"><span class="nav-number">1.1.</span> <span class="nav-text">概览</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E7%9A%84-Source-%E5%92%8C-Sink"><span class="nav-number">1.1.1.</span> <span class="nav-text">预定义的 Source 和 Sink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%84%E5%B8%A6%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%99%A8"><span class="nav-number">1.1.2.</span> <span class="nav-text">附带的连接器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Apache-Bahir-%E4%B8%AD%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%99%A8"><span class="nav-number">1.1.3.</span> <span class="nav-text">Apache Bahir 中的连接器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5Flink%E7%9A%84%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.1.4.</span> <span class="nav-text">连接Flink的其他方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5-I-O"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">异步 I&#x2F;O</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E6%9F%A5%E8%AF%A2%E7%8A%B6%E6%80%81"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">可查询状态</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Formats"><span class="nav-number">1.2.</span> <span class="nav-text">Formats</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Avro-format"><span class="nav-number">1.2.1.</span> <span class="nav-text">Avro format</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CSV-format"><span class="nav-number">1.2.2.</span> <span class="nav-text">CSV format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">高级配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hadoop-formats"><span class="nav-number">1.2.3.</span> <span class="nav-text">Hadoop formats</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Project-Configuration"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Project Configuration</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Using-Hadoop-InputFormats"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">Using Hadoop InputFormats</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Using-Hadoop-OutputFormats"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">Using Hadoop OutputFormats</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MongoDB-format"><span class="nav-number">1.2.4.</span> <span class="nav-text">MongoDB format</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parquet-format"><span class="nav-number">1.2.5.</span> <span class="nav-text">Parquet format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink-RowData"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">Flink RowData</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Avro-Records"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">Avro Records</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Generic-record"><span class="nav-number">1.2.5.2.1.</span> <span class="nav-text">Generic record</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Specific-record"><span class="nav-number">1.2.5.2.2.</span> <span class="nav-text">Specific record</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reflect-record"><span class="nav-number">1.2.5.2.3.</span> <span class="nav-text">Reflect record</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Parquet-files-%E5%BF%85%E5%A4%87%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.2.5.2.4.</span> <span class="nav-text">使用 Parquet files 必备条件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-files-format"><span class="nav-number">1.2.6.</span> <span class="nav-text">Text files format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E7%95%8C%E8%AF%BB%E5%8F%96%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.2.6.1.</span> <span class="nav-text">有界读取示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E8%AF%BB%E5%8F%96%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.2.6.2.</span> <span class="nav-text">连续读取示例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%B9%E9%94%99%E4%BF%9D%E8%AF%81"><span class="nav-number">1.3.</span> <span class="nav-text">容错保证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka"><span class="nav-number">1.4.</span> <span class="nav-text">Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96"><span class="nav-number">1.4.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Source"><span class="nav-number">1.4.2.</span> <span class="nav-text">Kafka Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">使用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-Partition-%E8%AE%A2%E9%98%85"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">Topic &#x2F; Partition 订阅</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E8%A7%A3%E6%9E%90"><span class="nav-number">1.4.2.3.</span> <span class="nav-text">消息解析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B5%B7%E5%A7%8B%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%82%B9"><span class="nav-number">1.4.2.4.</span> <span class="nav-text">起始消费位点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E7%95%8C-%E6%97%A0%E7%95%8C%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.2.5.</span> <span class="nav-text">有界 &#x2F; 无界模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B1%9E%E6%80%A7"><span class="nav-number">1.4.2.6.</span> <span class="nav-text">其他属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E6%A3%80%E6%9F%A5"><span class="nav-number">1.4.2.7.</span> <span class="nav-text">动态分区检查</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E5%92%8C%E6%B0%B4%E5%8D%B0"><span class="nav-number">1.4.2.8.</span> <span class="nav-text">事件时间和水印</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A9%BA%E9%97%B2"><span class="nav-number">1.4.2.9.</span> <span class="nav-text">空闲</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%82%B9%E6%8F%90%E4%BA%A4"><span class="nav-number">1.4.2.10.</span> <span class="nav-text">消费位点提交</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7"><span class="nav-number">1.4.2.11.</span> <span class="nav-text">监控</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Kafka-Consumer-%E6%8C%87%E6%A0%87"><span class="nav-number">1.4.2.11.1.</span> <span class="nav-text">Kafka Consumer 指标</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E5%85%A8"><span class="nav-number">1.4.2.12.</span> <span class="nav-text">安全</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-number">1.4.2.13.</span> <span class="nav-text">实现细节</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%86%E7%89%87-Source-Split"><span class="nav-number">1.4.2.13.1.</span> <span class="nav-text">数据源分片(Source Split)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E7%89%87%E6%9E%9A%E4%B8%BE%E5%99%A8-Split-Enumerator"><span class="nav-number">1.4.2.13.2.</span> <span class="nav-text">分片枚举器(Split Enumerator)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BA%90%E8%AF%BB%E5%8F%96%E5%99%A8-Source-Reader"><span class="nav-number">1.4.2.13.3.</span> <span class="nav-text">源读取器(Source Reader)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-SourceFunction"><span class="nav-number">1.4.3.</span> <span class="nav-text">Kafka SourceFunction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Sink"><span class="nav-number">1.4.4.</span> <span class="nav-text">Kafka Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-1"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">使用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">序列化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%B9%E9%94%99"><span class="nav-number">1.4.4.3.</span> <span class="nav-text">容错</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7-1"><span class="nav-number">1.4.4.4.</span> <span class="nav-text">监控</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Producer"><span class="nav-number">1.4.5.</span> <span class="nav-text">Kafka Producer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-%E8%BF%9E%E6%8E%A5%E5%99%A8%E6%8C%87%E6%A0%87"><span class="nav-number">1.4.6.</span> <span class="nav-text">Kafka 连接器指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-Kerberos-%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81"><span class="nav-number">1.4.7.</span> <span class="nav-text">启用 Kerberos 身份验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%87%E7%BA%A7%E5%88%B0%E6%9C%80%E8%BF%91%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%99%A8%E7%89%88%E6%9C%AC"><span class="nav-number">1.4.8.</span> <span class="nav-text">升级到最近的连接器版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5"><span class="nav-number">1.4.9.</span> <span class="nav-text">问题排查</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1"><span class="nav-number">1.4.9.1.</span> <span class="nav-text">数据丢失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UnknownTopicOrPartitionException"><span class="nav-number">1.4.9.2.</span> <span class="nav-text">UnknownTopicOrPartitionException</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ProducerFencedException"><span class="nav-number">1.4.9.3.</span> <span class="nav-text">ProducerFencedException</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cassandra"><span class="nav-number">1.5.</span> <span class="nav-text">Cassandra</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Apache-Cassandra"><span class="nav-number">1.5.1.</span> <span class="nav-text">安装 Apache Cassandra</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cassandra-Sinks"><span class="nav-number">1.5.2.</span> <span class="nav-text">Cassandra Sinks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Write-ahead-Log"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">Write-ahead Log</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E7%82%B9%E5%92%8C%E5%AE%B9%E9%94%99"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">检查点和容错</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90"><span class="nav-number">1.5.3.</span> <span class="nav-text">例子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Streaming-Tuple-Data-Type"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">Streaming Tuple Data Type</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Streaming-POJO-Data-Type"><span class="nav-number">1.5.3.2.</span> <span class="nav-text">Streaming POJO Data Type</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Elasticsearch"><span class="nav-number">1.6.</span> <span class="nav-text">Elasticsearch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Elasticsearch"><span class="nav-number">1.6.1.</span> <span class="nav-text">安装 Elasticsearch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Elasticsearch-Sink"><span class="nav-number">1.6.2.</span> <span class="nav-text">Elasticsearch Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Elasticsearch-Sinks-%E5%92%8C%E5%AE%B9%E9%94%99"><span class="nav-number">1.6.2.1.</span> <span class="nav-text">Elasticsearch Sinks 和容错</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%A4%B1%E8%B4%A5%E7%9A%84-Elasticsearch-%E8%AF%B7%E6%B1%82"><span class="nav-number">1.6.2.2.</span> <span class="nav-text">处理失败的 Elasticsearch 请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%86%85%E9%83%A8%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8"><span class="nav-number">1.6.2.3.</span> <span class="nav-text">配置内部批量处理器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%86-Elasticsearch-%E8%BF%9E%E6%8E%A5%E5%99%A8%E6%89%93%E5%8C%85%E5%88%B0-Uber-Jar-%E4%B8%AD"><span class="nav-number">1.6.3.</span> <span class="nav-text">将 Elasticsearch 连接器打包到 Uber-Jar 中</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Firehose"><span class="nav-number">1.7.</span> <span class="nav-text">Firehose</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kinesis"><span class="nav-number">1.8.</span> <span class="nav-text">Kinesis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="nav-number">1.9.</span> <span class="nav-text">文件系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#File-Source"><span class="nav-number">1.9.1.</span> <span class="nav-text">File Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-2"><span class="nav-number">1.9.1.1.</span> <span class="nav-text">使用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-Types"><span class="nav-number">1.9.1.2.</span> <span class="nav-text">Format Types</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#TextLine-Format"><span class="nav-number">1.9.1.2.1.</span> <span class="nav-text">TextLine Format</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SimpleStreamFormat-%E6%8A%BD%E8%B1%A1%E7%B1%BB"><span class="nav-number">1.9.1.2.2.</span> <span class="nav-text">SimpleStreamFormat 抽象类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Bulk-Format"><span class="nav-number">1.9.1.2.3.</span> <span class="nav-text">Bulk Format</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E6%9E%9A%E4%B8%BE%E7%B1%BB"><span class="nav-number">1.9.1.3.</span> <span class="nav-text">自定义文件枚举类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%93%E5%89%8D%E9%99%90%E5%88%B6"><span class="nav-number">1.9.1.4.</span> <span class="nav-text">当前限制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8E%E8%AE%B0"><span class="nav-number">1.9.1.5.</span> <span class="nav-text">后记</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#File-Sink"><span class="nav-number">1.9.2.</span> <span class="nav-text">File Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-Types-1"><span class="nav-number">1.9.2.1.</span> <span class="nav-text">Format Types</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Row-encoded-Formats"><span class="nav-number">1.9.2.1.1.</span> <span class="nav-text">Row-encoded Formats</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Bulk-encoded-Formats"><span class="nav-number">1.9.2.1.2.</span> <span class="nav-text">Bulk-encoded Formats</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Parquet-Format"><span class="nav-number">1.9.2.1.3.</span> <span class="nav-text">Parquet Format</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Avro-Format"><span class="nav-number">1.9.2.1.4.</span> <span class="nav-text">Avro Format</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ORC-Format"><span class="nav-number">1.9.2.1.5.</span> <span class="nav-text">ORC Format</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hadoop-SequenceFile-Format"><span class="nav-number">1.9.2.1.6.</span> <span class="nav-text">Hadoop SequenceFile Format</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%B6%E5%88%86%E9%85%8D"><span class="nav-number">1.9.2.2.</span> <span class="nav-text">桶分配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BB%9A%E5%8A%A8%E7%AD%96%E7%95%A5"><span class="nav-number">1.9.2.3.</span> <span class="nav-text">滚动策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Part-%E6%96%87%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F"><span class="nav-number">1.9.2.4.</span> <span class="nav-text">Part 文件生命周期</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Part-%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.9.2.4.1.</span> <span class="nav-text">Part 文件示例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Part-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="nav-number">1.9.2.4.2.</span> <span class="nav-text">Part 文件配置</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6"><span class="nav-number">1.9.2.5.</span> <span class="nav-text">文件合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">1.9.2.6.</span> <span class="nav-text">重要注意事项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">1.9.2.6.1.</span> <span class="nav-text">通用注意事项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BATCH-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">1.9.2.6.2.</span> <span class="nav-text">BATCH 注意事项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#S3-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">1.9.2.6.3.</span> <span class="nav-text">S3 注意事项</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RabbitMQ"><span class="nav-number">1.10.</span> <span class="nav-text">RabbitMQ</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RabbitMQ-Source"><span class="nav-number">1.10.1.</span> <span class="nav-text">RabbitMQ Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F-QoS-%E6%B6%88%E8%B4%B9%E8%80%85%E9%A2%84%E5%8F%96-Consumer-Prefetch"><span class="nav-number">1.10.1.1.</span> <span class="nav-text">服务质量 (QoS) &#x2F; 消费者预取(Consumer Prefetch)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RabbitMQ-Sink"><span class="nav-number">1.10.2.</span> <span class="nav-text">RabbitMQ Sink</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Google-Cloud-PubSub"><span class="nav-number">1.11.</span> <span class="nav-text">Google Cloud PubSub</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hybrid-Source"><span class="nav-number">1.12.</span> <span class="nav-text">Hybrid Source</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-position-for-next-source"><span class="nav-number">1.12.1.</span> <span class="nav-text">Start position for next source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BF%AE%E5%A4%8D%E4%BA%86%E5%9B%BE%E6%9E%84%E5%BB%BA%E6%97%B6%E7%9A%84%E5%BC%80%E5%A7%8B%E4%BD%8D%E7%BD%AE"><span class="nav-number">1.12.1.1.</span> <span class="nav-text">修复了图构建时的开始位置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%87%E6%8D%A2%E6%97%B6%E9%97%B4%E7%9A%84%E5%8A%A8%E6%80%81%E8%B5%B7%E5%A7%8B%E4%BD%8D%E7%BD%AE"><span class="nav-number">1.12.1.2.</span> <span class="nav-text">切换时间的动态起始位置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Apache-NiFi"><span class="nav-number">1.13.</span> <span class="nav-text">Apache NiFi</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Apache-NiFi-Source"><span class="nav-number">1.13.1.</span> <span class="nav-text">Apache NiFi Source</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Apache-NiFi-Sink"><span class="nav-number">1.13.2.</span> <span class="nav-text">Apache NiFi Sink</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Apache-Pulsar"><span class="nav-number">1.14.</span> <span class="nav-text">Apache Pulsar</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96"><span class="nav-number">1.14.1.</span> <span class="nav-text">添加依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pulsar-Source"><span class="nav-number">1.14.2.</span> <span class="nav-text">Pulsar Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.14.2.1.</span> <span class="nav-text">使用示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E5%AE%9A%E6%B6%88%E8%B4%B9%E7%9A%84-Topic-%E6%88%96%E8%80%85-Topic-%E5%88%86%E5%8C%BA"><span class="nav-number">1.14.2.2.</span> <span class="nav-text">指定消费的 Topic 或者 Topic 分区</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Topic-%E5%90%8D%E7%A7%B0%E7%AE%80%E5%86%99"><span class="nav-number">1.14.2.2.1.</span> <span class="nav-text">Topic 名称简写</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pulsar-Topic-%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84"><span class="nav-number">1.14.2.2.2.</span> <span class="nav-text">Pulsar Topic 层次结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Topic-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">1.14.2.2.3.</span> <span class="nav-text">配置 Topic 正则表达式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8"><span class="nav-number">1.14.2.3.</span> <span class="nav-text">反序列化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pulsar-%E8%AE%A2%E9%98%85"><span class="nav-number">1.14.2.4.</span> <span class="nav-text">Pulsar 订阅</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B5%B7%E5%A7%8B%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%BD%AE"><span class="nav-number">1.14.2.5.</span> <span class="nav-text">起始消费位置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%B9%E7%95%8C"><span class="nav-number">1.14.2.6.</span> <span class="nav-text">边界</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Source-%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.2.7.</span> <span class="nav-text">Source 配置项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Pulsar-Java-%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.2.7.1.</span> <span class="nav-text">Pulsar Java 客户端配置项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pulsar-%E6%B6%88%E8%B4%B9%E8%80%85-API-%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.2.7.2.</span> <span class="nav-text">Pulsar 消费者 API 配置项</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pulsar-Source%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.2.8.</span> <span class="nav-text">Pulsar Source配置项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E5%8F%91%E7%8E%B0"><span class="nav-number">1.14.2.9.</span> <span class="nav-text">动态分区发现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E5%92%8C%E6%B0%B4%E4%BD%8D%E7%BA%BF"><span class="nav-number">1.14.2.10.</span> <span class="nav-text">事件时间和水位线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E7%A1%AE%E8%AE%A4"><span class="nav-number">1.14.2.11.</span> <span class="nav-text">消息确认</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%8B%AC%E5%8D%A0%E5%92%8C%E7%81%BE%E5%A4%87%E8%AE%A2%E9%98%85%E4%B8%8B%E7%9A%84%E6%B6%88%E6%81%AF%E7%A1%AE%E8%AE%A4"><span class="nav-number">1.14.2.11.1.</span> <span class="nav-text">独占和灾备订阅下的消息确认</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%92%8C-key-%E5%85%B1%E4%BA%AB%E8%AE%A2%E9%98%85%E4%B8%8B%E7%9A%84%E6%B6%88%E6%81%AF%E7%A1%AE%E8%AE%A4"><span class="nav-number">1.14.2.11.2.</span> <span class="nav-text">共享和 key 共享订阅下的消息确认</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pulsar-Sink"><span class="nav-number">1.14.3.</span> <span class="nav-text">Pulsar Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">1.14.3.1.</span> <span class="nav-text">使用示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E5%AE%9A%E5%86%99%E5%85%A5%E7%9A%84-Topic-%E6%88%96%E8%80%85-Topic-%E5%88%86%E5%8C%BA"><span class="nav-number">1.14.3.2.</span> <span class="nav-text">指定写入的 Topic 或者 Topic 分区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8-1"><span class="nav-number">1.14.3.3.</span> <span class="nav-text">序列化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E8%B7%AF%E7%94%B1%E7%AD%96%E7%95%A5"><span class="nav-number">1.14.3.4.</span> <span class="nav-text">消息路由策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">1.14.3.5.</span> <span class="nav-text">发送一致性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E5%BB%B6%E6%97%B6%E5%8F%91%E9%80%81"><span class="nav-number">1.14.3.6.</span> <span class="nav-text">消息延时发送</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sink-%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.3.7.</span> <span class="nav-text">Sink 配置项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Pulsar-%E7%94%9F%E4%BA%A7%E8%80%85-API-%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.3.7.1.</span> <span class="nav-text">Pulsar 生产者 API 配置项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pulsar-Sink-%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-number">1.14.3.7.2.</span> <span class="nav-text">Pulsar Sink 配置项</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sink-%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87"><span class="nav-number">1.14.3.8.</span> <span class="nav-text">Sink 监控指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E7%AE%80%E8%BF%B0"><span class="nav-number">1.14.3.9.</span> <span class="nav-text">设计思想简述</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%97%A0%E7%8A%B6%E6%80%81%E7%9A%84-SinkWriter"><span class="nav-number">1.14.3.9.1.</span> <span class="nav-text">无状态的 SinkWriter</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pulsar-Schema-Evolution"><span class="nav-number">1.14.3.9.2.</span> <span class="nav-text">Pulsar Schema Evolution</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%87%E7%BA%A7%E8%87%B3%E6%9C%80%E6%96%B0%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%99%A8"><span class="nav-number">1.14.4.</span> <span class="nav-text">升级至最新的连接器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%AF%8A%E6%96%AD"><span class="nav-number">1.14.5.</span> <span class="nav-text">问题诊断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B7%B2%E7%9F%A5%E9%97%AE%E9%A2%98"><span class="nav-number">1.14.6.</span> <span class="nav-text">已知问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8-Java-11-%E4%B8%8A%E4%BD%BF%E7%94%A8%E4%B8%8D%E7%A8%B3%E5%AE%9A"><span class="nav-number">1.14.6.1.</span> <span class="nav-text">在 Java 11 上使用不稳定</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%87%AA%E5%8A%A8%E9%87%8D%E8%BF%9E-%E8%80%8C%E6%98%AF%E6%8A%9B%E5%87%BATransactionCoordinatorNotFound%E5%BC%82%E5%B8%B8"><span class="nav-number">1.14.6.2.</span> <span class="nav-text">不自动重连,而是抛出TransactionCoordinatorNotFound异常</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JDBC"><span class="nav-number">1.15.</span> <span class="nav-text">JDBC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Redis"><span class="nav-number">1.16.</span> <span class="nav-text">Redis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-Redis-Server"><span class="nav-number">1.16.1.</span> <span class="nav-text">Single Redis Server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Redis-Cluster"><span class="nav-number">1.16.2.</span> <span class="nav-text">Redis Cluster</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Redis-Sentinel"><span class="nav-number">1.16.3.</span> <span class="nav-text">Redis Sentinel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data-types"><span class="nav-number">1.16.4.</span> <span class="nav-text">data types</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ActiveMQ"><span class="nav-number">1.17.</span> <span class="nav-text">ActiveMQ</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Table-amp-SQL"><span class="nav-number">2.</span> <span class="nav-text">Table &amp; SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%A7%88-1"><span class="nav-number">2.1.</span> <span class="nav-text">概览</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%BF%9E%E6%8E%A5%E5%99%A8"><span class="nav-number">2.1.1.</span> <span class="nav-text">如何使用连接器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2%E8%A1%A8%E8%BF%9E%E6%8E%A5%E5%99%A8-%E6%A0%BC%E5%BC%8F%E8%B5%84%E6%BA%90"><span class="nav-number">2.1.2.</span> <span class="nav-text">转换表连接器&#x2F;格式资源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Schema-Mapping"><span class="nav-number">2.1.3.</span> <span class="nav-text">Schema Mapping</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">元数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E9%94%AE"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">主键</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E5%B1%9E%E6%80%A7"><span class="nav-number">2.1.3.3.</span> <span class="nav-text">时间属性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A1%8C%E6%97%B6%E9%97%B4%E5%B1%9E%E6%80%A7"><span class="nav-number">2.1.3.3.1.</span> <span class="nav-text">行时间属性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL-%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.1.4.</span> <span class="nav-text">SQL 类型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Formats-1"><span class="nav-number">2.2.</span> <span class="nav-text">Formats</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CSV-Format"><span class="nav-number">2.2.1.</span> <span class="nav-text">CSV Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-1"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%BD%BF%E7%94%A8-CSV-%E6%A0%BC%E5%BC%8F%E7%9A%84%E8%A1%A8"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">如何创建使用 CSV 格式的表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84"><span class="nav-number">2.2.1.4.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JSON-Format"><span class="nav-number">2.2.2.</span> <span class="nav-text">JSON Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-2"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E5%BC%A0%E5%9F%BA%E4%BA%8E-JSON-Format-%E7%9A%84%E8%A1%A8"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">如何创建一张基于 JSON Format 的表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-1"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">数据类型映射关系</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Avro-Format-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">Avro Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-3"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-Avro-format-%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">如何使用 Avro format 创建表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-2"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-1"><span class="nav-number">2.2.3.4.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Confluent-Avro-Format"><span class="nav-number">2.2.4.</span> <span class="nav-text">Confluent Avro Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-4"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%BD%BF%E7%94%A8-Avro-Confluent-%E6%A0%BC%E5%BC%8F%E7%9A%84%E8%A1%A8"><span class="nav-number">2.2.4.2.</span> <span class="nav-text">如何创建使用 Avro-Confluent 格式的表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-3"><span class="nav-number">2.2.4.3.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-2"><span class="nav-number">2.2.4.4.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Debezium-Format"><span class="nav-number">2.2.5.</span> <span class="nav-text">Debezium Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-5"><span class="nav-number">2.2.5.1.</span> <span class="nav-text">依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Debezium-Avro"><span class="nav-number">2.2.5.1.1.</span> <span class="nav-text">Debezium Avro</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Debezium-Json"><span class="nav-number">2.2.5.1.2.</span> <span class="nav-text">Debezium Json</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-Debezium-Format"><span class="nav-number">2.2.5.2.</span> <span class="nav-text">如何使用 Debezium Format</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Available-Metadata"><span class="nav-number">2.2.5.3.</span> <span class="nav-text">Available Metadata</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-4"><span class="nav-number">2.2.5.4.</span> <span class="nav-text">Format 参数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Debezium-Avro-1"><span class="nav-number">2.2.5.4.1.</span> <span class="nav-text">Debezium Avro</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Debezium-Json-1"><span class="nav-number">2.2.5.4.2.</span> <span class="nav-text">Debezium Json</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">2.2.5.5.</span> <span class="nav-text">注意事项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%A4%8D%E7%9A%84%E5%8F%98%E6%9B%B4%E4%BA%8B%E4%BB%B6"><span class="nav-number">2.2.5.5.1.</span> <span class="nav-text">重复的变更事件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9-Debezium-Postgres-Connector-%E4%BA%A7%E7%94%9F%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">2.2.5.5.2.</span> <span class="nav-text">消费 Debezium Postgres Connector 产生的数据</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-3"><span class="nav-number">2.2.5.6.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Canal-Format"><span class="nav-number">2.2.6.</span> <span class="nav-text">Canal Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-6"><span class="nav-number">2.2.6.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-Canal-Format"><span class="nav-number">2.2.6.2.</span> <span class="nav-text">如何使用 Canal Format</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Available-Metadata-1"><span class="nav-number">2.2.6.3.</span> <span class="nav-text">Available Metadata</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-5"><span class="nav-number">2.2.6.4.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-1"><span class="nav-number">2.2.6.5.</span> <span class="nav-text">注意事项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%A4%8D%E7%9A%84%E5%8F%98%E6%9B%B4%E4%BA%8B%E4%BB%B6-1"><span class="nav-number">2.2.6.5.1.</span> <span class="nav-text">重复的变更事件</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-4"><span class="nav-number">2.2.6.6.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Maxwell-Format"><span class="nav-number">2.2.7.</span> <span class="nav-text">Maxwell Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96%E9%A1%B9"><span class="nav-number">2.2.7.1.</span> <span class="nav-text">依赖项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Maxwell%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.2.7.2.</span> <span class="nav-text">如何使用Maxwell格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E7%94%A8%E5%85%83%E6%95%B0%E6%8D%AE"><span class="nav-number">2.2.7.3.</span> <span class="nav-text">可用元数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%BC%E5%BC%8F%E9%80%89%E9%A1%B9"><span class="nav-number">2.2.7.4.</span> <span class="nav-text">格式选项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-2"><span class="nav-number">2.2.7.5.</span> <span class="nav-text">注意事项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%A4%8D%E7%9A%84%E6%9B%B4%E6%94%B9%E4%BA%8B%E4%BB%B6"><span class="nav-number">2.2.7.5.1.</span> <span class="nav-text">重复的更改事件</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-5"><span class="nav-number">2.2.7.6.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ogg-Format"><span class="nav-number">2.2.8.</span> <span class="nav-text">Ogg Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-7"><span class="nav-number">2.2.8.1.</span> <span class="nav-text">依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Ogg-Json"><span class="nav-number">2.2.8.1.1.</span> <span class="nav-text">Ogg Json</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-to-use-Ogg-format"><span class="nav-number">2.2.8.2.</span> <span class="nav-text">How to use Ogg format</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Available-Metadata-2"><span class="nav-number">2.2.8.3.</span> <span class="nav-text">Available Metadata</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-Options"><span class="nav-number">2.2.8.4.</span> <span class="nav-text">Format Options</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-Type-Mapping"><span class="nav-number">2.2.8.5.</span> <span class="nav-text">Data Type Mapping</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parquet-%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.2.9.</span> <span class="nav-text">Parquet 格式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-8"><span class="nav-number">2.2.9.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8E-Parquet-%E6%A0%BC%E5%BC%8F%E7%9A%84%E8%A1%A8"><span class="nav-number">2.2.9.2.</span> <span class="nav-text">如何创建基于 Parquet 格式的表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-6"><span class="nav-number">2.2.9.3.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-6"><span class="nav-number">2.2.9.4.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Orc-Format"><span class="nav-number">2.2.10.</span> <span class="nav-text">Orc Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-9"><span class="nav-number">2.2.10.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E7%94%A8-Orc-%E6%A0%BC%E5%BC%8F%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%A1%A8%E6%A0%BC"><span class="nav-number">2.2.10.2.</span> <span class="nav-text">如何用 Orc 格式创建一个表格</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-7"><span class="nav-number">2.2.10.3.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-7"><span class="nav-number">2.2.10.4.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raw-Format"><span class="nav-number">2.2.11.</span> <span class="nav-text">Raw Format</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.2.11.1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Format-%E5%8F%82%E6%95%B0-8"><span class="nav-number">2.2.11.2.</span> <span class="nav-text">Format 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-8"><span class="nav-number">2.2.11.3.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-1"><span class="nav-number">2.3.</span> <span class="nav-text">Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-10"><span class="nav-number">2.3.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA-Kafka-%E8%A1%A8"><span class="nav-number">2.3.2.</span> <span class="nav-text">如何创建 Kafka 表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E7%94%A8%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE"><span class="nav-number">2.3.3.</span> <span class="nav-text">可用的元数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%BC%E5%BC%8F%E5%85%83%E4%BF%A1%E6%81%AF"><span class="nav-number">2.3.4.</span> <span class="nav-text">格式元信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%99%A8%E5%8F%82%E6%95%B0"><span class="nav-number">2.3.5.</span> <span class="nav-text">连接器参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E6%80%A7"><span class="nav-number">2.3.6.</span> <span class="nav-text">特性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E9%94%AE-Key-%E4%B8%8E%E6%B6%88%E6%81%AF%E4%BD%93-Value-%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.3.6.1.</span> <span class="nav-text">消息键(Key)与消息体(Value)的格式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E4%BD%93%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.3.6.1.1.</span> <span class="nav-text">消息体格式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E9%94%AE%E5%92%8C%E6%B6%88%E6%81%AF%E4%BD%93%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.3.6.1.2.</span> <span class="nav-text">消息键和消息体格式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%90%8D%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%AD%97%E6%AE%B5"><span class="nav-number">2.3.6.1.3.</span> <span class="nav-text">重名的格式字段</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Topic-%E5%92%8C-Partition-%E7%9A%84%E6%8E%A2%E6%B5%8B"><span class="nav-number">2.3.6.2.</span> <span class="nav-text">Topic 和 Partition 的探测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B5%B7%E5%A7%8B%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%82%B9-1"><span class="nav-number">2.3.6.3.</span> <span class="nav-text">起始消费位点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CDC-%E5%8F%98%E6%9B%B4%E6%97%A5%E5%BF%97-Changelog-Source"><span class="nav-number">2.3.6.4.</span> <span class="nav-text">CDC 变更日志(Changelog) Source</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sink-%E5%88%86%E5%8C%BA"><span class="nav-number">2.3.6.5.</span> <span class="nav-text">Sink 分区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7%E4%BF%9D%E8%AF%81"><span class="nav-number">2.3.6.6.</span> <span class="nav-text">一致性保证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Source-%E6%8C%89%E5%88%86%E5%8C%BA-Watermark"><span class="nav-number">2.3.6.7.</span> <span class="nav-text">Source 按分区 Watermark</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E5%85%A8-1"><span class="nav-number">2.3.6.8.</span> <span class="nav-text">安全</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-9"><span class="nav-number">2.3.7.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Upsert-Kafka"><span class="nav-number">2.4.</span> <span class="nav-text">Upsert Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-11"><span class="nav-number">2.4.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.4.2.</span> <span class="nav-text">完整示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Available-Metadata-3"><span class="nav-number">2.4.3.</span> <span class="nav-text">Available Metadata</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-1"><span class="nav-number">2.4.4.</span> <span class="nav-text">特性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Key-and-Value-Formats"><span class="nav-number">2.4.4.1.</span> <span class="nav-text">Key and Value Formats</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E9%94%AE%E7%BA%A6%E6%9D%9F"><span class="nav-number">2.4.4.2.</span> <span class="nav-text">主键约束</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7%E4%BF%9D%E8%AF%81-1"><span class="nav-number">2.4.4.3.</span> <span class="nav-text">一致性保证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E6%AF%8F%E4%B8%AA%E5%88%86%E5%8C%BA%E7%94%9F%E6%88%90%E7%9B%B8%E5%BA%94%E7%9A%84-watermark"><span class="nav-number">2.4.4.4.</span> <span class="nav-text">为每个分区生成相应的 watermark</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-10"><span class="nav-number">2.4.5.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Firehose-1"><span class="nav-number">2.5.</span> <span class="nav-text">Firehose</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kinesis-1"><span class="nav-number">2.6.</span> <span class="nav-text">Kinesis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JDBC-1"><span class="nav-number">2.7.</span> <span class="nav-text">JDBC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-12"><span class="nav-number">2.7.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA-JDBC-%E8%A1%A8"><span class="nav-number">2.7.2.</span> <span class="nav-text">如何创建 JDBC 表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%99%A8%E5%8F%82%E6%95%B0-1"><span class="nav-number">2.7.3.</span> <span class="nav-text">连接器参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-2"><span class="nav-number">2.7.4.</span> <span class="nav-text">特性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%94%AE%E5%A4%84%E7%90%86"><span class="nav-number">2.7.4.1.</span> <span class="nav-text">键处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%89%AB%E6%8F%8F"><span class="nav-number">2.7.4.2.</span> <span class="nav-text">分区扫描</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lookup-Cache"><span class="nav-number">2.7.4.3.</span> <span class="nav-text">Lookup Cache</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%82%E7%AD%89%E5%86%99%E5%85%A5"><span class="nav-number">2.7.4.4.</span> <span class="nav-text">幂等写入</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JDBC-Catalog"><span class="nav-number">2.7.5.</span> <span class="nav-text">JDBC Catalog</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JDBC-Catalog-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">2.7.5.1.</span> <span class="nav-text">JDBC Catalog 的使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JDBC-Catalog-for-PostgreSQL"><span class="nav-number">2.7.5.2.</span> <span class="nav-text">JDBC Catalog for PostgreSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#PostgreSQL-%E5%85%83%E7%A9%BA%E9%97%B4%E6%98%A0%E5%B0%84"><span class="nav-number">2.7.5.2.1.</span> <span class="nav-text">PostgreSQL 元空间映射</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JDBC-Catalog-for-MySQL"><span class="nav-number">2.7.5.3.</span> <span class="nav-text">JDBC Catalog for MySQL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MySQL-%E5%85%83%E7%A9%BA%E9%97%B4%E6%98%A0%E5%B0%84"><span class="nav-number">2.7.5.3.1.</span> <span class="nav-text">MySQL 元空间映射</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-11"><span class="nav-number">2.7.6.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Elasticsearch-1"><span class="nav-number">2.8.</span> <span class="nav-text">Elasticsearch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-13"><span class="nav-number">2.8.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA-Elasticsearch-%E8%A1%A8"><span class="nav-number">2.8.2.</span> <span class="nav-text">如何创建 Elasticsearch 表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%99%A8%E5%8F%82%E6%95%B0-2"><span class="nav-number">2.8.3.</span> <span class="nav-text">连接器参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-3"><span class="nav-number">2.8.4.</span> <span class="nav-text">特性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Key-%E5%A4%84%E7%90%86"><span class="nav-number">2.8.4.1.</span> <span class="nav-text">Key 处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E7%B4%A2%E5%BC%95"><span class="nav-number">2.8.4.2.</span> <span class="nav-text">动态索引</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84-12"><span class="nav-number">2.8.5.</span> <span class="nav-text">数据类型映射</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-1"><span class="nav-number">2.9.</span> <span class="nav-text">文件系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%96%87%E4%BB%B6"><span class="nav-number">2.9.1.</span> <span class="nav-text">分区文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#File-Formats"><span class="nav-number">2.9.2.</span> <span class="nav-text">File Formats</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Source"><span class="nav-number">2.9.3.</span> <span class="nav-text">Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95%E7%9B%91%E6%8E%A7"><span class="nav-number">2.9.3.1.</span> <span class="nav-text">目录监控</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E7%94%A8%E7%9A%84-Metadata"><span class="nav-number">2.9.3.2.</span> <span class="nav-text">可用的 Metadata</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Streaming-Sink"><span class="nav-number">2.9.4.</span> <span class="nav-text">Streaming Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BB%9A%E5%8A%A8%E7%AD%96%E7%95%A5-1"><span class="nav-number">2.9.4.1.</span> <span class="nav-text">滚动策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6-1"><span class="nav-number">2.9.4.2.</span> <span class="nav-text">文件合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%8F%90%E4%BA%A4"><span class="nav-number">2.9.4.3.</span> <span class="nav-text">分区提交</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%8F%90%E4%BA%A4%E8%A7%A6%E5%8F%91%E5%99%A8"><span class="nav-number">2.9.4.3.1.</span> <span class="nav-text">分区提交触发器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%97%B6%E9%97%B4%E6%8F%90%E5%8F%96%E5%99%A8"><span class="nav-number">2.9.4.3.2.</span> <span class="nav-text">分区时间提取器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%8F%90%E4%BA%A4%E7%AD%96%E7%95%A5"><span class="nav-number">2.9.4.3.3.</span> <span class="nav-text">分区提交策略</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sink-Parallelism"><span class="nav-number">2.9.5.</span> <span class="nav-text">Sink Parallelism</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">2.9.6.</span> <span class="nav-text">完整示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HBase"><span class="nav-number">2.10.</span> <span class="nav-text">HBase</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96-14"><span class="nav-number">2.10.1.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-HBase-%E8%A1%A8"><span class="nav-number">2.10.2.</span> <span class="nav-text">如何使用 HBase 表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%99%A8%E5%8F%82%E6%95%B0-3"><span class="nav-number">2.10.3.</span> <span class="nav-text">连接器参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84%E8%A1%A8"><span class="nav-number">2.10.4.</span> <span class="nav-text">数据类型映射表</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataGen"><span class="nav-number">2.11.</span> <span class="nav-text">DataGen</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Print"><span class="nav-number">2.12.</span> <span class="nav-text">Print</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E5%BC%A0%E5%9F%BA%E4%BA%8E-Print-%E7%9A%84%E8%A1%A8"><span class="nav-number">2.12.1.</span> <span class="nav-text">如何创建一张基于 Print 的表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%99%A8%E5%8F%82%E6%95%B0-4"><span class="nav-number">2.12.2.</span> <span class="nav-text">连接器参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BlackHole"><span class="nav-number">2.13.</span> <span class="nav-text">BlackHole</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA-BlackHole-%E8%A1%A8"><span class="nav-number">2.13.1.</span> <span class="nav-text">如何创建 BlackHole 表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%99%A8%E9%80%89%E9%A1%B9"><span class="nav-number">2.13.2.</span> <span class="nav-text">连接器选项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive"><span class="nav-number">2.14.</span> <span class="nav-text">Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%A7%88-2"><span class="nav-number">2.14.1.</span> <span class="nav-text">概览</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84Hive%E7%89%88%E6%9C%AC"><span class="nav-number">2.14.1.1.</span> <span class="nav-text">支持的Hive版本</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96%E9%A1%B9-1"><span class="nav-number">2.14.1.1.1.</span> <span class="nav-text">依赖项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Maven-%E4%BE%9D%E8%B5%96"><span class="nav-number">2.14.1.1.2.</span> <span class="nav-text">Maven 依赖</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%88%B0Hive"><span class="nav-number">2.14.1.2.</span> <span class="nav-text">连接到Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#java"><span class="nav-number">2.14.1.2.1.</span> <span class="nav-text">java</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sql"><span class="nav-number">2.14.1.2.2.</span> <span class="nav-text">sql</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yaml"><span class="nav-number">2.14.1.2.3.</span> <span class="nav-text">yaml</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DDL"><span class="nav-number">2.14.1.3.</span> <span class="nav-text">DDL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DML"><span class="nav-number">2.14.1.4.</span> <span class="nav-text">DML</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Catalog"><span class="nav-number">2.14.2.</span> <span class="nav-text">Hive Catalog</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE-HiveCatalog"><span class="nav-number">2.14.2.1.</span> <span class="nav-text">设置 HiveCatalog</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%9D%E8%B5%96%E9%A1%B9-2"><span class="nav-number">2.14.2.1.1.</span> <span class="nav-text">依赖项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-1"><span class="nav-number">2.14.2.1.2.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-HiveCatalog"><span class="nav-number">2.14.2.2.</span> <span class="nav-text">如何使用 HiveCatalog</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">2.14.2.2.1.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.14.2.3.</span> <span class="nav-text">支持的类型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-%E6%96%B9%E8%A8%80"><span class="nav-number">2.14.3.</span> <span class="nav-text">Hive 方言</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Hive-%E6%96%B9%E8%A8%80"><span class="nav-number">2.14.3.1.</span> <span class="nav-text">使用 Hive 方言</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SQL-%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="nav-number">2.14.3.1.1.</span> <span class="nav-text">SQL 客户端</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Table-API"><span class="nav-number">2.14.3.1.2.</span> <span class="nav-text">Table API</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DDL-1"><span class="nav-number">2.14.3.2.</span> <span class="nav-text">DDL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CATALOG"><span class="nav-number">2.14.3.2.1.</span> <span class="nav-text">CATALOG</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Show"><span class="nav-number">2.14.3.2.1.1.</span> <span class="nav-text">Show</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DATABASE"><span class="nav-number">2.14.3.2.2.</span> <span class="nav-text">DATABASE</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Show-1"><span class="nav-number">2.14.3.2.2.1.</span> <span class="nav-text">Show</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Create"><span class="nav-number">2.14.3.2.2.2.</span> <span class="nav-text">Create</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Alter"><span class="nav-number">2.14.3.2.2.3.</span> <span class="nav-text">Alter</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Drop"><span class="nav-number">2.14.3.2.2.4.</span> <span class="nav-text">Drop</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Use"><span class="nav-number">2.14.3.2.2.5.</span> <span class="nav-text">Use</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TABLE"><span class="nav-number">2.14.3.2.3.</span> <span class="nav-text">TABLE</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Show-2"><span class="nav-number">2.14.3.2.3.1.</span> <span class="nav-text">Show</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Create-1"><span class="nav-number">2.14.3.2.3.2.</span> <span class="nav-text">Create</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Alter-1"><span class="nav-number">2.14.3.2.3.3.</span> <span class="nav-text">Alter</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#VIEW"><span class="nav-number">2.14.3.2.3.4.</span> <span class="nav-text">VIEW</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#FUNCTION"><span class="nav-number">2.14.3.2.3.5.</span> <span class="nav-text">FUNCTION</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DML-amp-DQL-Beta"><span class="nav-number">2.14.3.3.</span> <span class="nav-text">DML &amp; DQL Beta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F"><span class="nav-number">2.14.3.4.</span> <span class="nav-text">注意</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Read-amp-Write"><span class="nav-number">2.14.4.</span> <span class="nav-text">Hive Read &amp; Write</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#read"><span class="nav-number">2.14.4.1.</span> <span class="nav-text">read</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%98%85%E8%AF%BB-Hive-%E8%A7%86%E5%9B%BE"><span class="nav-number">2.14.4.1.1.</span> <span class="nav-text">阅读 Hive 视图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96%E6%97%B6%E7%9A%84%E7%9F%A2%E9%87%8F%E5%8C%96%E4%BC%98%E5%8C%96"><span class="nav-number">2.14.4.1.2.</span> <span class="nav-text">读取时的矢量化优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BA%90%E5%B9%B6%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="nav-number">2.14.4.1.3.</span> <span class="nav-text">源并行推理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%88%86%E5%8C%BA%E6%8B%86%E5%88%86"><span class="nav-number">2.14.4.1.4.</span> <span class="nav-text">加载分区拆分</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%B4%E6%97%B6%E8%A1%A8%E8%BF%9E%E6%8E%A5"><span class="nav-number">2.14.4.2.</span> <span class="nav-text">临时表连接</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%B4%E6%97%B6%E5%8A%A0%E5%85%A5%E6%9C%80%E6%96%B0%E5%88%86%E5%8C%BA"><span class="nav-number">2.14.4.2.1.</span> <span class="nav-text">临时加入最新分区</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%B4%E6%97%B6%E5%8A%A0%E5%85%A5%E6%9C%80%E6%96%B0%E8%A1%A8"><span class="nav-number">2.14.4.2.2.</span> <span class="nav-text">临时加入最新表</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#write"><span class="nav-number">2.14.4.3.</span> <span class="nav-text">write</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Formats-2"><span class="nav-number">2.14.4.4.</span> <span class="nav-text">Formats</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Functions"><span class="nav-number">2.14.5.</span> <span class="nav-text">Hive Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87-HiveModule-%E4%BD%BF%E7%94%A8-Hive-%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="nav-number">2.14.5.1.</span> <span class="nav-text">通过 HiveModule 使用 Hive 内置函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hive-%E7%94%A8%E6%88%B7%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="nav-number">2.14.5.2.</span> <span class="nav-text">Hive 用户定义函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Hive-%E7%94%A8%E6%88%B7%E5%AE%9A%E4%B9%89%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">2.14.5.3.</span> <span class="nav-text">使用 Hive 用户定义的函数</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataSet"><span class="nav-number">3.</span> <span class="nav-text">DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96%E5%92%8C%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="nav-number">3.1.</span> <span class="nav-text">读取和写入文件系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Hadoop-%E7%9A%84-Input-OutputFormat-%E5%8C%85%E8%A3%85%E5%99%A8%E8%BF%9E%E6%8E%A5%E5%88%B0%E5%85%B6%E4%BB%96%E7%B3%BB%E7%BB%9F"><span class="nav-number">3.2.</span> <span class="nav-text">使用 Hadoop 的 Input&#x2F;OutputFormat 包装器连接到其他系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flink-%E4%B8%AD%E7%9A%84-Avro-%E6%94%AF%E6%8C%81"><span class="nav-number">3.3.</span> <span class="nav-text">Flink 中的 Avro 支持</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE-Microsoft-Azure-%E8%A1%A8%E5%AD%98%E5%82%A8"><span class="nav-number">3.3.1.</span> <span class="nav-text">访问 Microsoft Azure 表存储</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE-MongoDB"><span class="nav-number">3.4.</span> <span class="nav-text">访问 MongoDB</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">maoeryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">221</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://maoeryu.github.io/2022/09/13/flink%20connectors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="maoeryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlyingPig">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          flink connectors
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-09-13 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-13T00:00:00+08:00">2022-09-13</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-12-02 14:11:02" itemprop="dateModified" datetime="2022-12-02T14:11:02+08:00">2022-12-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8D%8F%E5%90%8C%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">协同框架</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h1><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><h3 id="预定义的-Source-和-Sink"><a href="#预定义的-Source-和-Sink" class="headerlink" title="预定义的 Source 和 Sink"></a>预定义的 Source 和 Sink</h3><p>一些比较基本的 Source 和 Sink 已经内置在 Flink 里.<br>预定义 data sources 支持从文件/目录/socket,以及 collections 和 iterators 中读取数据.<br>预定义 data sinks 支持把数据写入文件/标准输出(stdout)/标准错误输出(stderr)和 socket.</p>
<span id="more"></span>
<h3 id="附带的连接器"><a href="#附带的连接器" class="headerlink" title="附带的连接器"></a>附带的连接器</h3><p>连接器可以和多种多样的第三方系统进行交互.<br>目前支持以下系统:<br>Apache Kafka (source/sink)<br>Apache Cassandra (sink)<br>Amazon Kinesis Streams (source/sink)<br>Elasticsearch (sink)<br>FileSystem (sink)<br>RabbitMQ (source/sink)<br>Google PubSub (source/sink)<br>Hybrid Source (source)<br>Apache NiFi (source/sink)<br>Apache Pulsar (source)<br>JDBC (sink)</p>
<p>请记住,在使用一种连接器时,通常需要额外的第三方组件,比如:数据存储服务器或者消息队列.<br>要注意这些列举的连接器是 Flink 工程的一部分,包含在发布的源码中,但是不包含在二进制发行版中.<br>更多说明可以参考对应的子部分.</p>
<h3 id="Apache-Bahir-中的连接器"><a href="#Apache-Bahir-中的连接器" class="headerlink" title="Apache Bahir 中的连接器"></a>Apache Bahir 中的连接器</h3><p>Flink 还有些一些额外的连接器通过 Apache Bahir 发布, 包括:<br>Apache ActiveMQ (source/sink)<br>Apache Flume (sink)<br>Redis (sink):<br>Akka (sink)<br>Netty (source)</p>
<h3 id="连接Flink的其他方法"><a href="#连接Flink的其他方法" class="headerlink" title="连接Flink的其他方法"></a>连接Flink的其他方法</h3><h4 id="异步-I-O"><a href="#异步-I-O" class="headerlink" title="异步 I/O"></a>异步 I/O</h4><p>使用connector并不是唯一可以使数据进入或者流出Flink的方式.<br>一种常见的模式是从外部数据库或者 Web 服务查询数据得到初始数据流,然后通过 Map 或者 FlatMap 对初始数据流进行丰富和增强.<br>Flink 提供了异步 I/O API 来让这个过程更加简单/高效和稳定.</p>
<h4 id="可查询状态"><a href="#可查询状态" class="headerlink" title="可查询状态"></a>可查询状态</h4><p>当 Flink 应用程序需要向外部存储推送大量数据时会导致 I/O 瓶颈问题出现.<br>在这种场景下,如果对数据的读操作远少于写操作,那么让外部应用从 Flink 拉取所需的数据会是一种更好的方式.<br>可查询状态 接口可以实现这个功能,该接口允许被 Flink 托管的状态可以被按需查询.</p>
<h2 id="Formats"><a href="#Formats" class="headerlink" title="Formats"></a>Formats</h2><h3 id="Avro-format"><a href="#Avro-format" class="headerlink" title="Avro format"></a>Avro format</h3><p>Flink 内置支持 Apache Avro 格式.<br>在 Flink 中将更容易地读写基于 Avro schema 的 Avro 数据.<br>Flink 的序列化框架可以处理基于 Avro schemas 生成的类.<br>为了能够使用 Avro format,需要在自动构建工具(例如 Maven 或 SBT)中添加如下依赖到项目中.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>如果读取 Avro 文件数据,你必须指定 AvroInputFormat.<br>示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AvroInputFormat&lt;User&gt; users = <span class="keyword">new</span> AvroInputFormat&lt;User&gt;(in, User.class);</span><br><span class="line">DataStream&lt;User&gt; usersDS = env.createInput(users);</span><br></pre></td></tr></table></figure>

<p>注意,User 是一个通过 Avro schema生成的 POJO 类.<br>Flink 还允许选择 POJO 中字符串类型的键.<br>例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usersDS.keyBy(<span class="string">&quot;name&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>注意,在 Flink 中可以使用 GenericData.Record 类型,但是不推荐使用.<br>因为该类型的记录中包含了完整的 schema,导致数据非常密集,使用起来可能很慢.</p>
<p>Flink 的 POJO 字段选择也适用于从 Avro schema 生成的 POJO 类.<br>但是,只有将字段类型正确写入生成的类时,才能使用.<br>如果字段是 Object 类型,则不能将该字段用作 join 键或 grouping 键.<br>在 Avro 中如 {&quot;name&quot;: &quot;type_double_test&quot;, &quot;type&quot;: &quot;double&quot;}, 这样指定字段是可行的,但是如 ({&quot;name&quot;: &quot;type_double_test&quot;, &quot;type&quot;: [&quot;double&quot;]},) 这样指定包含一个字段的复合类型就会生成 Object 类型的字段.<br>注意,如 ({&quot;name&quot;: &quot;type_double_test&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;double&quot;]},) 这样指定 nullable 类型字段也是可能产生 Object 类型的.</p>
<h3 id="CSV-format"><a href="#CSV-format" class="headerlink" title="CSV format"></a>CSV format</h3><p>要使用 CSV 格式,您需要将 Flink CSV 依赖项添加到您的项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Flink 支持使用CsvReaderFormat.<br>读者利用 Jackson 库并允许传递 CSV 模式和解析选项的相应配置.</p>
<p>CsvReaderFormat可以像这样初始化和使用:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CsvReaderFormat&lt;SomePojo&gt; csvFormat = CsvReaderFormat.forPojo(SomePojo.class);</span><br><span class="line">FileSource&lt;SomePojo&gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build();</span><br></pre></td></tr></table></figure>

<p>在这种情况下,用于 CSV 解析的模式是根据SomePojo使用该Jackson库的类的字段自动派生的.</p>
<p>注意:您可能需要在@JsonPropertyOrder({field1, field2, ...})类定义中添加注释,字段顺序与 CSV 文件列的顺序完全匹配.</p>
<h4 id="高级配置"><a href="#高级配置" class="headerlink" title="高级配置"></a>高级配置</h4><p>如果您需要对 CSV 架构或解析选项进行更细粒度的控制,请使用更底层的forSchema静态工厂方法CsvReaderFormat:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CsvReaderFormat&lt;T&gt; <span class="title">forSchema</span><span class="params">(CsvMapper mapper, CsvSchema schema, TypeInformation&lt;T&gt; typeInformation)</span> </span></span><br></pre></td></tr></table></figure>

<p>下面是一个使用自定义列分隔符读取 POJO 的示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Has to match the exact order of columns in the CSV file</span></span><br><span class="line"><span class="meta">@JsonPropertyOrder(&#123;&quot;city&quot;,&quot;lat&quot;,&quot;lng&quot;,&quot;country&quot;,&quot;iso2&quot;,</span></span><br><span class="line"><span class="meta">                    &quot;adminName&quot;,&quot;capital&quot;,&quot;population&quot;&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CityPojo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String city;</span><br><span class="line">    <span class="keyword">public</span> BigDecimal lat;</span><br><span class="line">    <span class="keyword">public</span> BigDecimal lng;</span><br><span class="line">    <span class="keyword">public</span> String country;</span><br><span class="line">    <span class="keyword">public</span> String iso2;</span><br><span class="line">    <span class="keyword">public</span> String adminName;</span><br><span class="line">    <span class="keyword">public</span> String capital;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> population;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CsvMapper mapper = <span class="keyword">new</span> CsvMapper();</span><br><span class="line">CsvSchema schema =</span><br><span class="line">        mapper.schemaFor(CityPojo.class).withoutQuoteChar().withColumnSeparator(<span class="string">&#x27;|&#x27;</span>);</span><br><span class="line"></span><br><span class="line">CsvReaderFormat&lt;CityPojo&gt; csvFormat =</span><br><span class="line">        CsvReaderFormat.forSchema(mapper, schema, TypeInformation.of(CityPojo.class));</span><br><span class="line"></span><br><span class="line">FileSource&lt;CityPojo&gt; source =</span><br><span class="line">        FileSource.forRecordStreamFormat(csvFormat,Path.fromLocalFile(...)).build();</span><br></pre></td></tr></table></figure>

<p>对应的 CSV 文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Berlin|52.5167|13.3833|Germany|DE|Berlin|primary|3644826</span><br><span class="line">San Francisco|37.7562|-122.443|United States|US|California||3592294</span><br><span class="line">Beijing|39.905|116.3914|China|CN|Beijing|primary|19433000</span><br></pre></td></tr></table></figure>

<p>还可以使用细粒度Jackson设置读取更复杂的数据类型:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ComplexPojo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> id;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] array;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CsvReaderFormat&lt;ComplexPojo&gt; csvFormat = CsvReaderFormat.forSchema(</span><br><span class="line">                <span class="keyword">new</span> CsvMapper(),</span><br><span class="line">                CsvSchema.builder()</span><br><span class="line">                        .addColumn(<span class="keyword">new</span> CsvSchema.Column(<span class="number">0</span>, <span class="string">&quot;id&quot;</span>, CsvSchema.ColumnType.NUMBER))</span><br><span class="line">                        .addColumn(<span class="keyword">new</span> CsvSchema.Column(<span class="number">4</span>, <span class="string">&quot;array&quot;</span>, CsvSchema.ColumnType.ARRAY).withArrayElementSeparator(<span class="string">&quot;#&quot;</span>))</span><br><span class="line">                        .build(),</span><br><span class="line">                TypeInformation.of(ComplexPojo.class)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>对应的 CSV 文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0,1#2#3</span><br><span class="line">1,</span><br><span class="line">2,1</span><br></pre></td></tr></table></figure>

<p>与TextLineInputFormat类似,CsvReaderFormat可用于继续模式和批处理模式(参见TextLineInputFormat 示例).</p>
<h3 id="Hadoop-formats"><a href="#Hadoop-formats" class="headerlink" title="Hadoop formats"></a>Hadoop formats</h3><h4 id="Project-Configuration"><a href="#Project-Configuration" class="headerlink" title="Project Configuration"></a>Project Configuration</h4><p>对 Hadoop 的支持位于 flink-hadoop-compatibility Maven 模块中.</p>
<p>将以下依赖添加到 pom.xml 中使用 hadoop</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>如果你想在本地运行你的 Flink 应用(例如在 IDE 中),你需要按照如下所示将 hadoop-client 依赖也添加到 pom.xml:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="Using-Hadoop-InputFormats"><a href="#Using-Hadoop-InputFormats" class="headerlink" title="Using Hadoop InputFormats"></a>Using Hadoop InputFormats</h4><p>在 Flink 中使用 Hadoop InputFormats,必须首先使用 HadoopInputs 工具类的 readHadoopFile 或 createHadoopInput 包装 Input Format.<br>前者用于从 FileInputFormat 派生的 Input Format,而后者必须用于通用的 Input Format.<br>生成的 InputFormat 可通过使用 ExecutionEnvironmen#createInput 创建数据源.</p>
<p>生成的 DataStream 包含 2 元组,其中第一个字段是键,第二个字段是从 Hadoop InputFormat 接收的值.<br>下面的示例展示了如何使用 Hadoop 的 TextInputFormat.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;LongWritable, Text&gt;&gt; input =</span><br><span class="line">    env.createInput(HadoopInputs.readHadoopFile(<span class="keyword">new</span> TextInputFormat(),</span><br><span class="line">                        LongWritable.class, Text.class, textPath));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对数据进行一些处理.</span></span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<h4 id="Using-Hadoop-OutputFormats"><a href="#Using-Hadoop-OutputFormats" class="headerlink" title="Using Hadoop OutputFormats"></a>Using Hadoop OutputFormats</h4><p>Flink 为 Hadoop OutputFormats 提供了一个兼容性包装器.<br>支持任何实现 org.apache.hadoop.mapred.OutputFormat 或扩展 org.apache.hadoop.mapreduce.OutputFormat 的类.<br>OutputFormat 包装器期望其输入数据是包含键和值的 2-元组的 DataSet.<br>这些将由 Hadoop OutputFormat 处理.</p>
<p>下面的示例展示了如何使用 Hadoop 的 TextOutputFormat.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取我们希望发送的结果</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Text, IntWritable&gt;&gt; hadoopResult = [...];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 the Hadoop TextOutputFormat.</span></span><br><span class="line">HadoopOutputFormat&lt;Text, IntWritable&gt; hadoopOF =</span><br><span class="line">  <span class="comment">// 创建 Flink wrapper.</span></span><br><span class="line">  <span class="keyword">new</span> HadoopOutputFormat&lt;Text, IntWritable&gt;(</span><br><span class="line">    <span class="comment">// 设置 Hadoop OutputFormat 并指定 job.</span></span><br><span class="line">    <span class="keyword">new</span> TextOutputFormat&lt;Text, IntWritable&gt;(), job</span><br><span class="line">  );</span><br><span class="line">hadoopOF.getConfiguration().set(<span class="string">&quot;mapreduce.output.textoutputformat.separator&quot;</span>, <span class="string">&quot; &quot;</span>);</span><br><span class="line">TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outputPath));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 Hadoop TextOutputFormat 发送数据.</span></span><br><span class="line">hadoopResult.output(hadoopOF);</span><br></pre></td></tr></table></figure>

<h3 id="MongoDB-format"><a href="#MongoDB-format" class="headerlink" title="MongoDB format"></a>MongoDB format</h3><h3 id="Parquet-format"><a href="#Parquet-format" class="headerlink" title="Parquet format"></a>Parquet format</h3><p>Flink 支持读取 Parquet 文件并生成 Flink RowData 和 Avro 记录.<br>要使用 Parquet format,你需要将 flink-parquet 依赖添加到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-parquet<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>要使用 Avro 格式,你需要将 parquet-avro 依赖添加到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.parquet<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>parquet-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>it.unimi.dsi<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastutil<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>此格式与新的 Source 兼容,可以同时在批和流模式下使用.<br>因此,你可使用此格式处理以下两类数据:</p>
<ol>
<li>有界数据:列出所有文件并全部读取.</li>
<li>无界数据:监控目录中出现的新文件</li>
</ol>
<p>当你开启一个 File Source,会被默认为有界读取.<br>如果你想在连续读取模式下使用 File Source,你必须额外调用 <code>AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration)</code>.</p>
<p>Vectorized reader</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Parquet rows are decoded in batches</span></span><br><span class="line">FileSource.forBulkFileFormat(BulkFormat,Path...)</span><br><span class="line"><span class="comment">// Monitor the Paths to read data as unbounded data</span></span><br><span class="line">FileSource.forBulkFileFormat(BulkFormat,Path...)</span><br><span class="line">.monitorContinuously(Duration.ofMillis(<span class="number">5L</span>))</span><br><span class="line">.build();</span><br></pre></td></tr></table></figure>

<p>Avro Parquet reader</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Parquet rows are decoded in batches</span></span><br><span class="line">FileSource.forRecordStreamFormat(StreamFormat,Path...)</span><br><span class="line"><span class="comment">// Monitor the Paths to read data as unbounded data</span></span><br><span class="line">FileSource.forRecordStreamFormat(StreamFormat,Path...)</span><br><span class="line">        .monitorContinuously(Duration.ofMillis(<span class="number">5L</span>))</span><br><span class="line">        .build();</span><br></pre></td></tr></table></figure>

<p>下面的案例都是基于有界数据的.<br>如果你想在连续读取模式下使用 File Source,你必须额外调用 AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration).</p>
<h4 id="Flink-RowData"><a href="#Flink-RowData" class="headerlink" title="Flink RowData"></a>Flink RowData</h4><p>在此示例中,你将创建由 Parquet 格式的记录构成的 Flink RowDatas DataStream.<br>我们把 schema 信息映射为只读字段(&quot;f7&quot;/&quot;f4&quot;/&quot;f99&quot;).<br>每个批次读取 500 条记录.<br>其中,第一个布尔类型的参数用来指定是否需要将时间戳列处理为 UTC.<br>第二个布尔类型参数用来指定在进行 Parquet 字段映射时,是否要区分大小写.<br>这里不需要水印策略,因为记录中不包含事件时间戳.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> LogicalType[] fieldTypes =</span><br><span class="line">        <span class="keyword">new</span> LogicalType[] &#123;</span><br><span class="line">        <span class="keyword">new</span> DoubleType(), <span class="keyword">new</span> IntType(), <span class="keyword">new</span> VarCharType()</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> ParquetColumnarRowInputFormat&lt;FileSourceSplit&gt; format =</span><br><span class="line">        <span class="keyword">new</span> ParquetColumnarRowInputFormat&lt;&gt;(</span><br><span class="line">        <span class="keyword">new</span> Configuration(),</span><br><span class="line">        RowType.of(fieldTypes, <span class="keyword">new</span> String[] &#123;<span class="string">&quot;f7&quot;</span>, <span class="string">&quot;f4&quot;</span>, <span class="string">&quot;f99&quot;</span>&#125;),</span><br><span class="line">        <span class="number">500</span>,</span><br><span class="line">        <span class="keyword">false</span>,</span><br><span class="line">        <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">final</span> FileSource&lt;RowData&gt; source =</span><br><span class="line">        FileSource.forBulkFileFormat(format,  <span class="comment">/* Flink Path */</span>)</span><br><span class="line">        .build();</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;RowData&gt; stream =</span><br><span class="line">        env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="Avro-Records"><a href="#Avro-Records" class="headerlink" title="Avro Records"></a>Avro Records</h4><p>Flink 支持三种方式来读取 Parquet 文件并创建 Avro records :<br>Generic record<br>Specific record<br>Reflect record</p>
<h5 id="Generic-record"><a href="#Generic-record" class="headerlink" title="Generic record"></a>Generic record</h5><p>使用 JSON 定义 Avro schemas.<br>你可以从 Avro specification 获取更多关于 Avro schemas 和类型的信息.<br>此示例使用了一个在 official Avro tutorial 中描述的示例相似的 Avro schema:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;namespace&quot;</span>: <span class="string">&quot;example.avro&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;record&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;User&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;fields&quot;</span>: [</span><br><span class="line">    &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;name&quot;</span>, <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;favoriteNumber&quot;</span>,  <span class="attr">&quot;type&quot;</span>: [<span class="string">&quot;int&quot;</span>, <span class="string">&quot;null&quot;</span>]&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;favoriteColor&quot;</span>, <span class="attr">&quot;type&quot;</span>: [<span class="string">&quot;string&quot;</span>, <span class="string">&quot;null&quot;</span>]&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个 schema 定义了一个具有三个属性的的 user 记录:name,favoriteNumber 和 favoriteColor.<br>你可以 在 record specification 找到更多关于如何定义 Avro schema 的详细信息.</p>
<p>在此示例中,你将创建包含由 Avro Generic records 格式构成的 Parquet records 的 DataStream.<br>Flink 会基于 JSON 字符串解析 Avro schema.<br>也有很多其他的方式解析 schema,例如基于 java.io.File 或 java.io.InputStream.<br>请参考 Avro Schema 以获取更多详细信息.<br>然后,你可以通过 AvroParquetReaders 为 Avro Generic 记录创建 AvroParquetRecordFormat.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 解析 avro schema</span></span><br><span class="line"><span class="keyword">final</span> Schema schema =</span><br><span class="line">        <span class="keyword">new</span> Schema.Parser()</span><br><span class="line">        .parse(</span><br><span class="line">        <span class="string">&quot;&#123;\&quot;type\&quot;: \&quot;record\&quot;, &quot;</span></span><br><span class="line">        + <span class="string">&quot;\&quot;name\&quot;: \&quot;User\&quot;, &quot;</span></span><br><span class="line">        + <span class="string">&quot;\&quot;fields\&quot;: [\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;        &#123;\&quot;name\&quot;: \&quot;name\&quot;, \&quot;type\&quot;: \&quot;string\&quot; &#125;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;        &#123;\&quot;name\&quot;: \&quot;favoriteNumber\&quot;,  \&quot;type\&quot;: [\&quot;int\&quot;, \&quot;null\&quot;] &#125;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;        &#123;\&quot;name\&quot;: \&quot;favoriteColor\&quot;, \&quot;type\&quot;: [\&quot;string\&quot;, \&quot;null\&quot;] &#125;\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    ]\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    &#125;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSource&lt;GenericRecord&gt; source =</span><br><span class="line">        FileSource.forRecordStreamFormat(</span><br><span class="line">        AvroParquetReaders.forGenericRecord(schema), <span class="comment">/* Flink Path */</span>)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(<span class="number">10L</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> DataStream&lt;GenericRecord&gt; stream =</span><br><span class="line">        env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="Specific-record"><a href="#Specific-record" class="headerlink" title="Specific record"></a>Specific record</h5><p>基于之前定义的 schema,你可以通过利用 Avro 代码生成来生成类.<br>一旦生成了类,就不需要在程序中直接使用 schema.<br>你可以使用 avro-tools.jar 手动生成代码,也可以直接使用 Avro Maven 插件对配置的源目录中的任何 .avsc 文件执行代码生成.<br>请参考 Avro Getting Started 获取更多信息.</p>
<p>此示例使用了样例 schema testdata.avsc:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;<span class="attr">&quot;namespace&quot;</span>: <span class="string">&quot;org.apache.flink.formats.parquet.generated&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;record&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;Address&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;fields&quot;</span>: [</span><br><span class="line">      &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;num&quot;</span>, <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;int&quot;</span>&#125;,</span><br><span class="line">      &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;street&quot;</span>, <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;,</span><br><span class="line">      &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;city&quot;</span>, <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;,</span><br><span class="line">      &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;state&quot;</span>, <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;,</span><br><span class="line">      &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;zip&quot;</span>, <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>你可以使用 Avro Maven plugin 生成 Address Java 类.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@org</span>.apache.avro.specific.AvroGenerated</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Address</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">avro</span>.<span class="title">specific</span>.<span class="title">SpecificRecordBase</span> <span class="keyword">implements</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">avro</span>.<span class="title">specific</span>.<span class="title">SpecificRecord</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 生成的代码...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>你可以通过 AvroParquetReaders 为 Avro Specific 记录创建 AvroParquetRecordFormat, 然后创建一个包含由 Avro Specific records 格式构成的 Parquet records 的 DateStream.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> FileSource&lt;GenericRecord&gt; source =</span><br><span class="line">        FileSource.forRecordStreamFormat(</span><br><span class="line">                AvroParquetReaders.forSpecificRecord(Address.class), <span class="comment">/* Flink Path */</span>)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(<span class="number">10L</span>);</span><br><span class="line">        </span><br><span class="line"><span class="keyword">final</span> DataStream&lt;GenericRecord&gt; stream =</span><br><span class="line">        env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="Reflect-record"><a href="#Reflect-record" class="headerlink" title="Reflect record"></a>Reflect record</h5><p>除了需要预定义 Avro Generic 和 Specific 记录, Flink 还支持基于现有 Java POJO 类从 Parquet 文件创建 DateStream.<br>在这种场景中,Avro 会使用 Java 反射为这些 POJO 类生成 schema 和协议.<br>请参考 Avro reflect 文档获取更多关于 Java 类型到 Avro schemas 映射的详细信息.</p>
<p>本例使用了一个简单的 Java POJO 类 Datum:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Datum</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String a;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> b;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Datum</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Datum</span><span class="params">(String a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.a = a;</span><br><span class="line">        <span class="keyword">this</span>.b = b;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span> == o) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="keyword">null</span> || getClass() != o.getClass()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Datum datum = (Datum) o;</span><br><span class="line">        <span class="keyword">return</span> b == datum.b &amp;&amp; (a != <span class="keyword">null</span> ? a.equals(datum.a) : datum.a == <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = a != <span class="keyword">null</span> ? a.hashCode() : <span class="number">0</span>;</span><br><span class="line">        result = <span class="number">31</span> * result + b;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>你可以通过 AvroParquetReaders 为 Avro Reflect 记录创建一个 AvroParquetRecordFormat, 然后创建一个包含由 Avro Reflect records 格式构成的 Parquet records 的 DateStream.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> FileSource&lt;GenericRecord&gt; source =</span><br><span class="line">        FileSource.forRecordStreamFormat(</span><br><span class="line">                AvroParquetReaders.forReflectRecord(Datum.class), <span class="comment">/* Flink Path */</span>)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(<span class="number">10L</span>);</span><br><span class="line">        </span><br><span class="line"><span class="keyword">final</span> DataStream&lt;GenericRecord&gt; stream =</span><br><span class="line">        env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="使用-Parquet-files-必备条件"><a href="#使用-Parquet-files-必备条件" class="headerlink" title="使用 Parquet files 必备条件"></a>使用 Parquet files 必备条件</h5><p>为了支持读取 Avro Reflect 数据,Parquet 文件必须包含特定的 meta 信息.<br>为了生成 Parquet 数据,Avro schema 信息中必须包含 namespace, 以便让程序在反射执行过程中能确定唯一的 Java Class 对象.</p>
<p>下面的案例展示了上文中的 User 对象的 schema 信息.<br>但是当前案例包含了一个指定文件目录的 namespace(当前案例下的包路径),反射过程中可以找到对应的 User 类.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// avro schema with namespace</span></span><br><span class="line"><span class="keyword">final</span> String schema = </span><br><span class="line">                    <span class="string">&quot;&#123;\&quot;type\&quot;: \&quot;record\&quot;, &quot;</span></span><br><span class="line">                        + <span class="string">&quot;\&quot;name\&quot;: \&quot;User\&quot;, &quot;</span></span><br><span class="line">                        + <span class="string">&quot;\&quot;namespace\&quot;: \&quot;org.apache.flink.formats.parquet.avro\&quot;, &quot;</span></span><br><span class="line">                        + <span class="string">&quot;\&quot;fields\&quot;: [\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;        &#123;\&quot;name\&quot;: \&quot;name\&quot;, \&quot;type\&quot;: \&quot;string\&quot; &#125;,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;        &#123;\&quot;name\&quot;: \&quot;favoriteNumber\&quot;,  \&quot;type\&quot;: [\&quot;int\&quot;, \&quot;null\&quot;] &#125;,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;        &#123;\&quot;name\&quot;: \&quot;favoriteColor\&quot;, \&quot;type\&quot;: [\&quot;string\&quot;, \&quot;null\&quot;] &#125;\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;    ]\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;    &#125;&quot;</span>;</span><br></pre></td></tr></table></figure>

<p>由上述 scheme 信息创建的 Parquet 文件包含以下 meta 信息:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">creator:        parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)</span><br><span class="line">extra:          parquet.avro.schema &#x3D;</span><br><span class="line">&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;User&quot;,&quot;namespace&quot;:&quot;org.apache.flink.formats.parquet.avro&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;name&quot;,&quot;type&quot;:&quot;string&quot;&#125;,&#123;&quot;name&quot;:&quot;favoriteNumber&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;favoriteColor&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;</span><br><span class="line">extra:          writer.model.name &#x3D; avro</span><br><span class="line"></span><br><span class="line">file schema:    org.apache.flink.formats.parquet.avro.User</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">name:           REQUIRED BINARY L:STRING R:0 D:0</span><br><span class="line">favoriteNumber: OPTIONAL INT32 R:0 D:1</span><br><span class="line">favoriteColor:  OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line"></span><br><span class="line">row group 1:    RC:3 TS:143 OFFSET:4</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">name:            BINARY UNCOMPRESSED DO:0 FPO:4 SZ:47&#x2F;47&#x2F;1.00 VC:3 ENC:PLAIN,BIT_PACKED ST:[min: Jack, max: Tom, num_nulls: 0]</span><br><span class="line">favoriteNumber:  INT32 UNCOMPRESSED DO:0 FPO:51 SZ:41&#x2F;41&#x2F;1.00 VC:3 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 1, max: 3, num_nulls: 0]</span><br><span class="line">favoriteColor:   BINARY UNCOMPRESSED DO:0 FPO:92 SZ:55&#x2F;55&#x2F;1.00 VC:3 ENC:RLE,PLAIN,BIT_PACKED ST:[min: green, max: yellow, num_nulls: 0]</span><br></pre></td></tr></table></figure>

<p>使用包 org.apache.flink.formats.parquet.avro 路径下已定义的 User 类:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> Integer favoriteNumber;</span><br><span class="line">    <span class="keyword">private</span> String favoriteColor;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String name, Integer favoriteNumber, String favoriteColor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.favoriteNumber = favoriteNumber;</span><br><span class="line">        <span class="keyword">this</span>.favoriteColor = favoriteColor;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getFavoriteNumber</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> favoriteNumber;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getFavoriteColor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> favoriteColor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>你可以通过下面的程序读取类型为 User 的 Avro Reflect records:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> FileSource&lt;GenericRecord&gt; source =</span><br><span class="line">        FileSource.forRecordStreamFormat(</span><br><span class="line">        AvroParquetReaders.forReflectRecord(User.class), <span class="comment">/* Flink Path */</span>)</span><br><span class="line">        .build();</span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(<span class="number">10L</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> DataStream&lt;GenericRecord&gt; stream =</span><br><span class="line">        env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="Text-files-format"><a href="#Text-files-format" class="headerlink" title="Text files format"></a>Text files format</h3><p>Flink 支持使用 TextLineInputFormat 从文件中读取文本行.<br>此 format 使用 Java 的内置 InputStreamReader 以支持的字符集编码来解码字节流.<br>要使用该 format,你需要将 Flink Connector Files 依赖项添加到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-files<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>此 format 与新 Source 兼容,可以在批处理和流模式下使用.<br>因此,你可以通过两种方式使用此 format:</p>
<ol>
<li>批处理模式的有界读取</li>
<li>流模式的连续读取:监视目录中出现的新文件</li>
</ol>
<h4 id="有界读取示例"><a href="#有界读取示例" class="headerlink" title="有界读取示例"></a>有界读取示例</h4><p>在此示例中,我们创建了一个 DataStream,其中包含作为字符串的文本文件的行.<br>此处不需要水印策略,因为记录不包含事件时间戳.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> FileSource&lt;String&gt; source =</span><br><span class="line">  FileSource.forRecordStreamFormat(<span class="keyword">new</span> TextLineInputFormat(), <span class="comment">/* Flink Path */</span>)</span><br><span class="line">  .build();</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; stream =</span><br><span class="line">  env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="连续读取示例"><a href="#连续读取示例" class="headerlink" title="连续读取示例"></a>连续读取示例</h4><p>在此示例中,我们创建了一个 DataStream,随着新文件被添加到目录中,其中包含的文本文件行的字符串流将无限增长.<br>我们每秒会进行新文件监控.<br>此处不需要水印策略,因为记录不包含事件时间戳.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> FileSource&lt;String&gt; source =</span><br><span class="line">    FileSource.forRecordStreamFormat(<span class="keyword">new</span> TextLineInputFormat(), <span class="comment">/* Flink Path */</span>)</span><br><span class="line">  .monitorContinuously(Duration.ofSeconds(<span class="number">1L</span>))</span><br><span class="line">  .build();</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; stream =</span><br><span class="line">  env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;file-source&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="容错保证"><a href="#容错保证" class="headerlink" title="容错保证"></a>容错保证</h2><p>只有当 source 参与了快照机制的时候,Flink 才能保证对自定义状态的精确一次更新.<br>为了保证端到端精确一次的数据交付(在精确一次的状态语义上更进一步),sink需要参与 checkpointing 机制.</p>
<h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p>Flink 提供了 Apache Kafka 连接器使用精确一次(Exactly-once)的语义在 Kafka topic 中读取和写入数据.</p>
<h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>Apache Flink 集成了通用的 Kafka 连接器,它会尽力与 Kafka client 的最新版本保持同步.<br>该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化.<br>当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker.<br>有关 Kafka 兼容性的更多细节,请参考 Kafka 官方文档.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>如果使用 Kafka source,flink-connector-base 也需要包含在依赖中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-base<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Flink 目前的流连接器还不是二进制发行版的一部分.<br>在此处可以了解到如何链接它们,从而在集群中运行.</p>
<h3 id="Kafka-Source"><a href="#Kafka-Source" class="headerlink" title="Kafka Source"></a>Kafka Source</h3><p>该文档描述的是基于新数据源 API 的 Kafka Source.</p>
<h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><p>Kafka Source 提供了构建类来创建 KafkaSource 的实例.<br>以下代码片段展示了如何构建 KafkaSource 来消费 &quot;input-topic&quot; 最早位点的数据, 使用消费组 &quot;my-group&quot;,并且将 Kafka 消息体反序列化为字符串:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">    .setBootstrapServers(brokers)</span><br><span class="line">    .setTopics(<span class="string">&quot;input-topic&quot;</span>)</span><br><span class="line">    .setGroupId(<span class="string">&quot;my-group&quot;</span>)</span><br><span class="line">    .setStartingOffsets(OffsetsInitializer.earliest())</span><br><span class="line">    .setValueOnlyDeserializer(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;Kafka Source&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>以下属性在构建 KafkaSource 时是必须指定的:</p>
<ol>
<li>Bootstrap server,通过 setBootstrapServers(String) 方法配置</li>
<li>消费者组 ID,通过 setGroupId(String) 配置</li>
<li>要订阅的 Topic / Partition,请参阅 Topic / Partition 订阅一节</li>
<li>用于解析 Kafka 消息的反序列化器(Deserializer),请参阅消息解析一节</li>
</ol>
<h4 id="Topic-Partition-订阅"><a href="#Topic-Partition-订阅" class="headerlink" title="Topic / Partition 订阅"></a>Topic / Partition 订阅</h4><p>Kafka Source 提供了 3 种 Topic / Partition 的订阅方式:</p>
<ol>
<li><p>Topic 列表,订阅 Topic 列表中所有 Partition 的消息:<br><code>KafkaSource.builder().setTopics(&quot;topic-a&quot;, &quot;topic-b&quot;);</code></p>
</li>
<li><p>正则表达式匹配,订阅与正则表达式所匹配的 Topic 下的所有 Partition:<br><code>KafkaSource.builder().setTopicPattern(&quot;topic.*&quot;);</code></p>
</li>
<li><p>Partition 列表,订阅指定的 Partition:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> HashSet&lt;TopicPartition&gt; partitionSet = <span class="keyword">new</span> HashSet&lt;&gt;(Arrays.asList(</span><br><span class="line">        <span class="keyword">new</span> TopicPartition(<span class="string">&quot;topic-a&quot;</span>, <span class="number">0</span>),    <span class="comment">// Partition 0 of topic &quot;topic-a&quot;</span></span><br><span class="line">        <span class="keyword">new</span> TopicPartition(<span class="string">&quot;topic-b&quot;</span>, <span class="number">5</span>)));  <span class="comment">// Partition 5 of topic &quot;topic-b&quot;</span></span><br><span class="line">KafkaSource.builder().setPartitions(partitionSet);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="消息解析"><a href="#消息解析" class="headerlink" title="消息解析"></a>消息解析</h4><p>代码中需要提供一个反序列化器(Deserializer)来对 Kafka 的消息进行解析.<br>反序列化器通过 setDeserializer(KafkaRecordDeserializationSchema) 来指定,其中 KafkaRecordDeserializationSchema 定义了如何解析 Kafka 的 ConsumerRecord.</p>
<p>如果只需要 Kafka 消息中的消息体(value)部分的数据,可以使用 KafkaSource 构建类中的 setValueOnlyDeserializer(DeserializationSchema) 方法,其中 DeserializationSchema 定义了如何解析 Kafka 消息体中的二进制数据.</p>
<p>也可使用 Kafka 提供的解析器 来解析 Kafka 消息体.<br>例如使用 StringDeserializer 来将 Kafka 消息体解析成字符串:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line">KafkaSource.&lt;String&gt;builder()</span><br><span class="line">        .setDeserializer(KafkaRecordDeserializationSchema.valueOnly(StringDeserializer.class));</span><br></pre></td></tr></table></figure>

<h4 id="起始消费位点"><a href="#起始消费位点" class="headerlink" title="起始消费位点"></a>起始消费位点</h4><p>Kafka source 能够通过位点初始化器(OffsetsInitializer)来指定从不同的偏移量开始消费 .<br>内置的位点初始化器包括:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">KafkaSource.builder()</span><br><span class="line">    <span class="comment">// 从消费组提交的位点开始消费,不指定位点重置策略</span></span><br><span class="line">    .setStartingOffsets(OffsetsInitializer.committedOffsets())</span><br><span class="line">    <span class="comment">// 从消费组提交的位点开始消费,如果提交位点不存在,使用最早位点</span></span><br><span class="line">    .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST))</span><br><span class="line">    <span class="comment">// 从时间戳大于等于指定时间戳(毫秒)的数据开始消费</span></span><br><span class="line">    .setStartingOffsets(OffsetsInitializer.timestamp(<span class="number">1657256176000L</span>))</span><br><span class="line">    <span class="comment">// 从最早位点开始消费</span></span><br><span class="line">    .setStartingOffsets(OffsetsInitializer.earliest())</span><br><span class="line">    <span class="comment">// 从最末尾位点开始消费</span></span><br><span class="line">    .setStartingOffsets(OffsetsInitializer.latest());</span><br></pre></td></tr></table></figure>

<p>如果内置的初始化器不能满足需求,也可以实现自定义的位点初始化器(OffsetsInitializer).<br>如果未指定位点初始化器,将默认使用 OffsetsInitializer.earliest().</p>
<h4 id="有界-无界模式"><a href="#有界-无界模式" class="headerlink" title="有界 / 无界模式"></a>有界 / 无界模式</h4><p>Kafka Source 支持流式和批式两种运行模式.<br>默认情况下,KafkaSource 设置为以流模式运行,因此作业永远不会停止,直到 Flink 作业失败或被取消.<br>可以使用 setBounded(OffsetsInitializer) 指定停止偏移量使 Kafka Source 以批处理模式运行.<br>当所有分区都达到其停止偏移量时,Kafka Source 会退出运行.</p>
<p>流模式下运行通过使用 setUnbounded(OffsetsInitializer) 也可以指定停止消费位点,当所有分区达到其指定的停止偏移量时,Kafka Source 会退出运行.</p>
<h4 id="其他属性"><a href="#其他属性" class="headerlink" title="其他属性"></a>其他属性</h4><p>除了上述属性之外,您还可以使用 setProperties(Properties) 和 setProperty(String, String) 为 Kafka Source 和 Kafka Consumer 设置任意属性.<br>KafkaSource 有以下配置项:</p>
<ol>
<li>client.id.prefix,指定用于 Kafka Consumer 的客户端 ID 前缀</li>
<li>partition.discovery.interval.ms,定义 Kafka Source 检查新分区的时间间隔.</li>
<li>register.consumer.metrics 指定是否在 Flink 中注册 Kafka Consumer 的指标</li>
<li>commit.offsets.on.checkpoint 指定是否在进行 checkpoint 时将消费位点提交至 Kafka broker</li>
</ol>
<p>Kafka consumer 的配置可以参考 Apache Kafka 文档.</p>
<p>请注意,即使指定了以下配置项,构建器也会将其覆盖:</p>
<ol>
<li>key.deserializer 始终设置为 ByteArrayDeserializer</li>
<li>value.deserializer 始终设置为 ByteArrayDeserializer</li>
<li>auto.offset.reset.strategy 被 OffsetsInitializer#getAutoOffsetResetStrategy() 覆盖</li>
<li>partition.discovery.interval.ms 会在批模式下被覆盖为 -1</li>
</ol>
<h4 id="动态分区检查"><a href="#动态分区检查" class="headerlink" title="动态分区检查"></a>动态分区检查</h4><p>为了在不重启 Flink 作业的情况下处理 Topic 扩容或新建 Topic 等场景,可以将 Kafka Source 配置为在提供的 Topic / Partition 订阅模式下定期检查新分区.<br>要启用动态分区检查,请将 partition.discovery.interval.ms 设置为非负值:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KafkaSource.builder()</span><br><span class="line">    .setProperty(<span class="string">&quot;partition.discovery.interval.ms&quot;</span>, <span class="string">&quot;10000&quot;</span>); <span class="comment">// 每 10 秒检查一次新分区</span></span><br></pre></td></tr></table></figure>

<p>分区检查功能默认不开启.<br>需要显式地设置分区检查间隔才能启用此功能.</p>
<h4 id="事件时间和水印"><a href="#事件时间和水印" class="headerlink" title="事件时间和水印"></a>事件时间和水印</h4><p>默认情况下,Kafka Source 使用 Kafka 消息中的时间戳作为事件时间.<br>您可以定义自己的水印策略(Watermark Strategy) 以从消息中提取事件时间,并向下游发送水印:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromSource(kafkaSource, <span class="keyword">new</span> CustomWatermarkStrategy(), <span class="string">&quot;Kafka Source With Custom Watermark Strategy&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="空闲"><a href="#空闲" class="headerlink" title="空闲"></a>空闲</h4><p>如果并行度高于分区数,Kafka Source 不会自动进入空闲状态.<br>您将需要降低并行度或向水印策略添加空闲超时.<br>如果在这段时间内没有记录在流的分区中流动,则该分区被视为&quot;空闲&quot;并且不会阻止下游操作符中水印的进度.<br>这篇文档 描述了有关如何定义 <code>WatermarkStrategy#withIdleness</code> 的详细信息.</p>
<h4 id="消费位点提交"><a href="#消费位点提交" class="headerlink" title="消费位点提交"></a>消费位点提交</h4><p>Kafka source 在 checkpoint 完成时提交当前的消费位点 ,以保证 Flink 的 checkpoint 状态和 Kafka broker 上的提交位点一致.<br>如果未开启 checkpoint,Kafka source 依赖于 Kafka consumer 内部的位点定时自动提交逻辑,自动提交功能由 enable.auto.commit 和 auto.commit.interval.ms 两个 Kafka consumer 配置项进行配置.</p>
<p>注意:Kafka source 不依赖于 broker 上提交的位点来恢复失败的作业.<br>提交位点只是为了上报 Kafka consumer 和消费组的消费进度,以在 broker 端进行监控.</p>
<h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><p>Kafka source 会在不同的范围 (Scope)中汇报下列指标.</p>
<img src="/images/flgl117.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl118.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>该指标反映了最后一条数据的瞬时值.<br>之所以提供瞬时值是因为统计延迟直方图会消耗更多资源,瞬时值通常足以很好地反映延迟.</p>
<h5 id="Kafka-Consumer-指标"><a href="#Kafka-Consumer-指标" class="headerlink" title="Kafka Consumer 指标"></a>Kafka Consumer 指标</h5><p>Kafka consumer 的所有指标都注册在指标组 KafkaSourceReader.KafkaConsumer 下.<br>例如 Kafka consumer 的指标 records-consumed-total 将在该 Flink 指标中汇报: <code>&lt;some_parent_groups&gt;.operator.KafkaSourceReader.KafkaConsumer.records-consumed-total</code>.</p>
<p>您可以使用配置项 register.consumer.metrics 配置是否注册 Kafka consumer 的指标 .<br>默认此选项设置为 true.</p>
<p>关于 Kafka consumer 的指标,您可以参考 Apache Kafka 文档 了解更多详细信息.</p>
<h4 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h4><p>要启用加密和认证相关的安全配置,只需将安全配置作为其他属性配置在 Kafka source 上即可.<br>下面的代码片段展示了如何配置 Kafka source 以使用 PLAIN 作为 SASL 机制并提供 JAAS 配置:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">KafkaSource.builder()</span><br><span class="line">    .setProperty(<span class="string">&quot;security.protocol&quot;</span>, <span class="string">&quot;SASL_PLAINTEXT&quot;</span>)</span><br><span class="line">    .setProperty(<span class="string">&quot;sasl.mechanism&quot;</span>, <span class="string">&quot;PLAIN&quot;</span>)</span><br><span class="line">    .setProperty(<span class="string">&quot;sasl.jaas.config&quot;</span>, <span class="string">&quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;username\&quot; password=\&quot;password\&quot;;&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>另一个更复杂的例子,使用 SASL_SSL 作为安全协议并使用 SCRAM-SHA-256 作为 SASL 机制:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaSource.builder()</span><br><span class="line">    .setProperty(<span class="string">&quot;security.protocol&quot;</span>, <span class="string">&quot;SASL_SSL&quot;</span>)</span><br><span class="line">    <span class="comment">// SSL 配置</span></span><br><span class="line">    <span class="comment">// 配置服务端提供的 truststore (CA 证书) 的路径</span></span><br><span class="line">    <span class="comment">// Configure the path of truststore (CA) provided by the server</span></span><br><span class="line">    .setProperty(<span class="string">&quot;ssl.truststore.location&quot;</span>, <span class="string">&quot;/path/to/kafka.client.truststore.jks&quot;</span>)</span><br><span class="line">    .setProperty(<span class="string">&quot;ssl.truststore.password&quot;</span>, <span class="string">&quot;test1234&quot;</span>)</span><br><span class="line">    <span class="comment">// 如果要求客户端认证,则需要配置 keystore (私钥) 的路径</span></span><br><span class="line">    <span class="comment">// Configure the path of keystore (private key) if client authentication is required</span></span><br><span class="line">    .setProperty(<span class="string">&quot;ssl.keystore.location&quot;</span>, <span class="string">&quot;/path/to/kafka.client.keystore.jks&quot;</span>)</span><br><span class="line">    .setProperty(<span class="string">&quot;ssl.keystore.password&quot;</span>, <span class="string">&quot;test1234&quot;</span>)</span><br><span class="line">    <span class="comment">// SASL 配置</span></span><br><span class="line">    <span class="comment">// 将 SASL 机制配置为 as SCRAM-SHA-256</span></span><br><span class="line">    .setProperty(<span class="string">&quot;sasl.mechanism&quot;</span>, <span class="string">&quot;SCRAM-SHA-256&quot;</span>)</span><br><span class="line">    <span class="comment">// 配置 JAAS</span></span><br><span class="line">    .setProperty(<span class="string">&quot;sasl.jaas.config&quot;</span>, <span class="string">&quot;org.apache.kafka.common.security.scram.ScramLoginModule required username=\&quot;username\&quot; password=\&quot;password\&quot;;&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>如果在作业 JAR 中 Kafka 客户端依赖的类路径被重置了(relocate class),登录模块(login module)的类路径可能会不同,因此请根据登录模块在 JAR 中实际的类路径来改写以上配置.</p>
<p>关于安全配置的详细描述,请参阅 Apache Kafka 文档中的&quot;安全&quot;一节.</p>
<h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><p>如果你对 Kafka source 在新的 Source API 中的设计感兴趣,可阅读该部分作为参考.<br>关于新 Source API 的细节,Source API 文档和 FLIP-27 提供了更详细的描述.</p>
<p>在新 Source API 的抽象中,Kafka source 由以下几个部分组成.</p>
<h5 id="数据源分片-Source-Split"><a href="#数据源分片-Source-Split" class="headerlink" title="数据源分片(Source Split)"></a>数据源分片(Source Split)</h5><p>Kafka source 的数据源分片(source split)表示 Kafka topic 中的一个 partition.<br>Kafka 的数据源分片包括:</p>
<ol>
<li>该分片表示的 topic 和 partition</li>
<li>该 partition 的起始位点</li>
<li>该 partition 的停止位点,当 source 运行在批模式时适用</li>
</ol>
<p>Kafka source 分片的状态同时存储该 partition 的当前消费位点,该分片状态将会在 Kafka 源读取器(source reader)进行快照(snapshot) 时将当前消费位点保存为起始消费位点以将分片状态转换成不可变更的分片.</p>
<p>可查看 KafkaPartitionSplit 和 KafkaPartitionSplitState 类来了解细节.</p>
<h5 id="分片枚举器-Split-Enumerator"><a href="#分片枚举器-Split-Enumerator" class="headerlink" title="分片枚举器(Split Enumerator)"></a>分片枚举器(Split Enumerator)</h5><p>Kafka source 的分片枚举器负责检查在当前的 topic / partition 订阅模式下的新分片(partition),并将分片轮流均匀地分配给源读取器(source reader).<br>注意 Kafka source 的分片枚举器会将分片主动推送给源读取器,因此它无需处理来自源读取器的分片请求.</p>
<h5 id="源读取器-Source-Reader"><a href="#源读取器-Source-Reader" class="headerlink" title="源读取器(Source Reader)"></a>源读取器(Source Reader)</h5><p>Kafka source 的源读取器扩展了 SourceReaderBase,并使用单线程复用(single thread multiplex)的线程模型,使用一个由分片读取器 (split reader)驱动的 KafkaConsumer 来处理多个分片(partition).<br>消息会在从 Kafka 拉取下来后在分片读取器中立刻被解析.<br>分片的状态 即当前的消息消费进度会在 KafkaRecordEmitter 中更新,同时会在数据发送至下游时指定事件时间.</p>
<h3 id="Kafka-SourceFunction"><a href="#Kafka-SourceFunction" class="headerlink" title="Kafka SourceFunction"></a>Kafka SourceFunction</h3><p>FlinkKafkaConsumer 已被弃用并将在 Flink 1.15 中移除,请改用 KafkaSource.<br>如需参考,请参阅 Flink 1.13 文档.</p>
<h3 id="Kafka-Sink"><a href="#Kafka-Sink" class="headerlink" title="Kafka Sink"></a>Kafka Sink</h3><p>KafkaSink 可将数据流写入一个或多个 Kafka topic.</p>
<h4 id="使用方法-1"><a href="#使用方法-1" class="headerlink" title="使用方法"></a>使用方法</h4><p>Kafka sink 提供了构建类来创建 KafkaSink 的实例.<br>以下代码片段展示了如何将字符串数据按照至少一次(at lease once)的语义保证写入 Kafka topic:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = ...;</span><br><span class="line">        </span><br><span class="line">KafkaSink&lt;String&gt; sink = KafkaSink.&lt;String&gt;builder()</span><br><span class="line">        .setBootstrapServers(brokers)</span><br><span class="line">        .setRecordSerializer(KafkaRecordSerializationSchema.builder()</span><br><span class="line">            .setTopic(<span class="string">&quot;topic-name&quot;</span>)</span><br><span class="line">            .setValueSerializationSchema(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">            .build()</span><br><span class="line">        )</span><br><span class="line">        .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">        .build();</span><br><span class="line">        </span><br><span class="line">stream.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>以下属性在构建 KafkaSink 时是必须指定的:</p>
<ol>
<li>Bootstrap servers, setBootstrapServers(String)</li>
<li>消息序列化器(Serializer), setRecordSerializer(KafkaRecordSerializationSchema)</li>
<li>如果使用DeliveryGuarantee.EXACTLY_ONCE 的语义保证,则需要使用 setTransactionalIdPrefix(String)</li>
</ol>
<h4 id="序列化器"><a href="#序列化器" class="headerlink" title="序列化器"></a>序列化器</h4><p>构建时需要提供 KafkaRecordSerializationSchema 来将输入数据转换为 Kafka 的 ProducerRecord.<br>Flink 提供了 schema 构建器 以提供一些通用的组件,例如消息键(key)/消息体(value)序列化/topic 选择/消息分区,同样也可以通过实现对应的接口来进行更丰富的控制.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">KafkaRecordSerializationSchema.builder()</span><br><span class="line">    .setTopicSelector((element) -&gt; &#123;&lt;your-topic-selection-logic&gt;&#125;)</span><br><span class="line">    .setValueSerializationSchema(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">    .setKeySerializationSchema(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">    .setPartitioner(<span class="keyword">new</span> FlinkFixedPartitioner())</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>

<p>其中消息体(value)序列化方法和 topic 的选择方法是必须指定的,此外也可以通过 setKafkaKeySerializer(Serializer) 或 setKafkaValueSerializer(Serializer) 来使用 Kafka 提供而非 Flink 提供的序列化器.</p>
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>KafkaSink 总共支持三种不同的语义保证(DeliveryGuarantee).<br>对于 DeliveryGuarantee.AT_LEAST_ONCE 和 DeliveryGuarantee.EXACTLY_ONCE,Flink checkpoint 必须启用.<br>默认情况下 KafkaSink 使用 DeliveryGuarantee.NONE.<br>以下是对不同语义保证的解释:</p>
<ol>
<li><p>DeliveryGuarantee.NONE<br>不提供任何保证:消息有可能会因 Kafka broker 的原因发生丢失或因 Flink 的故障发生重复.</p>
</li>
<li><p>DeliveryGuarantee.AT_LEAST_ONCE<br>sink 在 checkpoint 时会等待 Kafka 缓冲区中的数据全部被 Kafka producer 确认.<br>消息不会因 Kafka broker 端发生的事件而丢失,但可能会在 Flink 重启时重复,因为 Flink 会重新处理旧数据.</p>
</li>
<li><p>DeliveryGuarantee.EXACTLY_ONCE<br>该模式下,Kafka sink 会将所有数据通过在 checkpoint 时提交的事务写入.<br>因此,如果 consumer 只读取已提交的数据(参见 Kafka consumer 配置 isolation.level),在 Flink 发生重启时不会发生数据重复.<br>然而这会使数据在 checkpoint 完成时才会可见,因此请按需调整 checkpoint 的间隔.<br>请确认事务 ID 的前缀(transactionIdPrefix)对不同的应用是唯一的,以保证不同作业的事务 不会互相影响！此外,强烈建议将 Kafka 的事务超时时间调整至远大于 checkpoint 最大间隔 + 最大重启时间,否则 Kafka 对未提交事务的过期处理会导致数据丢失.</p>
</li>
</ol>
<h4 id="监控-1"><a href="#监控-1" class="headerlink" title="监控"></a>监控</h4><p>Kafka sink 会在不同的范围(Scope)中汇报下列指标.</p>
<img src="/images/flgl119.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="Kafka-Producer"><a href="#Kafka-Producer" class="headerlink" title="Kafka Producer"></a>Kafka Producer</h3><p>FlinkKafkaProducer 已被弃用并将在 Flink 1.15 中移除,请改用 KafkaSink.<br>如需参考,请参阅 Flink 1.13 文档.</p>
<h3 id="Kafka-连接器指标"><a href="#Kafka-连接器指标" class="headerlink" title="Kafka 连接器指标"></a>Kafka 连接器指标</h3><p>Flink 的 Kafka 连接器通过 Flink 的指标系统提供一些指标来帮助分析 connector 的行为.<br>各个版本的 Kafka producer 和 consumer 会通过 Flink 的指标系统汇报 Kafka 内部的指标.<br>该 Kafka 文档列出了所有汇报的指标.</p>
<p>同样也可通过将 Kafka source 在该章节描述的 register.consumer.metrics,或 Kafka sink 的 register.producer.metrics 配置设置为 false 来关闭 Kafka 指标的注册.</p>
<h3 id="启用-Kerberos-身份验证"><a href="#启用-Kerberos-身份验证" class="headerlink" title="启用 Kerberos 身份验证"></a>启用 Kerberos 身份验证</h3><p>Flink 通过 Kafka 连接器提供了一流的支持,可以对 Kerberos 配置的 Kafka 安装进行身份验证.<br>只需在 flink-conf.yaml 中配置 Flink.<br>像这样为 Kafka 启用 Kerberos 身份验证:<br>1)通过设置以下内容配置 Kerberos 票据</p>
<ol>
<li><p>security.kerberos.login.use-ticket-cache:默认情况下,这个值是 true,Flink 将尝试在 kinit 管理的票据缓存中使用 Kerberos 票据.<br>注意:在 YARN 上部署的 Flink jobs 中使用 Kafka 连接器时,使用票据缓存的 Kerberos 授权将不起作用.</p>
</li>
<li><p>security.kerberos.login.keytab 和 security.kerberos.login.principal:要使用 Kerberos keytabs,需为这两个属性设置值.</p>
</li>
</ol>
<p>2)将 KafkaClient 追加到 security.kerberos.login.contexts:这告诉 Flink 将配置的 Kerberos 票据提供给 Kafka 登录上下文以用于 Kafka 身份验证.</p>
<p>一旦启用了基于 Kerberos 的 Flink 安全性后,只需在提供的属性配置中包含以下两个设置(通过传递给内部 Kafka 客户端),即可使用 Flink Kafka Consumer 或 Producer 向 Kafk a进行身份验证:</p>
<ol>
<li><p>将 security.protocol 设置为 SASL_PLAINTEXT(默认为 NONE):用于与 Kafka broker 进行通信的协议.<br>使用独立 Flink 部署时,也可以使用 SASL_SSL；请在此处查看如何为 SSL 配置 Kafka 客户端.</p>
</li>
<li><p>将 sasl.kerberos.service.name 设置为 kafka(默认为 kafka):此值应与用于 Kafka broker 配置的 sasl.kerberos.service.name 相匹配.<br>客户端和服务器配置之间的服务名称不匹配将导致身份验证失败.</p>
</li>
</ol>
<p>有关 Kerberos 安全性 Flink 配置的更多信息,请参见这里.<br>你也可以在这里进一步了解 Flink 如何在内部设置基于 kerberos 的安全性.</p>
<h3 id="升级到最近的连接器版本"><a href="#升级到最近的连接器版本" class="headerlink" title="升级到最近的连接器版本"></a>升级到最近的连接器版本</h3><p>通用的升级步骤概述见 升级 Jobs 和 Flink 版本指南.<br>对于 Kafka,你还需要遵循这些步骤:</p>
<ol>
<li><p>不要同时升级 Flink 和 Kafka 连接器</p>
</li>
<li><p>确保你对 Consumer 设置了 group.id</p>
</li>
<li><p>在 Consumer 上设置 setCommitOffsetsOnCheckpoints(true),以便读 offset 提交到 Kafka.<br>务必在停止和恢复 savepoint 前执行此操作.<br>你可能需要在旧的连接器版本上进行停止/重启循环来启用此设置.</p>
</li>
<li><p>在 Consumer 上设置 setStartFromGroupOffsets(true),以便我们从 Kafka 获取读 offset.<br>这只会在 Flink 状态中没有读 offset 时生效,这也是为什么下一步非要重要的原因.</p>
</li>
<li><p>修改 source/sink 分配到的 uid.<br>这会确保新的 source/sink 不会从旧的 sink/source 算子中读取状态.</p>
</li>
<li><p>使用 --allow-non-restored-state 参数启动新 job,因为我们在 savepoint 中仍然有先前连接器版本的状态.</p>
</li>
</ol>
<h3 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h3><p>如果你在使用 Flink 时对 Kafka 有问题,请记住,Flink 只封装 KafkaConsumer 或 KafkaProducer,你的问题可能独立于 Flink,有时可以通过升级 Kafka broker 程序/重新配置 Kafka broker 程序或在 Flink 中重新配置 KafkaConsumer 或 KafkaProducer 来解决.<br>下面列出了一些常见问题的示例.</p>
<h4 id="数据丢失"><a href="#数据丢失" class="headerlink" title="数据丢失"></a>数据丢失</h4><p>根据你的 Kafka 配置,即使在 Kafka 确认写入后,你仍然可能会遇到数据丢失.<br>特别要记住在 Kafka 的配置中设置以下属性:<br>acks<br>log.flush.interval.messages<br>log.flush.interval.ms<br>log.flush.*<br>上述选项的默认值是很容易导致数据丢失的.<br>请参考 Kafka 文档以获得更多的解释.</p>
<h4 id="UnknownTopicOrPartitionException"><a href="#UnknownTopicOrPartitionException" class="headerlink" title="UnknownTopicOrPartitionException"></a>UnknownTopicOrPartitionException</h4><p>导致此错误的一个可能原因是正在进行新的 leader 选举,例如在重新启动 Kafka broker 之后或期间.<br>这是一个可重试的异常,因此 Flink job 应该能够重启并恢复正常运行.<br>也可以通过更改 producer 设置中的 retries 属性来规避.<br>但是,这可能会导致重新排序消息,反过来可以通过将 max.in.flight.requests.per.connection 设置为 1 来避免不需要的消息.</p>
<h4 id="ProducerFencedException"><a href="#ProducerFencedException" class="headerlink" title="ProducerFencedException"></a>ProducerFencedException</h4><p>这个错误是由于 FlinkKafkaProducer 所生成的 transactional.id 与其他应用所使用的的产生了冲突.<br>多数情况下,由于 FlinkKafkaProducer 产生的 ID 都是以 taskName + &quot;-&quot; + operatorUid 为前缀的,这些产生冲突的应用也是使用了相同 Job Graph 的 Flink Job.<br>我们可以使用 setTransactionalIdPrefix() 方法来覆盖默认的行为,为每个不同的 Job 分配不同的 transactional.id 前缀来解决这个问题.</p>
<h2 id="Cassandra"><a href="#Cassandra" class="headerlink" title="Cassandra"></a>Cassandra</h2><p>此连接器提供将数据写入Apache Cassandra数据库的接收器.<br>要使用此连接器,请将以下依赖项添加到您的项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-cassandra_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>请注意,流连接器当前不是二进制分发的一部分.<br>在此处查看如何与它们链接以进行集群执行.</p>
<h3 id="安装-Apache-Cassandra"><a href="#安装-Apache-Cassandra" class="headerlink" title="安装 Apache Cassandra"></a>安装 Apache Cassandra</h3><p>有多种方法可以在本地机器上启动 Cassandra 实例:</p>
<ol>
<li>按照Cassandra 入门页面中的说明进行操作.</li>
<li>从官方 Docker 存储库启动一个运行 Cassandra 的容器.</li>
</ol>
<h3 id="Cassandra-Sinks"><a href="#Cassandra-Sinks" class="headerlink" title="Cassandra Sinks"></a>Cassandra Sinks</h3><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>Flink 的 Cassandra sink 是使用静态 CassandraSink.addSink(DataStream input)方法.<br>此方法返回一个 CassandraSinkBuilder,它提供了进一步配置接收器的方法,最后build()是接收器实例.</p>
<p>可以使用以下配置方法:</p>
<ol>
<li><p>setQuery(String query)<br>设置为接收器接收的每条记录执行的 upsert 查询.<br>该查询在内部被视为 CQL 语句.<br>请设置 upsert 查询以处理Tuple数据类型.<br>不要为处理POJO数据类型设置查询.</p>
</li>
<li><p>setClusterBuilder(ClusterBuilder clusterBuilder)<br>使用更复杂的设置(如一致性级别/重试策略等)设置用于配置与 cassandra 的连接的集群构建器.</p>
</li>
<li><p>setHost(String host[, int port])<br>setClusterBuilder() 的简单版本,带有主机/端口信息以连接到 Cassandra 实例</p>
</li>
<li><p>setMapperOptions(MapperOptions options)<br>设置用于配置 DataStax ObjectMapper 的映射器选项.<br>仅在处理POJO数据类型时适用.</p>
</li>
<li><p>setMaxConcurrentRequests(int maxConcurrentRequests, Duration timeout)<br>设置获得执行许可的最大允许并发请求数.<br>仅在未配置enableWriteAheadLog()时适用.</p>
</li>
<li><p>enableWriteAheadLog([CheckpointCommitter committer])<br>可选设置<br>允许对非确定性算法进行一次性处理.</p>
</li>
<li><p>setFailureHandler([CassandraFailureHandler failureHandler])<br>可选设置<br>设置自定义故障处理程序.</p>
</li>
<li><p>setDefaultKeyspace(String keyspace)<br>设置要使用的默认键空间.</p>
</li>
<li><p>enableIgnoreNullFields()<br>启用忽略空值,将空值视为未设置并避免写入空字段和创建墓碑.</p>
</li>
<li><p>build()<br>完成配置并构造 CassandraSink 实例.</p>
</li>
</ol>
<h4 id="Write-ahead-Log"><a href="#Write-ahead-Log" class="headerlink" title="Write-ahead Log"></a>Write-ahead Log</h4><p>检查点提交者将有关已完成检查点的附加信息存储在某些资源中.<br>此信息用于防止在发生故障时完全重播上次完成的检查点.<br>您可以使用 CassandraCommitter将这些存储在 cassandra 的单独表中.<br>请注意,该表不会被 Flink 清理.</p>
<p>如果查询是幂等的(意味着可以多次应用而不改变结果)并且启用了检查点,Flink 可以提供完全一次保证.<br>如果失败,失败的检查点将被完全重播.</p>
<p>此外,对于非确定性程序,必须启用预写日志.<br>对于这样的程序,重放的检查点可能与前一次尝试完全不同,这可能会使数据库处于不一致的状态,因为第一次尝试的一部分可能已经被写入.<br>预写日志保证重放的检查点与第一次尝试相同.<br>请注意,启用此功能将对延迟产生不利影响.</p>
<p>注意:预写日志功能目前是实验性的.<br>在许多情况下,使用连接器而不启用它就足够了.<br>请向开发邮件列表报告问题.</p>
<h4 id="检查点和容错"><a href="#检查点和容错" class="headerlink" title="检查点和容错"></a>检查点和容错</h4><p>启用检查点后,Cassandra Sink 保证向 C* 实例至少交付一次操作请求.<br>有关检查点文档和容错保证文档的更多详细信息</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>Cassandra sink 目前支持 Tuple 和 POJO 两种数据类型,Flink 会自动检测使用了哪种类型的输入.<br>有关这些流数据类型的一般用途,请参阅支持的数据类型.<br>我们展示了两个基于SocketWindowWordCount的实现,分别用于 POJO 和 Tuple 数据类型.</p>
<p>在所有这些示例中,我们假设已创建关联的 Keyspace example和 Table wordcount.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> KEYSPACE IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> example</span><br><span class="line">    WITH replication = &#123;&#x27;class&#x27;: &#x27;SimpleStrategy&#x27;, &#x27;replication_factor&#x27;: &#x27;1&#x27;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> example.wordcount (</span><br><span class="line">    word text,</span><br><span class="line">    count <span class="type">bigint</span>,</span><br><span class="line">    <span class="keyword">PRIMARY</span> KEY(word)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="Streaming-Tuple-Data-Type"><a href="#Streaming-Tuple-Data-Type" class="headerlink" title="Streaming Tuple Data Type"></a>Streaming Tuple Data Type</h4><p>在将 Java/Scala 元组数据类型的结果存储到 Cassandra 接收器时,需要设置 CQL upsert 语句(通过 setQuery(&#39;stmt&#39;))将每条记录持久化回数据库.<br>将 upsert 查询缓存为PreparedStatement,每个 Tuple 元素都将转换为语句的参数.</p>
<p>有关PreparedStatement和的详细信息BoundStatement,请访问DataStax Java 驱动程序手册</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// get the execution environment</span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// get input data by connecting to the socket</span></span><br><span class="line">DataStream&lt;String&gt; text = env.socketTextStream(hostname, port, <span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; result = text</span><br><span class="line">        .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// normalize and split the line</span></span><br><span class="line">                String[] words = value.toLowerCase().split(<span class="string">&quot;\\s&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// emit the pairs</span></span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    <span class="comment">//Do not accept empty word, since word is defined as primary key in C* table</span></span><br><span class="line">                    <span class="keyword">if</span> (!word.isEmpty()) &#123;</span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;String, Long&gt;(word, <span class="number">1L</span>));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .keyBy(value -&gt; value.f0)</span><br><span class="line">        .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">CassandraSink.addSink(result)</span><br><span class="line">        .setQuery(<span class="string">&quot;INSERT INTO example.wordcount(word, count) values (?, ?);&quot;</span>)</span><br><span class="line">        .setHost(<span class="string">&quot;127.0.0.1&quot;</span>)</span><br><span class="line">        .build();</span><br></pre></td></tr></table></figure>

<h4 id="Streaming-POJO-Data-Type"><a href="#Streaming-POJO-Data-Type" class="headerlink" title="Streaming POJO Data Type"></a>Streaming POJO Data Type</h4><p>流式传输 POJO 数据类型并将相同的 POJO 实体存储回 Cassandra 的示例.<br>此外,此 POJO 实现需要遵循DataStax Java Driver <code>com.datastax.driver.mapping.Mapper</code>对类进行注释,因为该实体的每个字段都使用 DataStax Java Driver类映射到指定表的关联列.</p>
<p>可以通过放置在 Pojo 类中的字段声明上的注释来定义每个表列的映射.<br>有关映射的详细信息,请参阅 CQL 文档定义映射类和CQL 数据类型</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// get the execution environment</span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// get input data by connecting to the socket</span></span><br><span class="line">DataStream&lt;String&gt; text = env.socketTextStream(hostname, port, <span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class="line">DataStream&lt;WordCount&gt; result = text</span><br><span class="line">        .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, WordCount&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;WordCount&gt; out)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// normalize and split the line</span></span><br><span class="line">                String[] words = value.toLowerCase().split(<span class="string">&quot;\\s&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// emit the pairs</span></span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!word.isEmpty()) &#123;</span><br><span class="line">                        <span class="comment">//Do not accept empty word, since word is defined as primary key in C* table</span></span><br><span class="line">                        out.collect(<span class="keyword">new</span> WordCount(word, <span class="number">1L</span>));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .keyBy(WordCount::getWord)</span><br><span class="line">        .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line">        .reduce(<span class="keyword">new</span> ReduceFunction&lt;WordCount&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> WordCount <span class="title">reduce</span><span class="params">(WordCount a, WordCount b)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> WordCount(a.getWord(), a.getCount() + b.getCount());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">CassandraSink.addSink(result)</span><br><span class="line">        .setHost(<span class="string">&quot;127.0.0.1&quot;</span>)</span><br><span class="line">        .setMapperOptions(() -&gt; <span class="keyword">new</span> Mapper.Option[]&#123;Mapper.Option.saveNullFields(<span class="keyword">true</span>)&#125;)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Table(keyspace = &quot;example&quot;, name = &quot;wordcount&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Column(name = &quot;word&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String word = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Column(name = &quot;count&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WordCount</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WordCount</span><span class="params">(String word, <span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.setWord(word);</span><br><span class="line">        <span class="keyword">this</span>.setCount(count);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getWord</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> word;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setWord</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = word;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCount</span><span class="params">(<span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.count = count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getWord() + <span class="string">&quot; : &quot;</span> + getCount();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><p>此连接器提供可以向 Elasticsearch 索引请求文档操作的 sinks.<br>要使用此连接器,请根据 Elasticsearch 的安装版本将以下依赖之一添加到你的项目中:</p>
<p>Elasticsearch 版本  Maven 依赖<br>6.x</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>7.x </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch7<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>请注意,流连接器目前不是二进制发行版的一部分.<br>有关如何将程序和用于集群执行的库一起打包,参考此文档.</p>
<h3 id="安装-Elasticsearch"><a href="#安装-Elasticsearch" class="headerlink" title="安装 Elasticsearch"></a>安装 Elasticsearch</h3><p>Elasticsearch 集群的设置可以参考此文档.</p>
<h3 id="Elasticsearch-Sink"><a href="#Elasticsearch-Sink" class="headerlink" title="Elasticsearch Sink"></a>Elasticsearch Sink</h3><p>下面的示例展示了如何配置并创建一个 sink:<br>Elasticsearch 6:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.Requests;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.sinkTo(</span><br><span class="line">    <span class="keyword">new</span> Elasticsearch6SinkBuilder&lt;String&gt;()</span><br><span class="line">        <span class="comment">// 下面的设置使 sink 在接收每个元素之后立即提交,否则这些元素将被缓存起来</span></span><br><span class="line">        .setBulkFlushMaxActions(<span class="number">1</span>)</span><br><span class="line">        .setHosts(<span class="keyword">new</span> HttpHost(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>))</span><br><span class="line">        .setEmitter(</span><br><span class="line">        (element, context, indexer) -&gt;</span><br><span class="line">        indexer.add(createIndexRequest(element)))</span><br><span class="line">        .build());</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> IndexRequest <span class="title">createIndexRequest</span><span class="params">(String element)</span> </span>&#123;</span><br><span class="line">    Map&lt;String, Object&gt; json = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    json.put(<span class="string">&quot;data&quot;</span>, element);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Requests.indexRequest()</span><br><span class="line">        .index(<span class="string">&quot;my-index&quot;</span>)</span><br><span class="line">        .type(<span class="string">&quot;my-type&quot;</span>)</span><br><span class="line">        .id(element)</span><br><span class="line">        .source(json);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Elasticsearch 7:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.Requests;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.sinkTo(</span><br><span class="line">    <span class="keyword">new</span> Elasticsearch7SinkBuilder&lt;String&gt;()</span><br><span class="line">        <span class="comment">// 下面的设置使 sink 在接收每个元素之后立即提交,否则这些元素将被缓存起来</span></span><br><span class="line">        .setBulkFlushMaxActions(<span class="number">1</span>)</span><br><span class="line">        .setHosts(<span class="keyword">new</span> HttpHost(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>))</span><br><span class="line">        .setEmitter(</span><br><span class="line">        (element, context, indexer) -&gt;</span><br><span class="line">        indexer.add(createIndexRequest(element)))</span><br><span class="line">        .build());</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> IndexRequest <span class="title">createIndexRequest</span><span class="params">(String element)</span> </span>&#123;</span><br><span class="line">    Map&lt;String, Object&gt; json = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    json.put(<span class="string">&quot;data&quot;</span>, element);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Requests.indexRequest()</span><br><span class="line">        .index(<span class="string">&quot;my-index&quot;</span>)</span><br><span class="line">        .id(element)</span><br><span class="line">        .source(json);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要注意的是,该示例仅演示了对每个传入的元素执行单个索引请求.<br>通常,ElasticsearchSinkFunction 可用于执行多个不同类型的请求(例如 DeleteRequest/ UpdateRequest 等).</p>
<p>在内部,Flink Elasticsearch Sink 的每个并行实例使用一个 BulkProcessor 向集群发送操作请求.<br>这会在元素批量发送到集群之前进行缓存.<br>BulkProcessor 一次执行一个批量请求,即不会存在两个并行刷新缓存的操作.</p>
<h4 id="Elasticsearch-Sinks-和容错"><a href="#Elasticsearch-Sinks-和容错" class="headerlink" title="Elasticsearch Sinks 和容错"></a>Elasticsearch Sinks 和容错</h4><p>通过启用 Flink checkpoint,Flink Elasticsearch Sink 保证至少一次将操作请求发送到 Elasticsearch 集群.<br>这是通过在进行 checkpoint 时等待 BulkProcessor 中所有挂起的操作请求来实现.<br>这有效地保证了在触发 checkpoint 之前所有的请求被 Elasticsearch 成功确认,然后继续处理发送到 sink 的记录.</p>
<p>关于 checkpoint 和容错的更多详细信息,请参见容错文档.</p>
<p>要使用具有容错特性的 Elasticsearch Sinks,需要在执行环境中启用作业拓扑的 checkpoint:</p>
<p>Elasticsearch 6:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.enableCheckpointing(<span class="number">5000</span>); <span class="comment">// 每 5000 毫秒执行一次 checkpoint</span></span><br><span class="line"></span><br><span class="line">Elasticsearch6SinkBuilder sinkBuilder = <span class="keyword">new</span> Elasticsearch6SinkBuilder&lt;String&gt;()</span><br><span class="line">    .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">    .setHosts(<span class="keyword">new</span> HttpHost(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>))</span><br><span class="line">    .setEmitter(</span><br><span class="line">    (element, context, indexer) -&gt; </span><br><span class="line">    indexer.add(createIndexRequest(element)));</span><br></pre></td></tr></table></figure>

<p>Elasticsearch 7:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.enableCheckpointing(<span class="number">5000</span>); <span class="comment">// 每 5000 毫秒执行一次 checkpoint</span></span><br><span class="line"></span><br><span class="line">Elasticsearch7SinkBuilder sinkBuilder = <span class="keyword">new</span> Elasticsearch7SinkBuilder&lt;String&gt;()</span><br><span class="line">    .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">    .setHosts(<span class="keyword">new</span> HttpHost(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>))</span><br><span class="line">    .setEmitter(</span><br><span class="line">    (element, context, indexer) -&gt; </span><br><span class="line">    indexer.add(createIndexRequest(element)));</span><br></pre></td></tr></table></figure>

<p>当为连接器配置 AT_LEAST_ONCE 交付时,使用具有确定性 id 和 upsert 方法的 UpdateRequests 可以在 Elasticsearch 中实现精确一次语义.</p>
<h4 id="处理失败的-Elasticsearch-请求"><a href="#处理失败的-Elasticsearch-请求" class="headerlink" title="处理失败的 Elasticsearch 请求"></a>处理失败的 Elasticsearch 请求</h4><p>Elasticsearch 操作请求可能由于多种原因而失败,包括节点队列容量暂时已满或者要被索引的文档格式错误.<br>Flink Elasticsearch Sink 允许用户通过通过指定一个退避策略来重试请求.</p>
<p>下面是一个例子:</p>
<p>Elasticsearch 6:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.sinkTo(</span><br><span class="line">    <span class="keyword">new</span> Elasticsearch6SinkBuilder&lt;String&gt;()</span><br><span class="line">        .setHosts(<span class="keyword">new</span> HttpHost(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>))</span><br><span class="line">        .setEmitter(</span><br><span class="line">        (element, context, indexer) -&gt;</span><br><span class="line">        indexer.add(createIndexRequest(element)))</span><br><span class="line">        <span class="comment">// 这里启用了一个指数退避重试策略,初始延迟为 1000 毫秒且最大重试次数为 5</span></span><br><span class="line">        .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line">        .build());</span><br></pre></td></tr></table></figure>

<p>Elasticsearch 7:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.sinkTo(</span><br><span class="line">    <span class="keyword">new</span> Elasticsearch7SinkBuilder&lt;String&gt;()</span><br><span class="line">        .setHosts(<span class="keyword">new</span> HttpHost(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>))</span><br><span class="line">        .setEmitter(</span><br><span class="line">        (element, context, indexer) -&gt;</span><br><span class="line">        indexer.add(createIndexRequest(element)))</span><br><span class="line">        <span class="comment">// 这里启用了一个指数退避重试策略,初始延迟为 1000 毫秒且最大重试次数为 5</span></span><br><span class="line">        .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line">        .build());</span><br></pre></td></tr></table></figure>

<p>上面的示例 sink 重新添加由于资源受限(例如:队列容量已满)而失败的请求.<br>对于其它类型的故障,例如文档格式错误,sink 将会失败.<br>如若未设置 BulkFlushBackoffStrategy (或者 FlushBackoffType.NONE),那么任何类型的错误都会导致 sink 失败.</p>
<p>重要提示:在失败时将请求重新添加回内部 BulkProcessor 会导致更长的 checkpoint,因为在进行 checkpoint 时,sink 还需要等待重新添加的请求被刷新.<br>例如,当使用 FlushBackoffType.EXPONENTIAL 时, checkpoint 会进行等待,直到 Elasticsearch 节点队列有足够的容量来处理所有挂起的请求,或者达到最大重试次数.</p>
<h4 id="配置内部批量处理器"><a href="#配置内部批量处理器" class="headerlink" title="配置内部批量处理器"></a>配置内部批量处理器</h4><p>通过使用以下在 Elasticsearch6SinkBuilder 中提供的方法,可以进一步配置内部的 BulkProcessor 关于其如何刷新缓存操作请求的行为:</p>
<ol>
<li>setBulkFlushMaxActions(int numMaxActions):刷新前最大缓存的操作数.</li>
<li>setBulkFlushMaxSizeMb(int maxSizeMb):刷新前最大缓存的数据量(以兆字节为单位).</li>
<li>setBulkFlushInterval(long intervalMillis):刷新的时间间隔(不论缓存操作的数量或大小如何).</li>
</ol>
<p>还支持配置如何对暂时性请求错误进行重试:</p>
<ol>
<li>setBulkFlushBackoffStrategy(FlushBackoffType flushBackoffType, int maxRetries, long delayMillis):退避延迟的类型,CONSTANT 或者 EXPONENTIAL,退避重试次数,退避重试的时间间隔.<br>对于常量延迟来说,此值是每次重试间的间隔.<br>对于指数延迟来说,此值是延迟的初始值.</li>
</ol>
<p>可以在此文档找到 Elasticsearch 的更多信息.</p>
<h3 id="将-Elasticsearch-连接器打包到-Uber-Jar-中"><a href="#将-Elasticsearch-连接器打包到-Uber-Jar-中" class="headerlink" title="将 Elasticsearch 连接器打包到 Uber-Jar 中"></a>将 Elasticsearch 连接器打包到 Uber-Jar 中</h3><p>建议构建一个包含所有依赖的 uber-jar (可执行的 jar),以便更好地执行你的 Flink 程序.<br>(更多信息参见此文档).</p>
<p>或者,你可以将连接器的 jar 文件放入 Flink 的 lib/ 目录下,使其在全局范围内可用,即可用于所有的作业.</p>
<h2 id="Firehose"><a href="#Firehose" class="headerlink" title="Firehose"></a>Firehose</h2><h2 id="Kinesis"><a href="#Kinesis" class="headerlink" title="Kinesis"></a>Kinesis</h2><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>连接器提供了 BATCH 模式和 STREAMING 模式统一的 Source 和 Sink.<br>Flink FileSystem abstraction 支持连接器对文件系统进行(分区)文件读写.<br>文件系统连接器为 BATCH 和 STREAMING 模式提供了相同的保证,而且对 STREAMING 模式执行提供了精确一次(exactly-once)语义保证.</p>
<p>连接器支持对任意(分布式的)文件系统(例如,POSIX/S3/HDFS)以某种数据格式format(例如,Avro/CSV/Parquet) 对文件进行写入,或者读取后生成数据流或一组记录.</p>
<h3 id="File-Source"><a href="#File-Source" class="headerlink" title="File Source"></a>File Source</h3><p>File Source 是基于 Source API 同时支持批模式和流模式文件读取的统一 Source.<br>File Source 分为以下两个部分:SplitEnumerator 和 SourceReader.</p>
<ol>
<li>SplitEnumerator 负责发现和识别需要读取的文件,并将这些文件分配给 SourceReader 进行读取.</li>
<li>SourceReader 请求需要处理的文件,并从文件系统中读取该文件.</li>
</ol>
<p>可能需要指定某种 format 与 File Source 联合进行解析 CSV/解码AVRO/或者读取 Parquet 列式文件.</p>
<p>有界流和无界流:<br>有界的 File Source(通过 SplitEnumerator)列出所有文件(一个过滤出隐藏文件的递归目录列表)并读取.<br>无界的 File Source 由配置定期扫描文件的 enumerator 创建.<br>在无界的情况下,SplitEnumerator 将像有界的 File Source 一样列出所有文件,但是不同的是,经过一个时间间隔之后,重复上述操作.<br>对于每一次列举操作,SplitEnumerator 会过滤掉之前已经检测过的文件,将新扫描到的文件发送给 SourceReader.</p>
<h4 id="使用方法-2"><a href="#使用方法-2" class="headerlink" title="使用方法"></a>使用方法</h4><p>可以通过调用以下 API 建立一个 File Source:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从文件流中读取文件内容</span></span><br><span class="line">FileSource.forRecordStreamFormat(StreamFormat,Path...);</span><br><span class="line">        </span><br><span class="line"><span class="comment">// 从文件中一次读取一批记录</span></span><br><span class="line">FileSource.forBulkFileFormat(BulkFormat,Path...);</span><br></pre></td></tr></table></figure>

<p>可以通过创建 FileSource.FileSourceBuilder 设置 File Source 的所有参数.</p>
<p>对于有界/批的使用场景,File Source 需要处理给定路径下的所有文件.<br>对于无界/流的使用场景,File Source 会定期检查路径下的新文件并读取.</p>
<p>当创建一个 File Source 时(通过上述任意方法创建的 FileSource.FileSourceBuilder), 默认情况下,Source 为有界/批的模式.<br>可以调用 AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration) 设置 Source 为持续的流模式.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> FileSource&lt;String&gt; source =</span><br><span class="line">        FileSource.forRecordStreamFormat(...)</span><br><span class="line">        .monitorContinuously(Duration.ofMillis(<span class="number">5</span>))  </span><br><span class="line">        .build();</span><br></pre></td></tr></table></figure>

<h4 id="Format-Types"><a href="#Format-Types" class="headerlink" title="Format Types"></a>Format Types</h4><p>通过 file formats 定义的文件 readers 读取每个文件.<br>其中定义了解析和读取文件内容的逻辑.<br>Source 支持多个解析类.<br>这些接口是实现简单性和灵活性/效率之间的折衷.</p>
<ol>
<li><p>StreamFormat 从文件流中读取文件内容.<br>它是最简单的格式实现, 并且提供了许多拆箱即用的特性(如 Checkpoint 逻辑),但是限制了可应用的优化(例如对象重用,批处理等等).</p>
</li>
<li><p>BulkFormat 从文件中一次读取一批记录.<br>它虽然是最 &quot;底层&quot; 的格式实现,但是提供了优化实现的最大灵活性.</p>
</li>
</ol>
<h5 id="TextLine-Format"><a href="#TextLine-Format" class="headerlink" title="TextLine Format"></a>TextLine Format</h5><p>使用 StreamFormat 格式化文件中的文本行.<br>Java 中内置的 InputStreamReader 对使用了支持各种字符集的字节流进行解码.<br>此格式不支持从 Checkpoint 进行恢复优化.<br>在恢复时,将重新读取并放弃在最后一个 Checkpoint 之前处理的行数.<br>这是由于无法通过字符集解码器追踪文件中的行偏移量,及其内部缓冲输入流和字符集解码器的状态.</p>
<h5 id="SimpleStreamFormat-抽象类"><a href="#SimpleStreamFormat-抽象类" class="headerlink" title="SimpleStreamFormat 抽象类"></a>SimpleStreamFormat 抽象类</h5><p>这是 StreamFormat 的简单版本,适用于不可拆分的格式.<br>可以通过实现 SimpleStreamFormat 接口自定义读取数组或文件:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">private static final class ArrayReaderFormat extends SimpleStreamFormat&lt;byte[]&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Reader&lt;<span class="keyword">byte</span>[]&gt; createReader(Configuration config, FSDataInputStream stream)</span><br><span class="line">            <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ArrayReader(stream);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;<span class="keyword">byte</span>[]&gt; getProducedType() &#123;</span><br><span class="line">        <span class="keyword">return</span> PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSource&lt;<span class="keyword">byte</span>[]&gt; source =</span><br><span class="line">                FileSource.forRecordStreamFormat(<span class="keyword">new</span> ArrayReaderFormat(), path).build();</span><br></pre></td></tr></table></figure>

<p>CsvReaderFormat 是一个实现 SimpleStreamFormat 接口的例子.<br>类似这样进行初始化:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CsvReaderFormat&lt;SomePojo&gt; csvFormat = CsvReaderFormat.forPojo(SomePojo.class);</span><br><span class="line">FileSource&lt;SomePojo&gt; source = </span><br><span class="line">        FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build();</span><br></pre></td></tr></table></figure>

<p>对于 CSV Format 的解析,在这个例子中,是根据使用 Jackson 库的 SomePojo 的字段自动生成的.<br>(注意:可能需要添加 @JsonPropertyOrder({field1, field2, ...}) 这个注释到自定义的类上,并且字段顺序与 CSV 文件列的顺序完全匹配).</p>
<p>如果需要对 CSV 模式或解析选项进行更细粒度的控制,可以使用 CsvReaderFormat 的更底层的 forSchema 静态工厂方法:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CsvReaderFormat&lt;T&gt; <span class="title">forSchema</span><span class="params">(CsvMapper mapper, CsvSchema schema, TypeInformation&lt;T&gt; typeInformation)</span> </span></span><br></pre></td></tr></table></figure>

<h5 id="Bulk-Format"><a href="#Bulk-Format" class="headerlink" title="Bulk Format"></a>Bulk Format</h5><p>BulkFormat 一次读取并解析一批记录.<br>BulkFormat 的实现包括 ORC Format或 Parquet Format等.<br>外部的 BulkFormat 类主要充当 reader 的配置持有者和工厂角色.<br>BulkFormat.Reader 是在 BulkFormat#createReader(Configuration, FileSourceSplit) 方法中创建的,然后完成读取操作.<br>如果在流的 checkpoint 执行期间基于 checkpoint 创建 Bulk reader,那么 reader 是在 BulkFormat#restoreReader(Configuration, FileSourceSplit) 方法中重新创建的.</p>
<p>可以通过将 SimpleStreamFormat 包装在 StreamFormatAdapter 中转换为 BulkFormat:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BulkFormat&lt;SomePojo, FileSourceSplit&gt; bulkFormat = </span><br><span class="line">        <span class="keyword">new</span> StreamFormatAdapter&lt;&gt;(CsvReaderFormat.forPojo(SomePojo.class));</span><br></pre></td></tr></table></figure>

<h4 id="自定义文件枚举类"><a href="#自定义文件枚举类" class="headerlink" title="自定义文件枚举类"></a>自定义文件枚举类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 针对 Hive 数据源的 FileEnumerator 实现类,基于 HiveTablePartition 生成拆分文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveSourceFileEnumerator</span> <span class="keyword">implements</span> <span class="title">FileEnumerator</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HiveSourceFileEnumerator</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/***</span></span><br><span class="line"><span class="comment">     * 拆分给定路径下的所有相关文件. &#123;<span class="doctag">@code</span></span></span><br><span class="line"><span class="comment">     * minDesiredSplits&#125; 是一个可选项,代表需要多少次拆分才能正确利用并行度</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Collection&lt;FileSourceSplit&gt; <span class="title">enumerateSplits</span><span class="params">(Path[] paths, <span class="keyword">int</span> minDesiredSplits)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// createInputSplits:splitting files into fragmented collections</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;(createInputSplits(...));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">/***</span></span><br><span class="line"><span class="comment">     * 创建 HiveSourceFileEnumerator 的工厂</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Provider</span> <span class="keyword">implements</span> <span class="title">FileEnumerator</span>.<span class="title">Provider</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> FileEnumerator <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> HiveSourceFileEnumerator(...);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用自定义文件枚举类</span></span><br><span class="line">    <span class="keyword">new</span> HiveSource&lt;&gt;(</span><br><span class="line">        ...,</span><br><span class="line">        <span class="keyword">new</span> HiveSourceFileEnumerator.Provider(</span><br><span class="line">        partitions != <span class="keyword">null</span> ? partitions : Collections.emptyList(),</span><br><span class="line">        <span class="keyword">new</span> JobConfWrapper(jobConf)),</span><br><span class="line">       ...);</span><br></pre></td></tr></table></figure>

<h4 id="当前限制"><a href="#当前限制" class="headerlink" title="当前限制"></a>当前限制</h4><p>对于大量积压的文件,Watermark 效果不佳.<br>这是因为 Watermark 急于在一个文件中推进,而下一个文件可能包含比 Watermark 更晚的数据.</p>
<p>对于无界 File Sources,枚举器会会将当前所有已处理文件的路径记录到 state 中,在某些情况下,这可能会导致状态变得相当大.<br>未来计划将引入一种压缩的方式来跟踪已经处理的文件(例如,将修改时间戳保持在边界以下).</p>
<h4 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h4><p>如果对新设计的 Source API 中的 File Sources 是如何工作的感兴趣,可以阅读本部分作为参考.<br>关于新的 Source API 的更多细节,请参考 documentation on data sources 和在 FLIP-27 中获取更加具体的讨论详情.</p>
<h3 id="File-Sink"><a href="#File-Sink" class="headerlink" title="File Sink"></a>File Sink</h3><p>File Sink 将传入的数据写入存储桶中.<br>考虑到输入流可以是无界的,每个桶中的数据被组织成有限大小的 Part 文件.<br>完全可以配置为基于时间的方式往桶中写入数据,比如可以设置每个小时的数据写入一个新桶中.<br>这意味着桶中将包含一个小时间隔内接收到的记录.</p>
<p>桶目录中的数据被拆分成多个 Part 文件.<br>对于相应的接收数据的桶的 Sink 的每个 Subtask,每个桶将至少包含一个 Part 文件.<br>将根据配置的滚动策略来创建其他 Part 文件.<br>对于 Row-encoded Formats(参考 Format Types)默认的策略是根据 Part 文件大小进行滚动,需要指定文件打开状态最长时间的超时以及文件关闭后的非活动状态的超时时间.<br>对于 Bulk-encoded Formats 在每次创建 Checkpoint 时进行滚动,并且用户也可以添加基于大小或者时间等的其他条件.</p>
<p>重要: 在 STREAMING 模式下使用 FileSink 需要开启 Checkpoint 功能.<br>文件只在 Checkpoint 成功时生成.<br>如果没有开启 Checkpoint 功能,文件将永远停留在 in-progress 或者 pending 的状态,并且下游系统将不能安全读取该文件数据.</p>
<img src="/images/flgl120.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="Format-Types-1"><a href="#Format-Types-1" class="headerlink" title="Format Types"></a>Format Types</h4><p>FileSink 不仅支持 Row-encoded 也支持 Bulk-encoded,例如 Apache Parquet.<br>这两种格式可以通过如下的静态方法进行构造:</p>
<ol>
<li>Row-encoded sink: FileSink.forRowFormat(basePath, rowEncoder)</li>
<li>Bulk-encoded sink: FileSink.forBulkFormat(basePath, bulkWriterFactory)</li>
</ol>
<p>不论创建 Row-encoded Format 或者 Bulk-encoded Format 的 Sink 时,都必须指定桶的路径以及对数据进行编码的逻辑.</p>
<p>请参考 JavaDoc 文档 FileSink 来获取所有的配置选项以及更多的不同数据格式实现的详细信息.</p>
<h5 id="Row-encoded-Formats"><a href="#Row-encoded-Formats" class="headerlink" title="Row-encoded Formats"></a>Row-encoded Formats</h5><p>Row-encoded Format 需要指定一个 Encoder,在输出数据到文件过程中被用来将单个行数据序列化为 OutputStream.</p>
<p>除了 bucket assigner,RowFormatBuilder 还允许用户指定以下属性:</p>
<ol>
<li>Custom RollingPolicy :自定义滚动策略覆盖 DefaultRollingPolicy</li>
<li>bucketCheckInterval (默认值 = 1 min) :基于滚动策略设置的检查时间间隔</li>
</ol>
<p>写入字符串的基本用法如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringEncoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.MemorySize;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.file.sink.FileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSink&lt;String&gt; sink = FileSink</span><br><span class="line">    .forRowFormat(<span class="keyword">new</span> Path(outputPath), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;(<span class="string">&quot;UTF-8&quot;</span>))</span><br><span class="line">    .withRollingPolicy(</span><br><span class="line">        DefaultRollingPolicy.builder()</span><br><span class="line">            .withRolloverInterval(Duration.ofSeconds(<span class="number">10</span>))</span><br><span class="line">            .withInactivityInterval(Duration.ofSeconds(<span class="number">10</span>))</span><br><span class="line">            .withMaxPartSize(MemorySize.ofMebiBytes(<span class="number">1</span>))</span><br><span class="line">            .build())</span><br><span class="line">  .build();</span><br><span class="line"></span><br><span class="line">input.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>这个例子中创建了一个简单的 Sink,默认的将记录分配给小时桶.<br>例子中还指定了滚动策略,当满足以下三个条件的任何一个时都会将 In-progress 状态文件进行滚动:</p>
<ol>
<li>包含了至少15分钟的数据量</li>
<li>从没接收延时5分钟之外的新纪录</li>
<li>文件大小已经达到 1GB(写入最后一条记录之后)</li>
</ol>
<h5 id="Bulk-encoded-Formats"><a href="#Bulk-encoded-Formats" class="headerlink" title="Bulk-encoded Formats"></a>Bulk-encoded Formats</h5><p>Bulk-encoded 的 Sink 的创建和 Row-encoded 的相似,但不需要指定 Encoder,而是需要指定 BulkWriter.Factory,请参考文档 BulkWriter.Factory .<br>BulkWriter 定义了如何添加和刷新新数据以及如何最终确定一批记录使用哪种编码字符集的逻辑.</p>
<p>Flink 内置了5种 BulkWriter 工厂类:<br>ParquetWriterFactory<br>AvroWriterFactory<br>SequenceFileWriterFactory<br>CompressWriterFactory<br>OrcBulkWriterFactory</p>
<p>重要 Bulk-encoded Format 仅支持一种继承了 CheckpointRollingPolicy 类的滚动策略.<br>在每个 Checkpoint 都会滚动.<br>另外也可以根据大小或处理时间进行滚动.</p>
<h5 id="Parquet-Format"><a href="#Parquet-Format" class="headerlink" title="Parquet Format"></a>Parquet Format</h5><p>Flink 内置了为 Avro Format 数据创建 Parquet 写入工厂的快捷方法.<br>在 AvroParquetWriters 类中可以发现那些方法以及相关的使用说明.</p>
<p>为了让 Parquet Format 数据写入更加通用,用户需要创建 ParquetWriterFactory 并且自定义实现 ParquetBuilder 接口.</p>
<p>如果在程序中使用 Parquet 的 Bulk-encoded Format,需要添加如下依赖到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-parquet_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>类似这样使用 FileSink 写入 Parquet Format 的 Avro 数据:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.connector.file.sink.FileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.parquet.avro.AvroParquetWriters;</span><br><span class="line"><span class="keyword">import</span> org.apache.avro.Schema;</span><br><span class="line"></span><br><span class="line">Schema schema = ...;</span><br><span class="line">DataStream&lt;GenericRecord&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSink&lt;GenericRecord&gt; sink = FileSink</span><br><span class="line">  .forBulkFormat(outputBasePath, AvroParquetWriters.forGenericRecord(schema))</span><br><span class="line">  .build();</span><br><span class="line"></span><br><span class="line">input.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>类似这样使用 FileSink 写入 Parquet Format 的 Protobuf 数据:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.connector.file.sink.FileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProtoRecord 是一个生成 protobuf 的类</span></span><br><span class="line">DataStream&lt;ProtoRecord&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSink&lt;ProtoRecord&gt; sink = FileSink</span><br><span class="line">  .forBulkFormat(outputBasePath, ParquetProtoWriters.forType(ProtoRecord.class))</span><br><span class="line">  .build();</span><br><span class="line"></span><br><span class="line">input.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<h5 id="Avro-Format"><a href="#Avro-Format" class="headerlink" title="Avro Format"></a>Avro Format</h5><p>Flink 也支持写入数据到 Avro Format 文件.<br>在 AvroWriters 类中可以发现一系列创建 Avro writer 工厂的便利方法及其相关说明.</p>
<p>如果在程序中使用 AvroWriters,需要添加如下依赖到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>类似这样使用 FileSink 写入数据到 Avro Format 文件中:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.connector.file.sink.FileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.avro.AvroWriters;</span><br><span class="line"><span class="keyword">import</span> org.apache.avro.Schema;</span><br><span class="line"></span><br><span class="line">Schema schema = ...;</span><br><span class="line">DataStream&lt;GenericRecord&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSink&lt;GenericRecord&gt; sink = FileSink</span><br><span class="line">  .forBulkFormat(outputBasePath, AvroWriters.forGenericRecord(schema))</span><br><span class="line">  .build();</span><br><span class="line"></span><br><span class="line">input.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>对于自定义创建的 Avro writers,例如,支持压缩功能,用户需要创建 AvroWriterFactory 并且自定义实现 AvroBuilder 接口:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">AvroWriterFactory&lt;?&gt; factory = <span class="keyword">new</span> AvroWriterFactory&lt;&gt;((AvroBuilder&lt;Address&gt;) out -&gt; &#123;</span><br><span class="line">  Schema schema = ReflectData.get().getSchema(Address.class);</span><br><span class="line">  DatumWriter&lt;Address&gt; datumWriter = <span class="keyword">new</span> ReflectDatumWriter&lt;&gt;(schema);</span><br><span class="line"></span><br><span class="line">  DataFileWriter&lt;Address&gt; dataFileWriter = <span class="keyword">new</span> DataFileWriter&lt;&gt;(datumWriter);</span><br><span class="line">  dataFileWriter.setCodec(CodecFactory.snappyCodec());</span><br><span class="line">  dataFileWriter.create(schema, out);</span><br><span class="line">  <span class="keyword">return</span> dataFileWriter;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Address&gt; stream = ...</span><br><span class="line">stream.sinkTo(FileSink.forBulkFormat(</span><br><span class="line">  outputBasePath,</span><br><span class="line">  factory).build());</span><br></pre></td></tr></table></figure>

<h5 id="ORC-Format"><a href="#ORC-Format" class="headerlink" title="ORC Format"></a>ORC Format</h5><p>ORC Format 的数据采用 Bulk-encoded Format,Flink 提供了 Vectorizer 接口的具体实现类 OrcBulkWriterFactory.</p>
<p>像其他列格式一样也是采用 Bulk-encoded Format,Flink 中 OrcBulkWriter 是使用 ORC 的 VectorizedRowBatch 实现批的方式输出数据的.</p>
<p>由于输入数据已经被转换成了 VectorizedRowBatch,所以用户必须继承抽象类 Vectorizer 并且覆写类中 vectorize(T element, VectorizedRowBatch batch) 这个方法.<br>正如看到的那样,此方法中提供了用户直接使用的 VectorizedRowBatch 类的实例,因此,用户不得不编写从输入 element 到 ColumnVectors 的转换逻辑,然后设置在 VectorizedRowBatch 实例中.</p>
<p>例如,如果是 Person 类型的输入元素,如下所示:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> age;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后,转换 Person 类型元素的实现并在 VectorizedRowBatch 中设置,如下所示:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PersonVectorizer</span> <span class="keyword">extends</span> <span class="title">Vectorizer</span>&lt;<span class="title">Person</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">PersonVectorizer</span><span class="params">(String schema)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(schema);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">vectorize</span><span class="params">(Person element, VectorizedRowBatch batch)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    BytesColumnVector nameColVector = (BytesColumnVector) batch.cols[<span class="number">0</span>];</span><br><span class="line">    LongColumnVector ageColVector = (LongColumnVector) batch.cols[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> row = batch.size++;</span><br><span class="line">    nameColVector.setVal(row, element.getName().getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    ageColVector.vector[row] = element.getAge();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果在程序中使用 ORC 的 Bulk-encoded Format,需要添加如下依赖到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-orc_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>然后,类似这样使用 FileSink 以 ORC Format 输出数据:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.connector.file.sink.FileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.orc.writer.OrcBulkWriterFactory;</span><br><span class="line"></span><br><span class="line">String schema = <span class="string">&quot;struct&lt;_col0:string,_col1:int&gt;&quot;</span>;</span><br><span class="line">DataStream&lt;Person&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> OrcBulkWriterFactory&lt;Person&gt; writerFactory = <span class="keyword">new</span> OrcBulkWriterFactory&lt;&gt;(<span class="keyword">new</span> PersonVectorizer(schema));</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> FileSink&lt;Person&gt; sink = FileSink</span><br><span class="line">  .forBulkFormat(outputBasePath, writerFactory)</span><br><span class="line">  .build();</span><br><span class="line"></span><br><span class="line">input.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>OrcBulkWriterFactory 还可以采用 Hadoop 的 Configuration 和 Properties,这样就可以提供自定义的 Hadoop 配置 和 ORC 输出属性.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">String schema = ...;</span><br><span class="line">Configuration conf = ...;</span><br><span class="line">Properties writerProperties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">writerProperties.setProperty(<span class="string">&quot;orc.compress&quot;</span>, <span class="string">&quot;LZ4&quot;</span>);</span><br><span class="line"><span class="comment">// 其他 ORC 属性也可以使用类似方式进行设置</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> OrcBulkWriterFactory&lt;Person&gt; writerFactory = <span class="keyword">new</span> OrcBulkWriterFactory&lt;&gt;(</span><br><span class="line">    <span class="keyword">new</span> PersonVectorizer(schema), writerProperties, conf);</span><br></pre></td></tr></table></figure>

<p>完整的 ORC 输出属性列表可以参考 此文档 .</p>
<p>用户在重写 vectorize(...) 方法时可以调用 addUserMetadata(...) 方法来添加自己的元数据到 ORC 文件中.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PersonVectorizer</span> <span class="keyword">extends</span> <span class="title">Vectorizer</span>&lt;<span class="title">Person</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">vectorize</span><span class="params">(Person element, VectorizedRowBatch batch)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    String metadataKey = ...;</span><br><span class="line">    ByteBuffer metadataValue = ...;</span><br><span class="line">    <span class="keyword">this</span>.addUserMetadata(metadataKey, metadataValue);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Hadoop-SequenceFile-Format"><a href="#Hadoop-SequenceFile-Format" class="headerlink" title="Hadoop SequenceFile Format"></a>Hadoop SequenceFile Format</h5><p>如果在程序中使用 SequenceFile 的 Bulk-encoded Format,需要添加如下依赖到项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-sequence-file<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>类似这样创建一个简单的 SequenceFile:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.connector.file.sink.FileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.GlobalConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.SequenceFile;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;LongWritable, Text&gt;&gt; input = ...;</span><br><span class="line">Configuration hadoopConf = HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration());</span><br><span class="line"><span class="keyword">final</span> FileSink&lt;Tuple2&lt;LongWritable, Text&gt;&gt; sink = FileSink</span><br><span class="line">  .forBulkFormat(</span><br><span class="line">    outputBasePath,</span><br><span class="line">    <span class="keyword">new</span> SequenceFileWriterFactory&lt;&gt;(hadoopConf, LongWritable.class, Text.class))</span><br><span class="line">  .build();</span><br><span class="line"></span><br><span class="line">input.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>SequenceFileWriterFactory 提供额外的构造参数设置是否开启压缩功能.</p>
<h4 id="桶分配"><a href="#桶分配" class="headerlink" title="桶分配"></a>桶分配</h4><p>桶的逻辑定义了如何将数据分配到基本输出目录内的子目录中.</p>
<p>Row-encoded Format 和 Bulk-encoded Format (参考 Format Types) 使用了 DateTimeBucketAssigner 作为默认的分配器.<br>默认的分配器 DateTimeBucketAssigner 会基于使用了格式为 yyyy-MM-dd--HH 的系统默认时区来创建小时桶.<br>日期格式( 即 桶大小)和时区都可以手动配置.</p>
<p>还可以在格式化构造器中通过调用 .withBucketAssigner(assigner) 方法指定自定义的 BucketAssigner.</p>
<p>Flink 内置了两种 BucketAssigners:</p>
<ol>
<li>DateTimeBucketAssigner :默认的基于时间的分配器</li>
<li>BasePathBucketAssigner :分配所有文件存储在基础路径上(单个全局桶)</li>
</ol>
<h4 id="滚动策略"><a href="#滚动策略" class="headerlink" title="滚动策略"></a>滚动策略</h4><p>RollingPolicy 定义了何时关闭给定的 In-progress Part 文件,并将其转换为 Pending 状态,然后在转换为 Finished 状态.<br>Finished 状态的文件,可供查看并且可以保证数据的有效性,在出现故障时不会恢复.<br>在 STREAMING 模式下,滚动策略结合 Checkpoint 间隔(到下一个 Checkpoint 成功时,文件的 Pending 状态才转换为 Finished 状态)共同控制 Part 文件对下游 readers 是否可见以及这些文件的大小和数量.<br>在 BATCH 模式下,Part 文件在 Job 最后对下游才变得可见,滚动策略只控制最大的 Part 文件大小.</p>
<p>Flink 内置了两种 RollingPolicies:<br>DefaultRollingPolicy<br>OnCheckpointRollingPolicy</p>
<h4 id="Part-文件生命周期"><a href="#Part-文件生命周期" class="headerlink" title="Part 文件生命周期"></a>Part 文件生命周期</h4><p>为了在下游使用 FileSink 作为输出,需要了解生成的输出文件的命名和生命周期.</p>
<p>Part 文件可以处于以下三种状态中的任意一种:</p>
<ol>
<li>In-progress :当前正在写入的 Part 文件处于 in-progress 状态</li>
<li>Pending :由于指定的滚动策略)关闭 in-progress 状态文件,并且等待提交</li>
<li>Finished :流模式(STREAMING)下的成功的 Checkpoint 或者批模式(BATCH)下输入结束,文件的 Pending 状态转换为 Finished 状态</li>
</ol>
<p>只有 Finished 状态下的文件才能被下游安全读取,并且保证不会被修改.</p>
<p>对于每个活动的桶,在任何给定时间每个写入 Subtask 中都有一个 In-progress 状态的 Part 文件,但可能有多个 Pending 状态和 Finished 状态的文件.</p>
<h5 id="Part-文件示例"><a href="#Part-文件示例" class="headerlink" title="Part 文件示例"></a>Part 文件示例</h5><p>为了更好的了解这些文件的生命周期,让我们看一个只有 2 个 Sink Subtask 的简单例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">└── 2019-08-25--12</span><br><span class="line">    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334</span><br><span class="line">    └── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575</span><br></pre></td></tr></table></figure>

<p>当这个 Part 文件 part-81fc4980-a6af-41c8-9937-9939408a734b-0 滚动时(比如说此文件变的很大时),此文件将进入 Pending 状态并且不能重命名.<br>Sink 就会打开一个新的 Part 文件: part-81fc4980-a6af-41c8-9937-9939408a734b-1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">└── 2019-08-25--12</span><br><span class="line">    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334</span><br><span class="line">    ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575</span><br><span class="line">    └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11</span><br></pre></td></tr></table></figure>

<p>part-81fc4980-a6af-41c8-9937-9939408a734b-0 现在是 Pending 状态,并且在下一个 Checkpoint 成功过后,立即成为 Finished 状态:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">└── 2019-08-25--12</span><br><span class="line">    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334</span><br><span class="line">    ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0</span><br><span class="line">    └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11</span><br></pre></td></tr></table></figure>

<p>根据桶策略创建新桶时,不会影响当前 In-progress 状态的文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">└── 2019-08-25--12</span><br><span class="line">    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334</span><br><span class="line">    ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0</span><br><span class="line">    └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11</span><br><span class="line">└── 2019-08-25--13</span><br><span class="line">    └── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.2b475fec-1482-4dea-9946-eb4353b475f1</span><br></pre></td></tr></table></figure>

<p>旧桶仍然可以接收新记录,因为桶策略是在每条记录上进行评估的.</p>
<h5 id="Part-文件配置"><a href="#Part-文件配置" class="headerlink" title="Part 文件配置"></a>Part 文件配置</h5><p>Finished 状态与 In-progress 状态的文件只能通过命名来区分.</p>
<p>默认的,文件命名策略如下:</p>
<ol>
<li>In-progress / Pending:part-<uid>-<partFileIndex>.inprogress.uid</li>
<li>Finished:part-<uid>-<partFileIndex> 当 Sink Subtask 实例化时,这的 uid 是一个分配给 Subtask 的随机 ID 值.<br>这个 uid 不具有容错机制,所以当 Subtask 从故障恢复时,uid 会重新生成.</li>
</ol>
<p>Flink 允许用户给 Part 文件名添加一个前缀和/或后缀.<br>使用 OutputFileConfig 来完成上述功能.<br>例如,Sink 将在创建文件的文件名上添加前缀 &quot;prefix&quot; 和后缀 &quot;.ext&quot;,如下所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">└── 2019-08-25--12</span><br><span class="line">    ├── prefix-4005733d-a830-4323-8291-8866de98b582-0.ext</span><br><span class="line">    ├── prefix-4005733d-a830-4323-8291-8866de98b582-1.ext.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334</span><br><span class="line">    ├── prefix-81fc4980-a6af-41c8-9937-9939408a734b-0.ext</span><br><span class="line">    └── prefix-81fc4980-a6af-41c8-9937-9939408a734b-1.ext.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11</span><br></pre></td></tr></table></figure>

<p>用户也可以使用 OutputFileConfig 采用如下方式添加前缀和后缀:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">OutputFileConfig config = OutputFileConfig</span><br><span class="line"> .builder()</span><br><span class="line"> .withPartPrefix(<span class="string">&quot;prefix&quot;</span>)</span><br><span class="line"> .withPartSuffix(<span class="string">&quot;.ext&quot;</span>)</span><br><span class="line"> .build();</span><br><span class="line">            </span><br><span class="line">FileSink&lt;Tuple2&lt;Integer, Integer&gt;&gt; sink = FileSink</span><br><span class="line"> .forRowFormat((<span class="keyword">new</span> Path(outputPath), <span class="keyword">new</span> SimpleStringEncoder&lt;&gt;(<span class="string">&quot;UTF-8&quot;</span>))</span><br><span class="line"> .withBucketAssigner(<span class="keyword">new</span> KeyBucketAssigner())</span><br><span class="line"> .withRollingPolicy(OnCheckpointRollingPolicy.build())</span><br><span class="line"> .withOutputFileConfig(config)</span><br><span class="line"> .build());</span><br></pre></td></tr></table></figure>

<h4 id="文件合并"><a href="#文件合并" class="headerlink" title="文件合并"></a>文件合并</h4><p>从 1.15 版本开始 FileSink 开始支持已经提交 pending 文件的合并,从而允许应用设置一个较小的时间周期并且避免生成大量的小文件.<br>尤其是当用户使用 bulk 格式 的时候: 这种格式要求用户必须在 checkpoint 的时候切换文件.</p>
<p>文件合并功能可以通过以下代码打开:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FileSink&lt;Integer&gt; fileSink=</span><br><span class="line">        FileSink.forRowFormat(<span class="keyword">new</span> Path(path),<span class="keyword">new</span> SimpleStringEncoder&lt;Integer&gt;())</span><br><span class="line">            .enableCompact(</span><br><span class="line">                FileCompactStrategy.Builder.newBuilder()</span><br><span class="line">                    .setNumCompactThreads(<span class="number">1024</span>)</span><br><span class="line">                    .enableCompactionOnCheckpoint(<span class="number">5</span>)</span><br><span class="line">                    .build(),</span><br><span class="line">                <span class="keyword">new</span> RecordWiseFileCompactor&lt;&gt;(</span><br><span class="line">                    <span class="keyword">new</span> DecoderBasedReader.Factory&lt;&gt;(SimpleStringDecoder::<span class="keyword">new</span>)))</span><br><span class="line">            .build();</span><br></pre></td></tr></table></figure>

<p>这一功能开启后,在文件转为 pending 状态与文件最终提交之间会进行文件合并.<br>这些 pending 状态的文件将首先被提交为一个以 . 开头的 临时文件.<br>这些文件随后将会按照用户指定的策略和合并方式进行合并并生成合并后的 pending 状态的文件.<br>然后这些文件将被发送给 Committer 并提交为正式文件,在这之后,原始的临时文件也会被删除掉.</p>
<p>当开启文件合并功能时,用户需要指定 FileCompactStrategy 与 FileCompactor .</p>
<p>FileCompactStrategy 指定何时以及哪些文件将被合并.<br>目前有两个并行的条件:目标文件大小与间隔的 Checkpoint 数量.<br>当目前缓存的文件的总大小达到指定的阈值,或自上次合并后经过的 Checkpoint 次数已经达到指定次数时, FileSink 将创建一个异步任务来合并当前缓存的文件.</p>
<p>FileCompactor 指定如何将给定的路径列表对应的文件进行合并将结果写入到文件中.<br>根据如何写文件,它可以分为两类:</p>
<ol>
<li><p>OutputStreamBasedFileCompactor : 用户将合并后的结果写入一个输出流中.<br>通常在用户不希望或者无法从输入文件中读取记录时使用.<br>这种类型的 CompactingFileWriter 的一个例子是 ConcatFileCompactor ,它直接将给定的文件进行合并并将结果写到输出流中.</p>
</li>
<li><p>RecordWiseFileCompactor : 这种类型的 CompactingFileWriter 会逐条读出输入文件的记录用户,然后和FileWriter一样写入输出文件中.<br>CompactingFileWriter 的一个例子是 RecordWiseFileCompactor ,它从给定的文件中读出记录并写出到 CompactingFileWriter 中.<br>用户需要指定如何从原始文件中读出记录.</p>
</li>
</ol>
<p>一旦启用了文件合并功能,此后若需要再关闭,必须在构建FileSink时显式调用disableCompact方法.<br>如果启用了文件合并功能,文件可见的时间会被延长.</p>
<h4 id="重要注意事项"><a href="#重要注意事项" class="headerlink" title="重要注意事项"></a>重要注意事项</h4><h5 id="通用注意事项"><a href="#通用注意事项" class="headerlink" title="通用注意事项"></a>通用注意事项</h5><p>当使用的 Hadoop 版本 &lt; 2.7 时, 当每次 Checkpoint 时请使用 OnCheckpointRollingPolicy 滚动 Part 文件.<br>原因是:如果 Part 文件 &quot;穿越&quot; 了 Checkpoint 的时间间隔, 然后,从失败中恢复过来时,FileSink 可能会使用文件系统的 truncate() 方法丢弃处于 In-progress 状态文件中的未提交数据.<br>这个方法在 Hadoop 2.7 版本之前是不支持的,Flink 将抛出异常.</p>
<p>鉴于 Flink 的 Sink 和 UDF 通常不会区分正常作业终止(例如 有限输入流)和 由于故障而终止, 在 Job 正常终止时,最后一个 In-progress 状态文件不会转换为 &quot;Finished&quot; 状态.</p>
<p>Flink 和 FileSink 永远不会覆盖已提交数据.<br>鉴于此,假定一个 In-progress 状态文件被后续成功的 Checkpoint 提交了,当尝试从这个旧的 Checkpoint / Savepoint 进行恢复时,FileSink 将拒绝继续执行并将抛出异常,因为程序无法找到 In-progress 状态的文件.</p>
<p>目前,FileSink 仅支持以下3种文件系统:HDFS/ S3 和 Local.<br>如果在运行时使用了不支持的文件系统,Flink 将抛出异常.</p>
<h5 id="BATCH-注意事项"><a href="#BATCH-注意事项" class="headerlink" title="BATCH 注意事项"></a>BATCH 注意事项</h5><p>虽然 Writer 是以用户指定的 parallelism 执行的,然而 Committer 是以 parallelism = 1 执行的.</p>
<p>Pending 状态文件被提交并且所有输入数据被处理完后,才转换为 Finished 状态.</p>
<p>当系统处于高可用状态下,并且正当 Committers 进行提交时如果 JobManager 发生了故障,那么可能会有副本.<br>这种情况将会在 Flink 的未来版本中进行修复.<br>(可以参考 FLIP-147 ) .</p>
<h5 id="S3-注意事项"><a href="#S3-注意事项" class="headerlink" title="S3 注意事项"></a>S3 注意事项</h5><p>对于 S3,FileSink 仅支持基于 Hadoop-based 文件系统的实现,而不支持基于 Presto 的实现.<br>如果 Job 中使用 FileSink 写入 S3,但是希望使用基于 Presto 的 Sink 做 Checkpoint,建议明确使用 &quot;s3a://&quot; (对于 Hadoop)作为 Sink 目标路径格式并且使用 &quot;s3p://&quot; 作为 Checkpoint 的目标路径格式(对于 Presto).<br>对于 Sink 和 Checkpoint 同时使用 &quot;s3://&quot; 可能导致不可控的行为,由于两者的实现 &quot;监听&quot; 同一格式路径.</p>
<p>在保证高效的同时还要保证 exactly-once 语义,FileSink 使用了 S3 的 Multi-part Upload 功能(MPU 功能开箱即用).<br>此功能允许以独立的块上传文件(因此称为 &quot;multi-part&quot;),当 MPU 的所有块都上传成功时,这些块就可以合并生成原始文件.<br>对于非活动的 MPU,S3 支持桶生命周期规则,用户可以使用该规则终止在启动后指定天数内未完成的多块上传操作.<br>这意味着,如果设置了这个规则,并在某些文件未完全上传的情况下执行 Savepoint,则其关联的 MPU 可能会在 Job 重启前超时.<br>这将导致 Job 无法从该 Savepoint 恢复,因为 Pending 状态的 Part 文件已不存在,那么 Flink Job 将失败并抛出异常,因为程序试图获取那些不存在的文件导致了失败.</p>
<h2 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h2><p>这个连接器可以访问 RabbitMQ 的数据流.<br>使用这个连接器,需要在工程里添加下面的依赖:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-rabbitmq<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>注意连接器现在没有包含在二进制发行版中.</p>
<h3 id="RabbitMQ-Source"><a href="#RabbitMQ-Source" class="headerlink" title="RabbitMQ Source"></a>RabbitMQ Source</h3><p>RMQSource 负责从 RabbitMQ 中消费数据,可以配置三种不同级别的保证:<br>1)精确一次: 保证精确一次需要以下条件</p>
<ol>
<li><p>开启 checkpointing: 开启 checkpointing 之后,消息在 checkpoints 完成之后才会被确认(然后从 RabbitMQ 队列中删除).</p>
</li>
<li><p>使用关联标识(Correlation ids): 关联标识是 RabbitMQ 的一个特性,消息写入 RabbitMQ 时在消息属性中设置.<br>从 checkpoint 恢复时有些消息可能会被重复处理,source 可以利用关联标识对消息进行去重.</p>
</li>
<li><p>非并发 source: 为了保证精确一次的数据投递,source 必须是非并发的(并行度设置为1).<br>这主要是由于 RabbitMQ 分发数据时是从单队列向多个消费者投递消息的.</p>
</li>
</ol>
<p>2)至少一次: 在 checkpointing 开启的条件下,如果没有使用关联标识或者 source 是并发的, 那么 source 就只能提供至少一次的保证.</p>
<p>3)无任何保证: 如果没有开启 checkpointing,source 就不能提供任何的数据投递保证.<br>使用这种设置时,source 一旦接收到并处理消息,消息就会被自动确认.</p>
<p>下面是一个保证 exactly-once 的 RabbitMQ source 示例.<br>注释部分展示了更加宽松的保证应该如何配置.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// checkpointing is required for exactly-once or at-least-once guarantees</span></span><br><span class="line">env.enableCheckpointing(...);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig.Builder()</span><br><span class="line">    .setHost(<span class="string">&quot;localhost&quot;</span>)</span><br><span class="line">    .setPort(<span class="number">5000</span>)</span><br><span class="line">    ...</span><br><span class="line">    .build();</span><br><span class="line">    </span><br><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> RMQSource&lt;String&gt;(</span><br><span class="line">        connectionConfig,            <span class="comment">// config for the RabbitMQ connection</span></span><br><span class="line">        <span class="string">&quot;queueName&quot;</span>,                 <span class="comment">// name of the RabbitMQ queue to consume</span></span><br><span class="line">        <span class="keyword">true</span>,                        <span class="comment">// use correlation ids; can be false if only at-least-once is required</span></span><br><span class="line">        <span class="keyword">new</span> SimpleStringSchema()))   <span class="comment">// deserialization schema to turn messages into Java objects</span></span><br><span class="line">    .setParallelism(<span class="number">1</span>);              <span class="comment">// non-parallel source is only required for exactly-once</span></span><br></pre></td></tr></table></figure>

<h4 id="服务质量-QoS-消费者预取-Consumer-Prefetch"><a href="#服务质量-QoS-消费者预取-Consumer-Prefetch" class="headerlink" title="服务质量 (QoS) / 消费者预取(Consumer Prefetch)"></a>服务质量 (QoS) / 消费者预取(Consumer Prefetch)</h4><p>RabbitMQ Source 通过 RMQConnectionConfig 类提供了一种简单的方式,来设置 source channel 上的 basicQos(见下方示例).<br>要注意的是这里的 prefetch count 是对单个 channel 设置的,并且由于每个并发的 source 都持有一个 connection/channel,因此这个值实际上会乘以 source 的并行度,来表示同一时间可以向这个 job 总共发送多少条未确认的消息.<br>如果需要更复杂的配置,可以通过重写 RMQSource#setupChannel(Connection) 方法来实现手动配置.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig.Builder()</span><br><span class="line">    .setPrefetchCount(<span class="number">30_000</span>)</span><br><span class="line">    ...</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>

<p>RabbitMQ Source 默认情况下是不设置 prefetch count 的,这意味着 RabbitMQ 服务器将会无限制地向 source 发送消息.<br>因此在生产环境中,最好要设置它.<br>当消费海量数据的队列并且启用 checkpointing 时,消息只有在做完 checkpoint 后才会被确认,因此也许需要对 prefetch count 做一些调整来减少不必要的循环.</p>
<p>更多关于 QoS 以及 prefetch 相关的内容可以参考 这里. 更多关于在 AMQP 0-9-1 中可选的选项可以参考 这里.</p>
<h3 id="RabbitMQ-Sink"><a href="#RabbitMQ-Sink" class="headerlink" title="RabbitMQ Sink"></a>RabbitMQ Sink</h3><p>该连接器提供了一个 RMQSink 类,用来向 RabbitMQ 队列发送数据.<br>下面是设置 RabbitMQ sink 的代码示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig.Builder()</span><br><span class="line">    .setHost(<span class="string">&quot;localhost&quot;</span>)</span><br><span class="line">    .setPort(<span class="number">5000</span>)</span><br><span class="line">    ...</span><br><span class="line">    .build();</span><br><span class="line">    </span><br><span class="line">stream.addSink(<span class="keyword">new</span> RMQSink&lt;String&gt;(</span><br><span class="line">    connectionConfig,            <span class="comment">// config for the RabbitMQ connection</span></span><br><span class="line">    <span class="string">&quot;queueName&quot;</span>,                 <span class="comment">// name of the RabbitMQ queue to send messages to</span></span><br><span class="line">    <span class="keyword">new</span> SimpleStringSchema()));  <span class="comment">// serialization schema to turn Java objects to messages</span></span><br></pre></td></tr></table></figure>

<h2 id="Google-Cloud-PubSub"><a href="#Google-Cloud-PubSub" class="headerlink" title="Google Cloud PubSub"></a>Google Cloud PubSub</h2><h2 id="Hybrid-Source"><a href="#Hybrid-Source" class="headerlink" title="Hybrid Source"></a>Hybrid Source</h2><p>HybridSource是一个包含具体源列表的源.<br>它解决了从异构源顺序读取输入以产生单个输入流的问题.</p>
<p>例如,一个引导用例可能需要从 S3 读取几天的有界输入,然后才能继续从 Kafka 获取最新的无界输入.<br>在有界文件输入完成而不中断应用程序的情况下HybridSource从 切换FileSource到KafkaSource.</p>
<p>在此之前HybridSource,需要创建多源拓扑并在用户域定义切换机制,导致操作复杂和效率低下.</p>
<p>从API 角度来看HybridSource,多个源在 Flink 作业图中显示为单个源.</p>
<p>要使用连接器,请将flink-connector-base依赖项添加到您的项目中:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-base<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="Start-position-for-next-source"><a href="#Start-position-for-next-source" class="headerlink" title="Start position for next source"></a>Start position for next source</h3><p>要在 a 中排列多个源,HybridSource除了最后一个源之外的所有源都需要有界.<br>因此,通常需要为源分配开始和结束位置.<br>最后一个源可能是有界的,在这种情况下HybridSource是有界的,否则是无界的.<br>详细信息取决于特定来源和外部存储系统.</p>
<p>在这里,我们按照 File/Kafka 示例介绍最基本的场景,然后介绍更复杂的场景.</p>
<h4 id="修复了图构建时的开始位置"><a href="#修复了图构建时的开始位置" class="headerlink" title="修复了图构建时的开始位置"></a>修复了图构建时的开始位置</h4><p>示例:从文件中读取到预定的切换时间,然后继续从 Kafka 中读取.<br>每个源都覆盖了一个预先已知的范围,因此可以预先创建包含的源,就好像它们被直接使用一样:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> switchTimestamp = ...; <span class="comment">// derive from file input paths</span></span><br><span class="line">FileSource&lt;String&gt; fileSource =</span><br><span class="line">  FileSource.forRecordStreamFormat(<span class="keyword">new</span> TextLineInputFormat(), Path.fromLocalFile(testDir)).build();</span><br><span class="line">KafkaSource&lt;String&gt; kafkaSource =</span><br><span class="line">          KafkaSource.&lt;String&gt;builder()</span><br><span class="line">                  .setStartingOffsets(OffsetsInitializer.timestamp(switchTimestamp + <span class="number">1</span>))</span><br><span class="line">                  .build();</span><br><span class="line">HybridSource&lt;String&gt; hybridSource =</span><br><span class="line">          HybridSource.builder(fileSource)</span><br><span class="line">                  .addSource(kafkaSource)</span><br><span class="line">                  .build();</span><br></pre></td></tr></table></figure>

<h4 id="切换时间的动态起始位置"><a href="#切换时间的动态起始位置" class="headerlink" title="切换时间的动态起始位置"></a>切换时间的动态起始位置</h4><p>示例:文件源读取非常大的积压,可能比下一个源可用的保留时间更长.<br>切换需要发生在&quot;当前时间 - X&quot;.<br>这需要在切换时间设置下一个源的开始时间.<br>在这里,我们需要从前一个文件枚举器中传输结束位置,以便KafkaSource 通过实现SourceFactory.</p>
<p>请注意,枚举器需要支持获取结束时间戳.<br>这当前可能需要源自定义.<br>在FLINK-23633FileSource中跟踪添加对动态结束位置的支持.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">FileSource&lt;String&gt; fileSource = CustomFileSource.readTillOneDayFromLatest();</span><br><span class="line">HybridSource&lt;String&gt; hybridSource =</span><br><span class="line">    HybridSource.&lt;String, CustomFileSplitEnumerator&gt;builder(fileSource)</span><br><span class="line">        .addSource(</span><br><span class="line">            switchContext -&gt; &#123;</span><br><span class="line">              CustomFileSplitEnumerator previousEnumerator =</span><br><span class="line">                  switchContext.getPreviousEnumerator();</span><br><span class="line">              <span class="comment">// how to get timestamp depends on specific enumerator</span></span><br><span class="line">              <span class="keyword">long</span> switchTimestamp = previousEnumerator.getEndTimestamp();</span><br><span class="line">              KafkaSource&lt;String&gt; kafkaSource =</span><br><span class="line">                  KafkaSource.&lt;String&gt;builder()</span><br><span class="line">                      .setStartingOffsets(OffsetsInitializer.timestamp(switchTimestamp + <span class="number">1</span>))</span><br><span class="line">                      .build();</span><br><span class="line">              <span class="keyword">return</span> kafkaSource;</span><br><span class="line">            &#125;,</span><br><span class="line">            Boundedness.CONTINUOUS_UNBOUNDED)</span><br><span class="line">        .build();</span><br></pre></td></tr></table></figure>

<h2 id="Apache-NiFi"><a href="#Apache-NiFi" class="headerlink" title="Apache NiFi"></a>Apache NiFi</h2><p>Apache NiFi 连接器提供了可以读取和写入的 Source 和 Sink.<br>使用这个连接器,需要在工程中添加下面的依赖:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-nifi<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="Apache-NiFi-Source"><a href="#Apache-NiFi-Source" class="headerlink" title="Apache NiFi Source"></a>Apache NiFi Source</h3><p>该连接器提供了一个 Source 可以用来从 Apache NiFi 读取数据到 Apache Flink.</p>
<p>NiFiSource(…) 类有两个构造方法.</p>
<ol>
<li>NiFiSource(SiteToSiteConfig config) - 构造一个 NiFiSource(…) ,需要指定参数 SiteToSiteConfig ,采用默认的等待时间 1000 ms.</li>
<li>NiFiSource(SiteToSiteConfig config, long waitTimeMs) - 构造一个 NiFiSource(…),需要指定参数 SiteToSiteConfig 和等待时间(单位为毫秒).</li>
</ol>
<p>示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment streamExecEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">SiteToSiteClientConfig clientConfig = <span class="keyword">new</span> SiteToSiteClient.Builder()</span><br><span class="line">        .url(<span class="string">&quot;http://localhost:8080/nifi&quot;</span>)</span><br><span class="line">        .portName(<span class="string">&quot;Data for Flink&quot;</span>)</span><br><span class="line">        .requestBatchCount(<span class="number">5</span>)</span><br><span class="line">        .buildConfig();</span><br><span class="line"></span><br><span class="line">SourceFunction&lt;NiFiDataPacket&gt; nifiSource = <span class="keyword">new</span> NiFiSource(clientConfig);</span><br></pre></td></tr></table></figure>

<p>数据从 Apache NiFi Output Port 读取,Apache NiFi Output Port 也被称为 &quot;Data for Flink&quot;,是 Apache NiFi Site-to-site 协议配置的一部分.</p>
<h3 id="Apache-NiFi-Sink"><a href="#Apache-NiFi-Sink" class="headerlink" title="Apache NiFi Sink"></a>Apache NiFi Sink</h3><p>该连接器提供了一个 Sink 可以用来把 Apache Flink 的数据写入到 Apache NiFi.</p>
<p>NiFiSink(…) 类只有一个构造方法.</p>
<ol>
<li>NiFiSink(SiteToSiteClientConfig, NiFiDataPacketBuilder<T>) 构造一个 NiFiSink(…),需要指定 SiteToSiteConfig 和 NiFiDataPacketBuilder 参数 ,NiFiDataPacketBuilder 可以将Flink数据转化成可以被NiFi识别的 NiFiDataPacket.</li>
</ol>
<p>示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment streamExecEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">SiteToSiteClientConfig clientConfig = <span class="keyword">new</span> SiteToSiteClient.Builder()</span><br><span class="line">        .url(<span class="string">&quot;http://localhost:8080/nifi&quot;</span>)</span><br><span class="line">        .portName(<span class="string">&quot;Data from Flink&quot;</span>)</span><br><span class="line">        .requestBatchCount(<span class="number">5</span>)</span><br><span class="line">        .buildConfig();</span><br><span class="line"></span><br><span class="line">SinkFunction&lt;NiFiDataPacket&gt; nifiSink = <span class="keyword">new</span> NiFiSink&lt;&gt;(clientConfig, <span class="keyword">new</span> NiFiDataPacketBuilder&lt;T&gt;() &#123;...&#125;);</span><br><span class="line"></span><br><span class="line">streamExecEnv.addSink(nifiSink);</span><br></pre></td></tr></table></figure>

<p>更多关于 Apache NiFi Site-to-Site Protocol 的信息请参考 这里.</p>
<h2 id="Apache-Pulsar"><a href="#Apache-Pulsar" class="headerlink" title="Apache Pulsar"></a>Apache Pulsar</h2><p>Flink 当前提供 Apache Pulsar Source 和 Sink 连接器,用户可以使用它从 Pulsar 读取数据,并保证每条数据只被处理一次.</p>
<h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><p>Pulsar Source 当前支持 Pulsar 2.8.1 之后的版本,但是 Pulsar Source 使用到了 Pulsar 的事务机制,建议在 Pulsar 2.9.2 及其之后的版本上使用 Pulsar Source 进行数据读取.</p>
<p>如果想要了解更多关于 Pulsar API 兼容性设计,可以阅读文档 PIP-72.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-pulsar<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Flink 的流连接器并不会放到发行文件里面一同发布,阅读此文档,了解如何将连接器添加到集群实例内.</p>
<h3 id="Pulsar-Source"><a href="#Pulsar-Source" class="headerlink" title="Pulsar Source"></a>Pulsar Source</h3><p>Pulsar Source 基于 Flink 最新的批流一体 API 进行开发.</p>
<h4 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h4><p>Pulsar Source 提供了 builder 类来构造 PulsarSource 实例.<br>下面的代码实例使用 builder 类创建的实例会从 &quot;persistent://public/default/my-topic&quot; 的数据开始端进行消费.<br>对应的 Pulsar Source 使用了 Exclusive(独占)的订阅方式消费消息,订阅名称为 my-subscription,并把消息体的二进制字节流以 UTF-8 的方式编码为字符串.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PulsarSource&lt;String&gt; pulsarSource = PulsarSource.builder()</span><br><span class="line">    .setServiceUrl(serviceUrl)</span><br><span class="line">    .setAdminUrl(adminUrl)</span><br><span class="line">    .setStartCursor(StartCursor.earliest())</span><br><span class="line">    .setTopics(<span class="string">&quot;my-topic&quot;</span>)</span><br><span class="line">    .setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(<span class="keyword">new</span> SimpleStringSchema()))</span><br><span class="line">    .setSubscriptionName(<span class="string">&quot;my-subscription&quot;</span>)</span><br><span class="line">    .setSubscriptionType(SubscriptionType.Exclusive)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">env.fromSource(source, WatermarkStrategy.noWatermarks(), <span class="string">&quot;Pulsar Source&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>如果使用构造类构造 PulsarSource,一定要提供下面几个属性:</p>
<ol>
<li>Pulsar 数据消费的地址,使用 setServiceUrl(String) 方法提供.</li>
<li>Pulsar HTTP 管理地址,使用 setAdminUrl(String) 方法提供.</li>
<li>Pulsar 订阅名称,使用 setSubscriptionName(String) 方法提供.</li>
<li>需要消费的 Topic 或者是 Topic 下面的分区,详见指定消费的 Topic 或者 Topic 分区.</li>
<li>解码 Pulsar 消息的反序列化器,详见反序列化器.</li>
</ol>
<h4 id="指定消费的-Topic-或者-Topic-分区"><a href="#指定消费的-Topic-或者-Topic-分区" class="headerlink" title="指定消费的 Topic 或者 Topic 分区"></a>指定消费的 Topic 或者 Topic 分区</h4><p>Pulsar Source 提供了两种订阅 Topic 或 Topic 分区的方式.</p>
<ol>
<li><p>Topic 列表,从这个 Topic 的所有分区上消费消息,例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PulsarSource.builder().setTopics(<span class="string">&quot;some-topic1&quot;</span>, <span class="string">&quot;some-topic2&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 topic &quot;topic-a&quot; 的 0 和 2 分区上消费</span></span><br><span class="line">PulsarSource.builder().setTopics(<span class="string">&quot;topic-a-partition-0&quot;</span>, <span class="string">&quot;topic-a-partition-2&quot;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>Topic 正则,Pulsar Source 使用给定的正则表达式匹配出所有合规的 Topic,例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PulsarSource.builder().setTopicPattern(<span class="string">&quot;topic-*&quot;</span>);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="Topic-名称简写"><a href="#Topic-名称简写" class="headerlink" title="Topic 名称简写"></a>Topic 名称简写</h5><p>从 Pulsar 2.0 之后,完整的 Topic 名称格式为 {persistent|non-persistent}://租户/命名空间/topic.<br>但是 Pulsar Source 不需要提供 Topic 名称的完整定义,因为 Topic 类型/租户/命名空间都设置了默认值.<br>Topic 类型:persistent<br>租户:public<br>命名空间:default</p>
<p>下面的表格提供了当前 Pulsar Topic 支持的简写方式:<br><img src="/images/flgl121.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<p>对于 Non-persistent(非持久化)Topic,Pulsar Source 不支持简写名称.<br>所以无法将 non-persistent://public/default/my-topic 简写成 non-persistent://my-topic.</p>
<h5 id="Pulsar-Topic-层次结构"><a href="#Pulsar-Topic-层次结构" class="headerlink" title="Pulsar Topic 层次结构"></a>Pulsar Topic 层次结构</h5><p>对于 Pulsar 而言,Topic 分区也是一种 Topic.<br>Pulsar 会将一个有分区的 Topic 在内部按照分区的大小拆分成等量的无分区 Topic.</p>
<p>由于 Pulsar 内部的分区实际实现为一个 Topic,我们将用&quot;分区&quot;来指代&quot;仅有一个分区的 Topic(Non-partitioned Topic)&quot;和&quot;具有多个分区的 Topic 下属的分区&quot;.</p>
<p>例如,在 Pulsar 的 sample 租户下面的 flink 命名空间里面创建了一个有 3 个分区的 Topic,给它起名为 simple-string.<br>可以在 Pulsar 上看到如下的 Topic 列表:</p>
<p>Topic 名称  是否分区<br>persistent://sample/flink/simple-string 是<br>persistent://sample/flink/simple-string-partition-0 否<br>persistent://sample/flink/simple-string-partition-1 否<br>persistent://sample/flink/simple-string-partition-2 否</p>
<p>这意味着,用户可以用上面的子 Topic 去直接消费分区里面的数据,不需要再去基于上层的父 Topic 去消费全部分区的数据.<br>例如:使用 PulsarSource.builder().setTopics(&quot;sample/flink/simple-string-partition-1&quot;, &quot;sample/flink/simple-string-partition-2&quot;) 将会只消费 Topic sample/flink/simple-string 分区 1 和 2 里面的消息.</p>
<h5 id="配置-Topic-正则表达式"><a href="#配置-Topic-正则表达式" class="headerlink" title="配置 Topic 正则表达式"></a>配置 Topic 正则表达式</h5><p>前面提到了 Pulsar Topic 有 persistent/non-persistent 两种类型,使用正则表达式消费数据的时候,Pulsar Source 会尝试从正则表达式里面解析出消息的类型.<br>例如:PulsarSource.builder().setTopicPattern(&quot;non-persistent://my-topic*&quot;) 会解析出 non-persistent 这个 Topic 类型.<br>如果用户使用 Topic 名称简写的方式,Pulsar Source 会使用默认的消息类型 persistent.</p>
<p>如果想用正则去消费 persistent 和 non-persistent 类型的 Topic,需要使用 RegexSubscriptionMode 定义 Topic 类型,例如:setTopicPattern(&quot;topic-*&quot;, RegexSubscriptionMode.AllTopics).</p>
<h4 id="反序列化器"><a href="#反序列化器" class="headerlink" title="反序列化器"></a>反序列化器</h4><p>反序列化器用于解析 Pulsar 消息,Pulsar Source 使用 PulsarDeserializationSchema 来定义反序列化器.<br>用户可以在 builder 类中使用 setDeserializationSchema(PulsarDeserializationSchema) 方法配置反序列化器.</p>
<p>如果用户只关心消息体的二进制字节流,并不需要其他属性来解析数据.<br>可以直接使用预定义的 PulsarDeserializationSchema.<br>Pulsar Source里面提供了 3 种预定义的反序列化器.</p>
<ol>
<li><p>使用 Pulsar 的 Schema 解析消息.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基础数据类型</span></span><br><span class="line">PulsarDeserializationSchema.pulsarSchema(Schema);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 结构类型 (JSON, Protobuf, Avro, etc.)</span></span><br><span class="line">PulsarDeserializationSchema.pulsarSchema(Schema, Class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 键值对类型</span></span><br><span class="line">PulsarDeserializationSchema.pulsarSchema(Schema, Class, Class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 Flink 的 DeserializationSchema 解析消息.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PulsarDeserializationSchema.flinkSchema(DeserializationSchema);</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 Flink 的 TypeInformation 解析消息.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PulsarDeserializationSchema.flinkTypeInfo(TypeInformation, ExecutionConfig);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>Pulsar 的 Message&lt;byte[]&gt; 包含了很多 额外的属性.<br>例如,消息的 key/消息发送时间/消息生产时间/用户在消息上自定义的键值对属性等.<br>可以使用 Message&lt;byte[]&gt; 接口来获取这些属性.</p>
<p>如果用户需要基于这些额外的属性来解析一条消息,可以实现 PulsarDeserializationSchema 接口.<br>并一定要确保 PulsarDeserializationSchema.getProducedType() 方法返回的 TypeInformation 是正确的结果.<br>Flink 使用 TypeInformation 将解析出来的结果序列化传递到下游算子.</p>
<h4 id="Pulsar-订阅"><a href="#Pulsar-订阅" class="headerlink" title="Pulsar 订阅"></a>Pulsar 订阅</h4><p>订阅是命名好的配置规则,指导消息如何投递给消费者.<br>Pulsar Source 需要提供一个独立的订阅名称,支持 Pulsar 的四种订阅模式:<br>exclusive(独占)<br>shared(共享)<br>failover(灾备)<br>key_shared(key 共享)</p>
<p>当前 Pulsar Source 里,独占 和 灾备 的实现没有区别,如果 Flink 的一个 reader 挂了,Pulsar Source 会把所有未消费的数据交给其他的 reader 来消费数据.</p>
<p>默认情况下,如果没有指定订阅类型,Pulsar Source 使用共享订阅类型(SubscriptionType.Shared).</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 名为 &quot;my-shared&quot; 的共享订阅</span></span><br><span class="line">PulsarSource.builder().setSubscriptionName(<span class="string">&quot;my-shared&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 名为 &quot;my-exclusive&quot; 的独占订阅</span></span><br><span class="line">PulsarSource.builder().setSubscriptionName(<span class="string">&quot;my-exclusive&quot;</span>).setSubscriptionType(SubscriptionType.Exclusive);</span><br></pre></td></tr></table></figure>

<p>如果想在 Pulsar Source 里面使用 key 共享 订阅,需要提供 RangeGenerator 实例.<br>RangeGenerator 会生成一组消息 key 的 hash 范围,Pulsar Source 会基于给定的范围来消费数据.</p>
<p>Pulsar Source 也提供了一个名为 UniformRangeGenerator 的默认实现,它会基于 flink 数据源的并行度将 hash 范围均分.</p>
<h4 id="起始消费位置"><a href="#起始消费位置" class="headerlink" title="起始消费位置"></a>起始消费位置</h4><p>Pulsar Source 使用 setStartCursor(StartCursor) 方法给定开始消费的位置.<br>内置的开始消费位置有:</p>
<ol>
<li>从 Topic 里面最早的一条消息开始消费.<br>StartCursor.earliest();</li>
<li>从 Topic 里面最新的一条消息开始消费.<br>StartCursor.latest();</li>
<li>从给定的消息开始消费.<br>StartCursor.fromMessageId(MessageId);</li>
<li>与前者不同的是,给定的消息可以跳过,再进行消费.<br>StartCursor.fromMessageId(MessageId, boolean);</li>
<li>从给定的消息发布时间开始消费,这个方法因为名称容易导致误解现在已经不建议使用.<br>你可以使用方法 StartCursor.fromPublishTime(long).<br>StartCursor.fromMessageTime(long);</li>
<li>从给定的消息发布时间开始消费.<br>StartCursor.fromPublishTime(long);</li>
</ol>
<p>每条消息都有一个固定的序列号,这个序列号在 Pulsar 上有序排列,其包含了 ledger/entry/partition 等原始信息,用于在 Pulsar 底层存储上查找到具体的消息.</p>
<p>Pulsar 称这个序列号为 MessageId,用户可以使用 DefaultImplementation.newMessageId(long ledgerId, long entryId, int partitionIndex) 创建它.</p>
<h4 id="边界"><a href="#边界" class="headerlink" title="边界"></a>边界</h4><p>Pulsar Source 默认情况下使用流的方式消费数据.<br>除非任务失败或者被取消,否则将持续消费数据.<br>用户可以使用 setBoundedStopCursor(StopCursor) 给定停止消费的位置,这种情况下会使用批的方式进行消费.<br>使用流的方式一样可以给定停止位置,使用 setUnboundedStopCursor(StopCursor) 方法即可.</p>
<p>在批模式下,使用 setBoundedStopCursor(StopCursor) 来指定一个消费停止位置.</p>
<p>内置的停止消费位置如下:</p>
<ol>
<li>永不停止.<br>StopCursor.never();</li>
<li>停止于 Pulsar 启动时 Topic 里面最新的那条数据.<br>StopCursor.latest();</li>
<li>停止于某条消息,结果里不包含此消息.<br>StopCursor.atMessageId(MessageId);</li>
<li>停止于某条消息之后,结果里包含此消息.<br>StopCursor.afterMessageId(MessageId);</li>
<li>停止于某个给定的消息事件时间戳,比如 Message&lt;byte[]&gt;.getEventTime(),消费结果里不包含此时间戳的消息.<br>StopCursor.atEventTime(long);</li>
<li>停止于某个给定的消息事件时间戳,比如 Message&lt;byte[]&gt;.getEventTime(),消费结果里包含此时间戳的消息.<br>StopCursor.afterEventTime(long);</li>
<li>停止于某个给定的消息发布时间戳,比如 Message&lt;byte[]&gt;.getPublishTime(),消费结果里不包含此时间戳的消息.<br>StopCursor.atPublishTime(long);</li>
<li>停止于某个给定的消息发布时间戳,比如 Message&lt;byte[]&gt;.getPublishTime(),消费结果里包含此时间戳的消息.<br>StopCursor.afterPublishTime(long);</li>
</ol>
<h4 id="Source-配置项"><a href="#Source-配置项" class="headerlink" title="Source 配置项"></a>Source 配置项</h4><p>除了前面提到的配置选项,Pulsar Source 还提供了丰富的选项供 Pulsar 专家使用,在 builder 类里通过 setConfig(ConfigOption<T>, T) 和 setConfig(Configuration) 方法给定下述的全部配置.</p>
<h5 id="Pulsar-Java-客户端配置项"><a href="#Pulsar-Java-客户端配置项" class="headerlink" title="Pulsar Java 客户端配置项"></a>Pulsar Java 客户端配置项</h5><h5 id="Pulsar-消费者-API-配置项"><a href="#Pulsar-消费者-API-配置项" class="headerlink" title="Pulsar 消费者 API 配置项"></a>Pulsar 消费者 API 配置项</h5><p>Pulsar 提供了消费者 API 和读者 API 两套 API 来进行数据消费,它们可用于不同的业务场景.<br>Flink 上的 Pulsar Source 使用消费者 API 进行消费,它的配置定义于 Pulsar 的 ConsumerConfigurationData 内.<br>Pulsar Source 将其中大部分的可供用户定义的配置定义于 PulsarSourceOptions 内.</p>
<h4 id="Pulsar-Source配置项"><a href="#Pulsar-Source配置项" class="headerlink" title="Pulsar Source配置项"></a>Pulsar Source配置项</h4><p>下述配置主要用于性能调优或者是控制消息确认的行为.<br>如非必要,可以不用强制配置.</p>
<h4 id="动态分区发现"><a href="#动态分区发现" class="headerlink" title="动态分区发现"></a>动态分区发现</h4><p>为了能在启动 Flink 任务之后还能发现在 Pulsar 上扩容的分区或者是新创建的 Topic,Pulsar Source 提供了动态分区发现机制.<br>该机制不需要重启 Flink 任务.<br>对选项 PulsarSourceOptions.PULSAR_PARTITION_DISCOVERY_INTERVAL_MS 设置一个正整数即可启用.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 10 秒查询一次分区信息</span></span><br><span class="line">PulsarSource.builder()</span><br><span class="line">        .setConfig(PulsarSourceOptions.PULSAR_PARTITION_DISCOVERY_INTERVAL_MS, <span class="number">10000</span>);</span><br></pre></td></tr></table></figure>

<p>默认情况下,Pulsar 启用动态分区发现,查询间隔为 30 秒.<br>用户可以给定一个负数,将该功能禁用.<br>如果使用批的方式消费数据,将无法启用该功能.</p>
<h4 id="事件时间和水位线"><a href="#事件时间和水位线" class="headerlink" title="事件时间和水位线"></a>事件时间和水位线</h4><p>默认情况下,Pulsar Source 使用 Pulsar 的 Message&lt;byte[]&gt; 里面的时间作为解析结果的时间戳.<br>用户可以使用 WatermarkStrategy 来自行解析出想要的消息时间,并向下游传递对应的水位线.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromSource(pulsarSource, <span class="keyword">new</span> CustomWatermarkStrategy(), <span class="string">&quot;Pulsar Source With Custom Watermark Strategy&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="消息确认"><a href="#消息确认" class="headerlink" title="消息确认"></a>消息确认</h4><p>一旦在 Topic 上创建了订阅,消息便会存储在 Pulsar 里.<br>即使没有消费者,消息也不会被丢弃.<br>只有当 Pulsar Source 同 Pulsar 确认此条消息已经被消费,该消息才以某种机制会被移除.<br>Pulsar Source 支持四种订阅方式,它们的消息确认方式也大不相同.</p>
<h5 id="独占和灾备订阅下的消息确认"><a href="#独占和灾备订阅下的消息确认" class="headerlink" title="独占和灾备订阅下的消息确认"></a>独占和灾备订阅下的消息确认</h5><p>独占 和 灾备 订阅下,Pulsar Source 使用累进式确认方式.<br>确认某条消息已经被处理时,其前面消息会自动被置为已读.<br>Pulsar Source 会在 Flink 完成检查点时将对应时刻消费的消息置为已读,以此来保证 Pulsar 状态与 Flink 状态一致.</p>
<p>如果用户没有在 Flink 上启用检查点,Pulsar Source 可以使用周期性提交来将消费状态提交给 Pulsar,使用配置 PulsarSourceOptions.PULSAR_AUTO_COMMIT_CURSOR_INTERVAL 来进行定义.</p>
<p>需要注意的是,此种场景下,Pulsar Source 并不依赖于提交到 Pulsar 的状态来做容错.<br>消息确认只是为了能在 Pulsar 端看到对应的消费处理情况.</p>
<h5 id="共享和-key-共享订阅下的消息确认"><a href="#共享和-key-共享订阅下的消息确认" class="headerlink" title="共享和 key 共享订阅下的消息确认"></a>共享和 key 共享订阅下的消息确认</h5><p>共享 和 key 共享 需要依次确认每一条消息,所以 Pulsar Source 在 Pulsar 事务里面进行消息确认,然后将事务提交到 Pulsar.</p>
<p>首先需要在 Pulsar 的 borker.conf 文件里面启用事务:<br>transactionCoordinatorEnabled=true</p>
<p>Pulsar Source 创建的事务的默认超时时间为 3 小时,请确保这个时间大于 Flink 检查点的间隔.<br>用户可以使用 PulsarSourceOptions.PULSAR_TRANSACTION_TIMEOUT_MILLIS 来设置事务的超时时间.</p>
<p>如果用户无法启用 Pulsar 的事务,或者是因为项目禁用了检查点,需要将 PulsarSourceOptions.PULSAR_ENABLE_AUTO_ACKNOWLEDGE_MESSAGE 选项设置为 true,消息从 Pulsar 消费后会被立刻置为已读.<br>Pulsar Source 无法保证此种场景下的消息一致性.</p>
<p>Pulsar Source 在 Pulsar 上使用日志的形式记录某个事务下的消息确认,为了更好的性能,请缩短 Flink 做检查点的间隔.</p>
<h3 id="Pulsar-Sink"><a href="#Pulsar-Sink" class="headerlink" title="Pulsar Sink"></a>Pulsar Sink</h3><p>Pulsar Sink 连接器可以将经过 Flink 处理后的数据写入一个或多个 Pulsar Topic 或者 Topic 下的某些分区.<br>Pulsar Sink 基于 Flink 最新的 Sink API 实现.<br>如果想要使用旧版的使用 SinkFuntion 接口实现的 Sink 连接器,可以使用 StreamNative 维护的 pulsar-flink.</p>
<h4 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h4><p>Pulsar Sink 使用 builder 类来创建 PulsarSink 实例.<br>下面示例展示了如何通过 Pulsar Sink 以&quot;至少一次&quot;的语义将字符串类型的数据发送给 topic1.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = ...</span><br><span class="line"></span><br><span class="line">PulsarSink&lt;String&gt; sink = PulsarSink.builder()</span><br><span class="line">    .setServiceUrl(serviceUrl)</span><br><span class="line">    .setAdminUrl(adminUrl)</span><br><span class="line">    .setTopics(<span class="string">&quot;topic1&quot;</span>)</span><br><span class="line">    .setSerializationSchema(PulsarSerializationSchema.flinkSchema(<span class="keyword">new</span> SimpleStringSchema()))</span><br><span class="line">    .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">stream.sinkTo(sink);</span><br></pre></td></tr></table></figure>

<p>下列为创建一个 PulsarSink 实例必需的属性:</p>
<ol>
<li>Pulsar 数据消费的地址,使用 setServiceUrl(String) 方法提供.</li>
<li>Pulsar HTTP 管理地址,使用 setAdminUrl(String) 方法提供.</li>
<li>需要发送到的 Topic 或者是 Topic 下面的分区,详见指定写入的topic或者topic分区.</li>
<li>编码 Pulsar 消息的序列化器,详见序列化器.</li>
</ol>
<p>在创建 PulsarSink 时,建议使用 setProducerName(String) 来指定 PulsarSink 内部使用的 Pulsar 生产者名称.<br>这样方便在数据监控页面找到对应的生产者监控指标.</p>
<h4 id="指定写入的-Topic-或者-Topic-分区"><a href="#指定写入的-Topic-或者-Topic-分区" class="headerlink" title="指定写入的 Topic 或者 Topic 分区"></a>指定写入的 Topic 或者 Topic 分区</h4><p>PulsarSink 指定写入 Topic 的方式和 Pulsar Source 指定消费的 Topic 或者 Topic 分区的方式类似.<br>PulsarSink 支持以 mixin 风格指定写入的 Topic 或分区.<br>因此,可以指定一组 Topic 或者分区或者是两者都有.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Topic &quot;some-topic1&quot; 和 &quot;some-topic2&quot;</span></span><br><span class="line">PulsarSink.builder().setTopics(<span class="string">&quot;some-topic1&quot;</span>, <span class="string">&quot;some-topic2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Topic &quot;topic-a&quot; 的分区 0 和 2</span></span><br><span class="line">PulsarSink.builder().setTopics(<span class="string">&quot;topic-a-partition-0&quot;</span>, <span class="string">&quot;topic-a-partition-2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Topic &quot;topic-a&quot; 以及 Topic &quot;some-topic2&quot; 分区 0 和 2</span></span><br><span class="line">PulsarSink.builder().setTopics(<span class="string">&quot;topic-a-partition-0&quot;</span>, <span class="string">&quot;topic-a-partition-2&quot;</span>, <span class="string">&quot;some-topic2&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>动态分区发现默认处于开启状态,这意味着 PulsarSink 将会周期性地从 Pulsar 集群中查询 Topic 的元数据来获取可能有的分区数量变更信息.<br>使用 PulsarSinkOptions.PULSAR_TOPIC_METADATA_REFRESH_INTERVAL 配置项来指定查询的间隔时间.</p>
<p>可以选择实现 TopicRouter 接口来自定义消息路由策略.<br>此外,阅读 Topic 名称简写将有助于理解 Pulsar 的分区在 Pulsar 连接器中的配置方式.</p>
<p>如果在 PulsarSink 中同时指定了某个 Topic 和其下属的分区,那么 PulsarSink 将会自动将两者合并,仅使用外层的 Topic.</p>
<p>举个例子,如果通过 PulsarSink.builder().setTopics(&quot;some-topic1&quot;, &quot;some-topic1-partition-0&quot;) 来指定写入的 Topic,那么其结果等价于 PulsarSink.builder().setTopics(&quot;some-topic1&quot;).</p>
<h4 id="序列化器-1"><a href="#序列化器-1" class="headerlink" title="序列化器"></a>序列化器</h4><p>序列化器(PulsarSerializationSchema)负责将 Flink 中的每条记录序列化成 byte 数组,并通过网络发送至指定的写入 Topic.<br>和 Pulsar Source 类似的是,序列化器同时支持使用基于 Flink 的 SerializationSchema 接口实现序列化器和使用 Pulsar 原生的 Schema 类型实现的序列化器.<br>不过序列化器并不支持 Pulsar 的 Schema.AUTO_PRODUCE_BYTES().</p>
<p>如果不需要指定 Message 接口中提供的 key 或者其他的消息属性,可以从上述 2 种预定义的 PulsarSerializationSchema 实现中选择适合需求的一种使用.</p>
<ol>
<li><p>使用 Pulsar 的 Schema 来序列化 Flink 中的数据.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 原始数据类型</span></span><br><span class="line">PulsarSerializationSchema.pulsarSchema(Schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 有结构数据类型(JSON/Protobuf/Avro 等)</span></span><br><span class="line">PulsarSerializationSchema.pulsarSchema(Schema, Class)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 键值对类型</span></span><br><span class="line">PulsarSerializationSchema.pulsarSchema(Schema, Class, Class)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 Flink 的 SerializationSchema 来序列化数据.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PulsarSerializationSchema.flinkSchema(SerializationSchema)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>同时使用 PulsarSerializationSchema.pulsarSchema() 以及在 builder 中指定 PulsarSinkBuilder.enableSchemaEvolution() 可以启用 Schema evolution 特性.<br>该特性会使用 Pulsar Broker 端提供的 Schema 版本兼容性检测以及 Schema 版本演进.<br>下列示例展示了如何启用 Schema Evolution.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Schema&lt;SomePojo&gt; schema = Schema.AVRO(SomePojo.class);</span><br><span class="line">PulsarSerializationSchema&lt;SomePojo&gt; pulsarSchema = PulsarSerializationSchema.pulsarSchema(schema, SomePojo.class);</span><br><span class="line"></span><br><span class="line">PulsarSink&lt;String&gt; sink = PulsarSink.builder()</span><br><span class="line">    ...</span><br><span class="line">    .setSerializationSchema(pulsarSchema)</span><br><span class="line">    .enableSchemaEvolution()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>

<p>如果想要使用 Pulsar 原生的 Schema 序列化消息而不需要 Schema Evolution 特性,那么写入的 Topic 会使用 Schema.BYTES 作为消息的 Schema,对应 Topic 的消费者需要自己负责反序列化的工作.</p>
<p>例如,如果使用 PulsarSerializationSchema.pulsarSchema(Schema.STRING) 而不使用 PulsarSinkBuilder.enableSchemaEvolution().<br>那么在写入 Topic 中所记录的消息 Schema 将会是 Schema.BYTES.</p>
<h4 id="消息路由策略"><a href="#消息路由策略" class="headerlink" title="消息路由策略"></a>消息路由策略</h4><p>在 Pulsar Sink 中,消息路由发生在于分区之间,而非上层 Topic.<br>对于给定 Topic 的情况,路由算法会首先会查询出 Topic 之上所有的分区信息,并在这些分区上实现消息的路由.<br>Pulsar Sink 默认提供 2 种路由策略的实现.</p>
<ol>
<li><p>KeyHashTopicRouter:使用消息的 key 对应的哈希值来取模计算出消息对应的 Topic 分区.<br>使用此路由可以将具有相同 key 的消息发送至同一个 Topic 分区.<br>消息的 key 可以在自定义 PulsarSerializationSchema 时,在 serialize() 方法内使用 PulsarMessageBuilder.key(String key) 来予以指定.<br>如果消息没有包含 key,此路由策略将从 Topic 分区中随机选择一个发送.<br>可以使用 MessageKeyHash.JAVA_HASH 或者 MessageKeyHash.MURMUR3_32_HASH 两种不同的哈希算法来计算消息 key 的哈希值.<br>使用 PulsarSinkOptions.PULSAR_MESSAGE_KEY_HASH 配置项来指定想要的哈希算法.</p>
</li>
<li><p>RoundRobinRouter:轮换使用用户给定的 Topic 分区.<br>消息将会轮替地选取 Topic 分区,当往某个 Topic 分区里写入指定数量的消息后,将会轮换至下一个 Topic 分区.<br>使用 PulsarSinkOptions.PULSAR_BATCHING_MAX_MESSAGES 指定向一个 Topic 分区中写入的消息数量.</p>
</li>
</ol>
<p>还可以通过实现 TopicRouter 接口来自定义消息路由策略,请注意 TopicRouter 的实现需要能被序列化.</p>
<p>在 TopicRouter 内可以指定任意的 Topic 分区(即使这个 Topic 分区不在 setTopics() 指定的列表中).<br>因此,当使用自定义的 TopicRouter 时,PulsarSinkBuilder.setTopics 选项是可选的.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TopicRouter</span>&lt;<span class="title">IN</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">route</span><span class="params">(IN in, List&lt;String&gt; partitions, PulsarSinkContext context)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(SinkConfiguration sinkConfiguration)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 默认无操作</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如前文所述,Pulsar 分区的内部被实现为一个无分区的 Topic,一般情况下 Pulsar 客户端会隐藏这个实现,并且提供内置的消息路由策略.<br>Pulsar Sink 并没有使用 Pulsar 客户端提供的路由策略和封装,而是使用了 Pulsar 客户端更底层的 API 自行实现了消息路由逻辑.<br>这样做的主要目的是能够在属于不同 Topic 的分区之间定义更灵活的消息路由策略.</p>
<h4 id="发送一致性"><a href="#发送一致性" class="headerlink" title="发送一致性"></a>发送一致性</h4><p>PulsarSink 支持三种发送一致性.</p>
<ol>
<li><p>NONE:Flink 应用运行时可能出现数据丢失的情况.<br>在这种模式下,Pulsar Sink 发送消息后并不会检查消息是否发送成功.<br>此模式具有最高的吞吐量,可用于一致性没有要求的场景.</p>
</li>
<li><p>AT_LEAST_ONCE:每条消息至少有一条对应消息发送至 Pulsar,发送至 Pulsar 的消息可能会因为 Flink 应用重启而出现重复.</p>
</li>
<li><p>EXACTLY_ONCE:每条消息有且仅有一条对应消息发送至 Pulsar.<br>发送至 Pulsar 的消息不会有重复也不会丢失.<br>Pulsar Sink 内部依赖 Pulsar 事务和两阶段提交协议来保证每条记录都能正确发往 Pulsar.</p>
</li>
</ol>
<h4 id="消息延时发送"><a href="#消息延时发送" class="headerlink" title="消息延时发送"></a>消息延时发送</h4><p>消息延时发送特性可以让指定发送的每一条消息需要延时一段时间后才能被下游的消费者所消费.<br>当延时消息发送特性启用时,Pulsar Sink 会立刻将消息发送至 Pulsar Broker.<br>但该消息在指定的延迟时间到达前将会保持对下游消费者不可见.</p>
<p>消息延时发送仅在 Shared 订阅模式下有效,在 Exclusive 和 Failover 模式下该特性无效.</p>
<p>可以使用 MessageDelayer.fixed(Duration) 创建一个 MessageDelayer 来为所有消息指定恒定的接收时延,或者实现 MessageDelayer 接口来为不同的消息指定不同的接收时延.</p>
<p>消息对下游消费者的可见时间应当基于 PulsarSinkContext.processTime() 计算得到.</p>
<h4 id="Sink-配置项"><a href="#Sink-配置项" class="headerlink" title="Sink 配置项"></a>Sink 配置项</h4><p>可以在 builder 类里通过 setConfig(ConfigOption<T>, T) 和 setConfig(Configuration) 方法给定下述的全部配置.</p>
<p>PulsarClient 和 PulsarAdmin 配置项<br>Pulsar Sink 和 Pulsar Source 公用的配置选项可参考<br>Pulsar Java 客户端配置项<br>Pulsar 管理 API 配置项</p>
<h5 id="Pulsar-生产者-API-配置项"><a href="#Pulsar-生产者-API-配置项" class="headerlink" title="Pulsar 生产者 API 配置项"></a>Pulsar 生产者 API 配置项</h5><p>Pulsar Sink 使用生产者 API 来发送消息.<br>Pulsar 的 ProducerConfigurationData 中大部分的配置项被映射为 PulsarSinkOptions 里的选项.</p>
<h5 id="Pulsar-Sink-配置项"><a href="#Pulsar-Sink-配置项" class="headerlink" title="Pulsar Sink 配置项"></a>Pulsar Sink 配置项</h5><p>下述配置主要用于性能调优或者是控制消息确认的行为.<br>如非必要,可以不用考虑配置.</p>
<h4 id="Sink-监控指标"><a href="#Sink-监控指标" class="headerlink" title="Sink 监控指标"></a>Sink 监控指标</h4><p>下列表格列出了当前 Sink 支持的监控指标,前 6 个指标是 FLIP-33: Standardize Connector Metrics 中规定的 Sink 连接器应当支持的标准指标.</p>
<p>指标 numBytesOut/numRecordsOut 和 numRecordsOutErrors 从 Pulsar Producer 实例的监控指标中获得.</p>
<p>currentSendTime 记录了最近一条消息从放入生产者的缓冲队列到消息被消费确认所耗费的时间.<br>这项指标在 NONE 发送一致性下不可用.</p>
<p>默认情况下,Pulsar 生产者每隔 60 秒才会刷新一次监控数据,然而 Pulsar Sink 每 500 毫秒就会从 Pulsar 生产者中获得最新的监控数据.<br>因此 numRecordsOut/numBytesOut/numAcksReceived 以及 numRecordsOutErrors 4 个指标实际上每 60 秒才会刷新一次.</p>
<p>如果想要更高地刷新评率,可以通过如下方式来将 Pulsar 生产者的监控数据刷新频率调整至相应值(最低为1s):<br>builder.setConfig(PulsarOptions.PULSAR_STATS_INTERVAL_SECONDS, 1L);</p>
<p>numBytesOutRate 和 numRecordsOutRate 指标是 Flink 内部通过 numBytesOut 和 numRecordsOut 计数器,在一个 60 秒的窗口内计算得到的.</p>
<h4 id="设计思想简述"><a href="#设计思想简述" class="headerlink" title="设计思想简述"></a>设计思想简述</h4><p>Pulsar Sink 遵循 FLIP-191 中定义的 Sink API 设计.</p>
<h5 id="无状态的-SinkWriter"><a href="#无状态的-SinkWriter" class="headerlink" title="无状态的 SinkWriter"></a>无状态的 SinkWriter</h5><p>在 EXACTLY_ONCE 一致性下,Pulsar Sink 不会将事务相关的信息存放于检查点快照中.<br>这意味着当 Flink 应用重启时,Pulsar Sink 会创建新的事务实例.<br>上一次运行过程中任何未提交事务中的消息会因为超时中止而无法被下游的消费者所消费.<br>这样的设计保证了 SinkWriter 是无状态的.</p>
<h5 id="Pulsar-Schema-Evolution"><a href="#Pulsar-Schema-Evolution" class="headerlink" title="Pulsar Schema Evolution"></a>Pulsar Schema Evolution</h5><p>Pulsar Schema Evolution 允许用户在一个 Flink 应用程序中使用的数据模型发生特定改变后(比如向基于 ARVO 的 POJO 类中增加或删除一个字段),仍能使用同一个 Flink 应用程序的代码.</p>
<p>可以在 Pulsar 集群内指定哪些类型的数据模型的改变是被允许的,详情请参阅 Pulsar Schema Evolution.</p>
<h3 id="升级至最新的连接器"><a href="#升级至最新的连接器" class="headerlink" title="升级至最新的连接器"></a>升级至最新的连接器</h3><p>常见的升级步骤,请参阅升级应用程序和 Flink 版本.<br>Pulsar 连接器没有在 Flink 端存储消费的状态,所有的消费信息都推送到了 Pulsar.<br>所以需要注意下面的事项:<br>不要同时升级 Pulsar 连接器和 Pulsar 服务端的版本.<br>使用最新版本的 Pulsar 客户端来消费消息.</p>
<h3 id="问题诊断"><a href="#问题诊断" class="headerlink" title="问题诊断"></a>问题诊断</h3><p>使用 Flink 和 Pulsar 交互时如果遇到问题,由于 Flink 内部实现只是基于 Pulsar 的 Java 客户端和管理 API 而开发的.</p>
<p>用户遇到的问题可能与 Flink 无关,请先升级 Pulsar 的版本/Pulsar 客户端的版本,或者修改 Pulsar 的配置/Pulsar 连接器的配置来尝试解决问题.</p>
<h3 id="已知问题"><a href="#已知问题" class="headerlink" title="已知问题"></a>已知问题</h3><p>本节介绍有关 Pulsar 连接器的一些已知问题.</p>
<h4 id="在-Java-11-上使用不稳定"><a href="#在-Java-11-上使用不稳定" class="headerlink" title="在 Java 11 上使用不稳定"></a>在 Java 11 上使用不稳定</h4><p>Pulsar connector 在 Java 11 中有一些尚未修复的问题.<br>我们当前推荐在 Java 8 环境中运行Pulsar connector.</p>
<h4 id="不自动重连-而是抛出TransactionCoordinatorNotFound异常"><a href="#不自动重连-而是抛出TransactionCoordinatorNotFound异常" class="headerlink" title="不自动重连,而是抛出TransactionCoordinatorNotFound异常"></a>不自动重连,而是抛出TransactionCoordinatorNotFound异常</h4><p>Pulsar 事务机制仍在积极发展中,当前版本并不稳定.<br>Pulsar 2.9.2 引入了这个问题 a break change.<br>如果您使用 Pulsar 2.9.2或更高版本与较旧的 Pulsar 客户端一起使用,您可能会收到一个&quot;TransactionCoordinatorNotFound&quot;异常.</p>
<p>您可以使用最新的pulsar-client-all分支来解决这个问题.</p>
<h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>该连接器可以向 JDBC 数据库写入数据.</p>
<p>添加下面的依赖以便使用该连接器(同时添加 JDBC 驱动):</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>已创建的 JDBC Sink 能够保证至少一次的语义.<br>更有效的精确执行一次可以通过 upsert 语句或幂等更新实现.</p>
<p>用法示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env</span><br><span class="line">        .fromElements(...)</span><br><span class="line">        .addSink(JdbcSink.sink(</span><br><span class="line">                <span class="string">&quot;insert into books (id, title, author, price, qty) values (?,?,?,?,?)&quot;</span>,</span><br><span class="line">                (ps, t) -&gt; &#123;</span><br><span class="line">                    ps.setInt(<span class="number">1</span>, t.id);</span><br><span class="line">                    ps.setString(<span class="number">2</span>, t.title);</span><br><span class="line">                    ps.setString(<span class="number">3</span>, t.author);</span><br><span class="line">                    ps.setDouble(<span class="number">4</span>, t.price);</span><br><span class="line">                    ps.setInt(<span class="number">5</span>, t.qty);</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="keyword">new</span> JdbcConnectionOptions.JdbcConnectionOptionsBuilder()</span><br><span class="line">                        .withUrl(getDbMetadata().getUrl())</span><br><span class="line">                        .withDriverName(getDbMetadata().getDriverClass())</span><br><span class="line">                        .build()));</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p>这个连接器提供了一个可以写入Redis的 Sink,也可以将数据发布到Redis PubSub.<br>要使用此连接器,请将以下依赖项添加到您的项目中.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>提供向 Redis 发送数据的接口的类.<br>sink 可以使用三种不同的方法与不同类型的 Redis 环境进行通信:<br>Single Redis Server<br>Redis Cluster<br>Redis Sentinel</p>
<h3 id="Single-Redis-Server"><a href="#Single-Redis-Server" class="headerlink" title="Single Redis Server"></a>Single Redis Server</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisExampleMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.HSET, <span class="string">&quot;HASH_NAME&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.f0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder().setHost(<span class="string">&quot;127.0.0.1&quot;</span>).build();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; stream = ...;</span><br><span class="line">stream.addSink(<span class="keyword">new</span> RedisSink&lt;Tuple2&lt;String, String&gt;&gt;(conf, <span class="keyword">new</span> RedisExampleMapper()));</span><br></pre></td></tr></table></figure>

<h3 id="Redis-Cluster"><a href="#Redis-Cluster" class="headerlink" title="Redis Cluster"></a>Redis Cluster</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisClusterConfig.Builder()</span><br><span class="line">    .setNodes(<span class="keyword">new</span> HashSet&lt;InetSocketAddress&gt;(Arrays.asList(<span class="keyword">new</span> InetSocketAddress(<span class="number">5601</span>)))).build();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; stream = ...;</span><br><span class="line">stream.addSink(<span class="keyword">new</span> RedisSink&lt;Tuple2&lt;String, String&gt;&gt;(conf, <span class="keyword">new</span> RedisExampleMapper()));</span><br></pre></td></tr></table></figure>

<h3 id="Redis-Sentinel"><a href="#Redis-Sentinel" class="headerlink" title="Redis Sentinel"></a>Redis Sentinel</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisSentinelConfig conf = <span class="keyword">new</span> FlinkJedisSentinelConfig.Builder()</span><br><span class="line">    .setMasterName(<span class="string">&quot;master&quot;</span>).setSentinels(...).build();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; stream = ...;</span><br><span class="line">stream.addSink(<span class="keyword">new</span> RedisSink&lt;Tuple2&lt;String, String&gt;&gt;(conf, <span class="keyword">new</span> RedisExampleMapper()));</span><br></pre></td></tr></table></figure>

<h3 id="data-types"><a href="#data-types" class="headerlink" title="data types"></a>data types</h3><img src="/images/flgl122.png" width="300" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="ActiveMQ"><a href="#ActiveMQ" class="headerlink" title="ActiveMQ"></a>ActiveMQ</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-activemq_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>The source class is called AMQSource, and the sink is AMQSink.</p>
<h1 id="Table-amp-SQL"><a href="#Table-amp-SQL" class="headerlink" title="Table &amp; SQL"></a>Table &amp; SQL</h1><h2 id="概览-1"><a href="#概览-1" class="headerlink" title="概览"></a>概览</h2><p>Flink 的 Table API &amp; SQL 程序可以连接到其他外部系统,用于读取和写入批处理和流式表.<br>表源提供对存储在外部系统(例如数据库/键值存储/消息队列或文件系统)中的数据的访问.<br>表接收器将表发送到外部存储系统.<br>根据源和接收器的类型,它们支持不同的格式,例如 CSV/Avro/Parquet 或 ORC.</p>
<p>本页介绍如何使用原生支持的连接器在 Flink 中注册表源和表接收器.<br>注册源或接收器后,可以通过 Table API 和 SQL 语句访问它.</p>
<h3 id="如何使用连接器"><a href="#如何使用连接器" class="headerlink" title="如何使用连接器"></a>如何使用连接器</h3><p>Flink 支持使用 SQLCREATE TABLE语句注册表.<br>可以定义表名/表模式和用于连接到外部系统的表选项.</p>
<p>以下代码显示了如何连接到 Kafka 以读取和写入 JSON 记录的完整示例.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyUserTable (</span><br><span class="line">  <span class="comment">-- declare the schema of the table</span></span><br><span class="line">  `<span class="keyword">user</span>` <span class="type">BIGINT</span>,</span><br><span class="line">  `message` STRING,</span><br><span class="line">  `rowtime` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,    <span class="comment">-- use a metadata column to access Kafka&#x27;s record timestamp</span></span><br><span class="line">  `proctime` <span class="keyword">AS</span> PROCTIME(),    <span class="comment">-- use a computed column to define a proctime attribute</span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> `rowtime` <span class="keyword">AS</span> `rowtime` <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>    <span class="comment">-- use a WATERMARK statement to define a rowtime attribute</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="comment">-- declare the external system to connect to</span></span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;topic_name&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>   <span class="comment">-- declare a format for this system</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>所需的连接属性将转换为基于字符串的键值对.<br>工厂kafka将根据工厂标识符(json在本例中) 从键值对创建配置的表源/表接收器和相应的格式.<br>在为每个组件搜索恰好一个匹配的工厂时,会考虑通过 Java 的服务提供者接口 (SPI)找到的所有工厂.</p>
<p>如果找不到工厂或多个工厂与给定属性匹配,则会引发异常,并提供有关考虑的工厂和支持的属性的附加信息.</p>
<h3 id="转换表连接器-格式资源"><a href="#转换表连接器-格式资源" class="headerlink" title="转换表连接器/格式资源"></a>转换表连接器/格式资源</h3><p>Flink 使用 Java 的服务提供者接口 (SPI)通过标识符加载表连接器/格式工厂.<br>由于每个 table connector/format 命名的 SPI 资源文件org.apache.flink.table.factories.Factory都在同一个目录下META-INF/services,在构建使用多个 table connector/format 的项目的 uber-jar 时,这些资源文件会相互覆盖,从而导致 Flink 失败加载表连接器/格式工厂.</p>
<p>META-INF/services在这种情况下,推荐的方式是通过maven shade插件的ServicesResourceTransformer转换目录下的这些资源文件.<br>给定示例的 pom.xml 文件内容,其中包含项目中的连接器flink-sql-connector-hive-3.1.2和格式flink-parquet.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>myProject<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--  other project dependencies  ...--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-sql-connector-hive-3.1.2_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-parquet_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span> <span class="attr">combine.children</span>=<span class="string">&quot;append&quot;</span>&gt;</span></span><br><span class="line">                            <span class="comment">&lt;!-- The service transformer is needed to merge META-INF/services files --&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;</span>/&gt;</span></span><br><span class="line">                            <span class="comment">&lt;!-- ... --&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置完成后,在构建上述项目的 uber-jar 时,ServicesResourceTransformer目录下的表连接器/格式资源文件将被合并而不是相互覆盖.</p>
<h3 id="Schema-Mapping"><a href="#Schema-Mapping" class="headerlink" title="Schema Mapping"></a>Schema Mapping</h3><p>SQL 语句的主体子句CREATE TABLE定义物理列/约束和水印的名称和类型.<br>Flink 不保存数据,因此模式定义仅声明如何将物理列从外部系统映射到 Flink 的表示.<br>映射可能不是按名称映射的,它取决于格式和连接器的实现.<br>例如,MySQL 数据库表按字段名称映射(不区分大小写),CSV 文件系统按字段顺序映射(字段名称可以是任意的).<br>这将在每个连接器中进行解释.</p>
<p>以下示例显示了一个简单的模式,没有时间属性和输入/输出到表列的一对一字段映射.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  MyField1 <span class="type">INT</span>,</span><br><span class="line">  MyField2 STRING,</span><br><span class="line">  MyField3 <span class="type">BOOLEAN</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h4><p>一些连接器和格式公开了额外的元数据字段,可以在物理有效负载列旁边的元数据列中访问这些字段.<br>有关元数据列的更多信息,请参阅该CREATE TABLE部分 .</p>
<h4 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h4><p>主键约束告诉表的一列或一组列是唯一的并且它们不包含空值.<br>主键唯一标识表中的一行.</p>
<p>源表的主键是用于优化的元数据信息.<br>sink 表的主键通常由 sink 实现用于 upserting.</p>
<p>SQL 标准指定约束可以是 ENFORCED 或 NOT ENFORCED.<br>这控制是否对传入/传出数据执行约束检查.<br>Flink 不拥有数据,我们想要支持的唯一模式是 NOT ENFORCED 模式.<br>由用户来确保查询强制执行密钥完整性.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  MyField1 <span class="type">INT</span>,</span><br><span class="line">  MyField2 STRING,</span><br><span class="line">  MyField3 <span class="type">BOOLEAN</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (MyField1, MyField2) <span class="keyword">NOT</span> ENFORCED  <span class="comment">-- defines a primary key on columns</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="时间属性"><a href="#时间属性" class="headerlink" title="时间属性"></a>时间属性</h4><p>使用无界流表时,时间属性是必不可少的.<br>因此 proctime 和 rowtime 属性都可以定义为模式的一部分.</p>
<p>有关 Flink 中时间处理,尤其是事件时间的更多信息,我们推荐通用事件时间部分.</p>
<p>Proctime 属性<br>为了在模式中声明 proctime 属性,您可以使用计算列语法来声明从PROCTIME()内置函数生成的计算列.<br>计算列是不存储在物理数据中的虚拟列.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  MyField1 <span class="type">INT</span>,</span><br><span class="line">  MyField2 STRING,</span><br><span class="line">  MyField3 <span class="type">BOOLEAN</span>,</span><br><span class="line">  MyField4 <span class="keyword">AS</span> PROCTIME() <span class="comment">-- declares a proctime attribute</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h5 id="行时间属性"><a href="#行时间属性" class="headerlink" title="行时间属性"></a>行时间属性</h5><p>为了控制表的事件时间行为,Flink 提供了预定义的时间戳提取器和水印策略.</p>
<p>有关在 DDL 中定义时间属性的更多信息,请参阅CREATE TABLE 语句.</p>
<p>支持以下时间戳提取器:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- use the existing TIMESTAMP(3) field in schema as the rowtime attribute</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  ts_field <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_field <span class="keyword">AS</span> ...</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- use system functions or UDFs or expressions to extract the expected TIMESTAMP(3) rowtime field</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  log_ts STRING,</span><br><span class="line">  ts_field <span class="keyword">AS</span> TO_TIMESTAMP(log_ts),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_field <span class="keyword">AS</span> ...</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>支持以下水印策略:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Sets a watermark strategy for strictly ascending rowtime attributes. Emits a watermark of the</span></span><br><span class="line"><span class="comment">-- maximum observed timestamp so far. Rows that have a timestamp bigger to the max timestamp</span></span><br><span class="line"><span class="comment">-- are not late.</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  ts_field <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_field <span class="keyword">AS</span> ts_field</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum</span></span><br><span class="line"><span class="comment">-- observed timestamp so far minus 1. Rows that have a timestamp bigger or equal to the max timestamp</span></span><br><span class="line"><span class="comment">-- are not late.</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  ts_field <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_field <span class="keyword">AS</span> ts_field <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;0.001&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Sets a watermark strategy for rowtime attributes which are out-of-order by a bounded time interval.</span></span><br><span class="line"><span class="comment">-- Emits watermarks which are the maximum observed timestamp minus the specified delay, e.g. 2 seconds.</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  ts_field <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_field <span class="keyword">AS</span> ts_field <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>确保始终声明时间戳和水印.<br>触发基于时间的操作需要水印.</p>
<h3 id="SQL-类型"><a href="#SQL-类型" class="headerlink" title="SQL 类型"></a>SQL 类型</h3><p>请参阅数据类型页面了解如何在 SQL 中声明类型.</p>
<h2 id="Formats-1"><a href="#Formats-1" class="headerlink" title="Formats"></a>Formats</h2><p>Flink 提供了一套与表连接器(table connector)一起使用的表格式(table format).<br>表格式是一种存储格式,定义了如何把二进制数据映射到表的列上.</p>
<p>Flink 支持以下格式:<br><img src="/images/flgl123.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<h3 id="CSV-Format"><a href="#CSV-Format" class="headerlink" title="CSV Format"></a>CSV Format</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>CSV Format 允许我们基于 CSV schema 进行解析和生成 CSV 数据.<br>目前 CSV schema 是基于 table schema 推断而来的.</p>
<h4 id="依赖-1"><a href="#依赖-1" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何创建使用-CSV-格式的表"><a href="#如何创建使用-CSV-格式的表" class="headerlink" title="如何创建使用 CSV 格式的表"></a>如何创建使用 CSV 格式的表</h4><p>以下是一个使用 Kafka 连接器和 CSV 格式创建表的示例.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  category_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING,</span><br><span class="line">  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;csv.ignore-parse-errors&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;csv.allow-comments&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数"><a href="#Format-参数" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl124.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl125.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="数据类型映射"><a href="#数据类型映射" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前 CSV 的 schema 都是从 table schema 推断而来的.<br>显式地定义 CSV schema 暂不支持.<br>Flink 的 CSV Format 数据使用 jackson databind API 去解析 CSV 字符串.</p>
<p>下面的表格列出了flink数据和CSV数据的对应关系.</p>
<table>
<thead>
<tr>
<th align="left">Flink SQL 类型</th>
<th align="left">CSV 类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR / VARCHAR / STRING</td>
<td align="left">string</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">boolean</td>
</tr>
<tr>
<td align="left">BINARY / VARBINARY</td>
<td align="left">string with encoding: base64</td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">string with format: date</td>
</tr>
<tr>
<td align="left">TIME</td>
<td align="left">string with format: time</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">string with format: date-time</td>
</tr>
<tr>
<td align="left">INTERVAL</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">ARRAY</td>
<td align="left">array</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">object</td>
</tr>
</tbody></table>
<h3 id="JSON-Format"><a href="#JSON-Format" class="headerlink" title="JSON Format"></a>JSON Format</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>JSON Format 能读写 JSON 格式的数据.<br>当前,JSON schema 是从 table schema 中自动推导而得的.</p>
<h4 id="依赖-2"><a href="#依赖-2" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何创建一张基于-JSON-Format-的表"><a href="#如何创建一张基于-JSON-Format-的表" class="headerlink" title="如何创建一张基于 JSON Format 的表"></a>如何创建一张基于 JSON Format 的表</h4><p>以下是一个利用 Kafka 以及 JSON Format 构建表的例子.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  category_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING,</span><br><span class="line">  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;json.fail-on-missing-field&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;false&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;json.ignore-parse-errors&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-1"><a href="#Format-参数-1" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl126.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl127.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl128.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="数据类型映射关系"><a href="#数据类型映射关系" class="headerlink" title="数据类型映射关系"></a>数据类型映射关系</h4><p>当前,JSON schema 将会自动从 table schema 之中自动推导得到.<br>不支持显式地定义 JSON schema.</p>
<p>在 Flink 中,JSON Format 使用 jackson databind API 去解析和生成 JSON.<br>下表列出了 Flink 中的数据类型与 JSON 中的数据类型的映射关系.</p>
<table>
<thead>
<tr>
<th align="left">Flink SQL 类型</th>
<th align="left">JSON 类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR / VARCHAR / STRING</td>
<td align="left">string</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">boolean</td>
</tr>
<tr>
<td align="left">BINARY / VARBINARY</td>
<td align="left">string with encoding: base64</td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">string with format: date</td>
</tr>
<tr>
<td align="left">TIME</td>
<td align="left">string with format: time</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">string with format: date-time</td>
</tr>
<tr>
<td align="left">TIMESTAMP_WITH_LOCAL_TIME_ZONE</td>
<td align="left">string with format: date-time (with UTC time zone)</td>
</tr>
<tr>
<td align="left">INTERVAL</td>
<td align="left">number</td>
</tr>
<tr>
<td align="left">ARRAY</td>
<td align="left">array</td>
</tr>
<tr>
<td align="left">MAP / MULTISET</td>
<td align="left">object</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">object</td>
</tr>
</tbody></table>
<h3 id="Avro-Format-1"><a href="#Avro-Format-1" class="headerlink" title="Avro Format"></a>Avro Format</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Apache Avro format 允许基于 Avro schema 读取和写入 Avro 数据.<br>目前,Avro schema 从 table schema 推导而来.</p>
<h4 id="依赖-3"><a href="#依赖-3" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何使用-Avro-format-创建表"><a href="#如何使用-Avro-format-创建表" class="headerlink" title="如何使用 Avro format 创建表"></a>如何使用 Avro format 创建表</h4><p>这是使用 Kafka 连接器和 Avro format 创建表的示例.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  category_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING,</span><br><span class="line">  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-2"><a href="#Format-参数-2" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl129.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="数据类型映射-1"><a href="#数据类型映射-1" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前,Avro schema 通常是从 table schema 中推导而来.<br>尚不支持显式定义 Avro schema.<br>因此,下表列出了从 Flink 类型到 Avro 类型的类型映射.</p>
<table>
<thead>
<tr>
<th align="left">Flink SQL 类型</th>
<th align="left">Avro 类型</th>
<th align="left">Avro 逻辑类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR / VARCHAR / STRING</td>
<td align="left">string</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">boolean</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">BINARY / VARBINARY</td>
<td align="left">bytes</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">fixed</td>
<td align="left">decimal</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">int</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">int</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">int</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">long</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">float</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">double</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">int</td>
<td align="left">date</td>
</tr>
<tr>
<td align="left">TIME</td>
<td align="left">int</td>
<td align="left">time-millis</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">long</td>
<td align="left">timestamp-millis</td>
</tr>
<tr>
<td align="left">ARRAY</td>
<td align="left">array</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">MAP(key 必须是 string/char/varchar 类型)</td>
<td align="left">map</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">MULTISET(元素必须是 string/char/varchar 类型)</td>
<td align="left">map</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">record</td>
<td align="left"></td>
</tr>
</tbody></table>
<p>除了上面列出的类型,Flink 支持读取/写入 nullable 的类型.<br>Flink 将 nullable 的类型映射到 Avro union(something, null),其中 something 是从 Flink 类型转换的 Avro 类型.</p>
<h3 id="Confluent-Avro-Format"><a href="#Confluent-Avro-Format" class="headerlink" title="Confluent Avro Format"></a>Confluent Avro Format</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Avro Schema Registry (avro-confluent) 格式能让你读取被 io.confluent.kafka.serializers.KafkaAvroSerializer 序列化的记录,以及可以写入成能被 io.confluent.kafka.serializers.KafkaAvroDeserializer 反序列化的记录.</p>
<p>当以这种格式读取(反序列化)记录时,将根据记录中编码的 schema 版本 id 从配置的 Confluent Schema Registry 中获取 Avro writer schema ,而从 table schema 中推断出 reader schema.</p>
<p>当以这种格式写入(序列化)记录时,Avro schema 是从 table schema 中推断出来的,并会用来检索要与数据一起编码的 schema id.<br>我们会在配置的 Confluent Schema Registry 中配置的 subject 下,检索 schema id.<br>subject 通过 avro-confluent.subject 参数来制定.</p>
<p>Avro Schema Registry 格式只能与 Apache Kafka SQL 连接器或 Upsert Kafka SQL 连接器一起使用.</p>
<h4 id="依赖-4"><a href="#依赖-4" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro-confluent-registry<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>For Maven, SBT, Gradle, or other build automation tools, please also ensure that Confluent&#39;s maven repository at <code>https://packages.confluent.io/maven/</code> is configured in your project&#39;s build files.</p>
<h4 id="如何创建使用-Avro-Confluent-格式的表"><a href="#如何创建使用-Avro-Confluent-格式的表" class="headerlink" title="如何创建使用 Avro-Confluent 格式的表"></a>如何创建使用 Avro-Confluent 格式的表</h4><p>以下是一个使用 Kafka 连接器和 Confluent Avro 格式创建表的示例.<br>使用原始的 UTF-8 字符串作为 Kafka 的 key,Schema Registry 中注册的 Avro 记录作为 Kafka 的 values 的表的示例:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_created (</span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 该列映射到 Kafka 原始的 UTF-8 key</span></span><br><span class="line">  the_kafka_key STRING,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">-- 映射到 Kafka value 中的 Avro 字段的一些列</span></span><br><span class="line">  id STRING,</span><br><span class="line">  name STRING,</span><br><span class="line">  email STRING</span><br><span class="line"></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_events_example1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="comment">-- UTF-8 字符串作为 Kafka 的 keys,使用表中的 &#x27;the_kafka_key&#x27; 列</span></span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;raw&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.fields&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;the_kafka_key&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro-confluent&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.avro-confluent.url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;http://localhost:8082&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.fields-include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;EXCEPT_KEY&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们可以像下面这样将数据写入到 kafka 表中:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> user_created</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 将 user id 复制至映射到 kafka key 的列中</span></span><br><span class="line">  id <span class="keyword">as</span> the_kafka_key,</span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 所有的 values</span></span><br><span class="line">  id, name, email</span><br><span class="line"><span class="keyword">FROM</span> some_table</span><br></pre></td></tr></table></figure>

<p>Kafka 的 key 和 value 在 Schema Registry 中都注册为 Avro 记录的表的示例:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_created (</span><br><span class="line">  <span class="comment">-- 该列映射到 Kafka key 中的 Avro 字段 &#x27;id&#x27;</span></span><br><span class="line">  kafka_key_id STRING,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">-- 映射到 Kafka value 中的 Avro 字段的一些列</span></span><br><span class="line">  id STRING,</span><br><span class="line">  name STRING, </span><br><span class="line">  email STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_events_example2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 注意:由于哈希分区,在 Kafka key 的上下文中,schema 升级几乎从不向后也不向前兼容.</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro-confluent&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.avro-confluent.url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;http://localhost:8082&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.fields&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka_key_id&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 在本例中,我们希望 Kafka 的 key 和 value 的 Avro 类型都包含 &#x27;id&#x27; 字段</span></span><br><span class="line">  <span class="comment">-- =&gt; 给表中与 Kafka key 字段关联的列添加一个前缀来避免冲突</span></span><br><span class="line">  <span class="string">&#x27;key.fields-prefix&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka_key_&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro-confluent&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.avro-confluent.url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;http://localhost:8082&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.fields-include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;EXCEPT_KEY&#x27;</span>,</span><br><span class="line">   </span><br><span class="line">  <span class="comment">-- 自 Flink 1.13 起,subjects 具有一个默认值, 但是可以被覆盖:</span></span><br><span class="line">  <span class="string">&#x27;key.avro-confluent.subject&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_events_example2-key2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.avro-confluent.subject&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_events_example2-value2&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>使用 upsert-kafka 连接器,Kafka 的 value 在 Schema Registry 中注册为 Avro 记录的表的示例:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_created (</span><br><span class="line">  </span><br><span class="line">  <span class="comment">-- 该列映射到 Kafka 原始的 UTF-8 key</span></span><br><span class="line">  kafka_key_id STRING,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">-- 映射到 Kafka value 中的 Avro 字段的一些列</span></span><br><span class="line">  id STRING, </span><br><span class="line">  name STRING, </span><br><span class="line">  email STRING, </span><br><span class="line">  </span><br><span class="line">  <span class="comment">-- upsert-kafka 连接器需要一个主键来定义 upsert 行为</span></span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (kafka_key_id) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line"></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;upsert-kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_events_example3&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="comment">-- UTF-8 字符串作为 Kafka 的 keys</span></span><br><span class="line">  <span class="comment">-- 在本例中我们不指定 &#x27;key.fields&#x27;,因为它由表的主键决定</span></span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;raw&#x27;</span>,</span><br><span class="line">  </span><br><span class="line">  <span class="comment">-- 在本例中,我们希望 Kafka 的 key 和 value 的 Avro 类型都包含 &#x27;id&#x27; 字段</span></span><br><span class="line">  <span class="comment">-- =&gt; 给表中与 Kafka key 字段关联的列添加一个前缀来避免冲突</span></span><br><span class="line">  <span class="string">&#x27;key.fields-prefix&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka_key_&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro-confluent&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.avro-confluent.url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;http://localhost:8082&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.fields-include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;EXCEPT_KEY&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-3"><a href="#Format-参数-3" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl130.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl131.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl132.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="数据类型映射-2"><a href="#数据类型映射-2" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前 Apache Flink 都是从 table schema 去推断反序列化期间的 Avro reader schema 和序列化期间的 Avro writer schema.<br>显式地定义 Avro schema 暂不支持.<br>Apache Avro Format中描述了 Flink 数据类型和 Avro 类型的对应关系.</p>
<p>除了此处列出的类型之外,Flink 还支持读取/写入可为空(nullable)的类型.<br>Flink 将可为空的类型映射到 Avro union(something, null), 其中 something 是从 Flink 类型转换的 Avro 类型.</p>
<h3 id="Debezium-Format"><a href="#Debezium-Format" class="headerlink" title="Debezium Format"></a>Debezium Format</h3><p>Changelog-Data-Capture Format<br>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Debezium 是一个 CDC(Changelog Data Capture,变更数据捕获)的工具,可以把来自 MySQL/PostgreSQL/Oracle/Microsoft SQL Server 和许多其他数据库的更改实时流式传输到 Kafka 中.<br>Debezium 为变更日志提供了统一的格式结构,并支持使用 JSON 和 Apache Avro 序列化消息.</p>
<p>Flink 支持将 Debezium JSON 和 Avro 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中.<br>在很多情况下,利用这个特性非常的有用,例如</p>
<ol>
<li>将增量数据从数据库同步到其他系统</li>
<li>日志审计</li>
<li>数据库的实时物化视图</li>
<li>关联维度数据库的变更历史,等等.</li>
</ol>
<p>Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Debezium 格式的 JSON 或 Avro 消息,输出到 Kafka 等存储中.<br>但需要注意的是,目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息.<br>因此,Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Debezium 消息.</p>
<h4 id="依赖-5"><a href="#依赖-5" class="headerlink" title="依赖"></a>依赖</h4><h5 id="Debezium-Avro"><a href="#Debezium-Avro" class="headerlink" title="Debezium Avro"></a>Debezium Avro</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro-confluent-registry<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="Debezium-Json"><a href="#Debezium-Json" class="headerlink" title="Debezium Json"></a>Debezium Json</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何使用-Debezium-Format"><a href="#如何使用-Debezium-Format" class="headerlink" title="如何使用 Debezium Format"></a>如何使用 Debezium Format</h4><p>Debezium 为变更日志提供了统一的格式,这是一个 JSON 格式的从 MySQL product 表捕获的更新操作的简单示例:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;before&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="number">111</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Big 2-wheel scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;weight&quot;</span>: <span class="number">5.18</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;after&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="number">111</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Big 2-wheel scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;weight&quot;</span>: <span class="number">5.15</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;source&quot;: &#123;...&#125;,</span><br><span class="line">  &quot;op&quot;: &quot;u&quot;,</span><br><span class="line">  &quot;ts_ms&quot;: 1589362330904,</span><br><span class="line">  &quot;transaction&quot;: null</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意: 请参考 Debezium 文档,了解每个字段的含义.</p>
<p>MySQL 产品表有4列(id/name/description/weight).<br>上面的 JSON 消息是 products 表上的一条更新事件,其中 id = 111 的行的 weight 值从 5.18 更改为 5.15.<br>假设此消息已同步到 Kafka 主题 products_binlog,则可以使用以下 DDL 来使用此主题并解析更改事件.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> topic_products (</span><br><span class="line">  <span class="comment">-- schema 与 MySQL 的 products 表完全相同</span></span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  description STRING,</span><br><span class="line">  weight <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;products_binlog&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="comment">-- 使用 &#x27;debezium-json&#x27; format 来解析 Debezium 的 JSON 消息</span></span><br><span class="line">  <span class="comment">-- 如果 Debezium 用 Avro 编码消息,请使用 &#x27;debezium-avro-confluent&#x27;</span></span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;debezium-json&#x27;</span>  <span class="comment">-- 如果 Debezium 用 Avro 编码消息,请使用 &#x27;debezium-avro-confluent&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>在某些情况下,用户在设置 Debezium Kafka Connect 时,可能会开启 Kafka 的配置 &#39;value.converter.schemas.enable&#39;,用来在消息体中包含 schema 信息.<br>然后,Debezium JSON 消息可能如下所示:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;schema&quot;: &#123;...&#125;,</span><br><span class="line">  &quot;payload&quot;: &#123;</span><br><span class="line">    &quot;before&quot;: &#123;</span><br><span class="line">      &quot;id&quot;: 111,</span><br><span class="line">      &quot;name&quot;: &quot;scooter&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Big 2-wheel scooter&quot;,</span><br><span class="line">      &quot;weight&quot;: 5.18</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;after&quot;: &#123;</span><br><span class="line">      &quot;id&quot;: 111,</span><br><span class="line">      &quot;name&quot;: &quot;scooter&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;Big 2-wheel scooter&quot;,</span><br><span class="line">      &quot;weight&quot;: 5.15</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;source&quot;: &#123;...&#125;,</span><br><span class="line">    &quot;op&quot;: &quot;u&quot;,</span><br><span class="line">    &quot;ts_ms&quot;: 1589362330904,</span><br><span class="line">    &quot;transaction&quot;: null</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为了解析这一类信息,你需要在上述 DDL WITH 子句中添加选项 &#39;debezium-json.schema-include&#39; = &#39;true&#39;(默认为 false).<br>通常情况下,建议不要包含 schema 的描述,因为这样会使消息变得非常冗长,并降低解析性能.</p>
<p>在将主题注册为 Flink 表之后,可以将 Debezium 消息用作变更日志源.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- MySQL &quot;products&quot; 的实时物化视图</span></span><br><span class="line"><span class="comment">-- 计算相同产品的最新平均重量</span></span><br><span class="line"><span class="keyword">SELECT</span> name, <span class="built_in">AVG</span>(weight) <span class="keyword">FROM</span> topic_products <span class="keyword">GROUP</span> <span class="keyword">BY</span> name;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将 MySQL &quot;products&quot; 表的所有数据和增量更改同步到</span></span><br><span class="line"><span class="comment">-- Elasticsearch &quot;products&quot; 索引,供将来查找</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> elasticsearch_products</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> topic_products;</span><br></pre></td></tr></table></figure>

<h4 id="Available-Metadata"><a href="#Available-Metadata" class="headerlink" title="Available Metadata"></a>Available Metadata</h4><p>以下格式元数据可以作为表定义中的只读 ( VIRTUAL) 列公开.<br>仅当相应的连接器转发格式元数据时,注意格式元数据字段才可用.<br>目前,只有 Kafka 连接器能够公开其值格式的元数据字段.</p>
<img src="/images/flgl133.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl134.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>以下示例显示了如何访问 Kafka 中的 Debezium 元数据字段:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  origin_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.ingestion-timestamp&#x27;</span> VIRTUAL,</span><br><span class="line">  event_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.timestamp&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_database STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.database&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_schema STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.schema&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_table STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.table&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_properties MAP<span class="operator">&lt;</span>STRING, STRING<span class="operator">&gt;</span> METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.properties&#x27;</span> VIRTUAL,</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;debezium-json&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-4"><a href="#Format-参数-4" class="headerlink" title="Format 参数"></a>Format 参数</h4><p>Flink 提供了 debezium-avro-confluent 和 debezium-json 两种 format 来解析 Debezium 生成的 JSON 格式和 Avro 格式的消息.<br>请使用 debezium-avro-confluent 来解析 Debezium 的 Avro 消息,使用 debezium-json 来解析 Debezium 的 JSON 消息.</p>
<h5 id="Debezium-Avro-1"><a href="#Debezium-Avro-1" class="headerlink" title="Debezium Avro"></a>Debezium Avro</h5><img src="/images/flgl135.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl136.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl137.png" style="margin-left: 0px; padding-bottom: 10px;">

<h5 id="Debezium-Json-1"><a href="#Debezium-Json-1" class="headerlink" title="Debezium Json"></a>Debezium Json</h5><img src="/images/flgl138.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl139.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl140.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><h5 id="重复的变更事件"><a href="#重复的变更事件" class="headerlink" title="重复的变更事件"></a>重复的变更事件</h5><p>在正常的操作环境下,Debezium 应用能以 exactly-once 的语义投递每条变更事件.<br>在这种情况下,Flink 消费 Debezium 产生的变更事件能够工作得很好.<br>然而,当有故障发生时,Debezium 应用只能保证 at-least-once 的投递语义.<br>可以查看 Debezium 官方文档 了解更多关于 Debezium 的消息投递语义.<br>这也意味着,在非正常情况下,Debezium 可能会投递重复的变更事件到 Kafka 中,当 Flink 从 Kafka 中消费的时候就会得到重复的事件.<br>这可能会导致 Flink query 的运行得到错误的结果或者非预期的异常.<br>因此,建议在这种情况下,将作业参数 table.exec.source.cdc-events-duplicate 设置成 true,并在该 source 上定义 PRIMARY KEY.<br>框架会生成一个额外的有状态算子,使用该 primary key 来对变更事件去重并生成一个规范化的 changelog 流.</p>
<h5 id="消费-Debezium-Postgres-Connector-产生的数据"><a href="#消费-Debezium-Postgres-Connector-产生的数据" class="headerlink" title="消费 Debezium Postgres Connector 产生的数据"></a>消费 Debezium Postgres Connector 产生的数据</h5><p>如果你正在使用 Debezium PostgreSQL Connector 捕获变更到 Kafka,请确保被监控表的 REPLICA IDENTITY 已经被配置成 FULL 了,默认值是 DEFAULT.<br>否则,Flink SQL 将无法正确解析 Debezium 数据.</p>
<p>当配置为 FULL 时,更新和删除事件将完整包含所有列的之前的值.<br>当为其他配置时,更新和删除事件的 &quot;before&quot; 字段将只包含 primary key 字段的值,或者为 null(没有 primary key).<br>你可以通过运行 ALTER TABLE <your-table-name> REPLICA IDENTITY FULL 来更改 REPLICA IDENTITY 的配置.<br>请阅读 Debezium 关于 PostgreSQL REPLICA IDENTITY 的文档 了解更多.</p>
<h4 id="数据类型映射-3"><a href="#数据类型映射-3" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前,Debezium Format 使用 JSON Format 进行序列化和反序列化.<br>有关数据类型映射的更多详细信息,请参考 JSON Format 文档 和 Confluent Avro Format 文档.</p>
<h3 id="Canal-Format"><a href="#Canal-Format" class="headerlink" title="Canal Format"></a>Canal Format</h3><p>Changelog-Data-Capture Format<br>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Canal 是一个 CDC(ChangeLog Data Capture,变更日志数据捕获)工具,可以实时地将 MySQL 变更传输到其他系统.<br>Canal 为变更日志提供了统一的数据格式,并支持使用 JSON 或 protobuf 序列化消息(Canal 默认使用 protobuf).</p>
<p>Flink 支持将 Canal 的 JSON 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中.<br>在很多情况下,利用这个特性非常的有用,例如:</p>
<ol>
<li>将增量数据从数据库同步到其他系统</li>
<li>日志审计</li>
<li>数据库的实时物化视图</li>
<li>关联维度数据库的变更历史,等等.</li>
</ol>
<p>Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Canal 格式的 JSON 消息,输出到 Kafka 等存储中.<br>但需要注意的是,目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息.<br>因此,Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Canal 消息.</p>
<p>注意:未来会支持 Canal protobuf 类型消息的解析以及输出 Canal 格式的消息.</p>
<h4 id="依赖-6"><a href="#依赖-6" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何使用-Canal-Format"><a href="#如何使用-Canal-Format" class="headerlink" title="如何使用 Canal Format"></a>如何使用 Canal Format</h4><p>Canal 为变更日志提供了统一的格式,下面是一个从 MySQL 库 products 表中捕获更新操作的简单示例:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;data&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;id&quot;</span>: <span class="string">&quot;111&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;scooter&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Big 2-wheel scooter&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;weight&quot;</span>: <span class="string">&quot;5.18&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;database&quot;</span>: <span class="string">&quot;inventory&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;es&quot;</span>: <span class="number">1589373560000</span>,</span><br><span class="line">  <span class="attr">&quot;id&quot;</span>: <span class="number">9</span>,</span><br><span class="line">  <span class="attr">&quot;isDdl&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">&quot;mysqlType&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="string">&quot;INTEGER&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;VARCHAR(255)&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;VARCHAR(512)&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;weight&quot;</span>: <span class="string">&quot;FLOAT&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;old&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;weight&quot;</span>: <span class="string">&quot;5.15&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;pkNames&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;id&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;sql&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;sqlType&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="attr">&quot;description&quot;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="attr">&quot;weight&quot;</span>: <span class="number">7</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;table&quot;</span>: <span class="string">&quot;products&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;ts&quot;</span>: <span class="number">1589373560798</span>,</span><br><span class="line">  <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;UPDATE&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意:有关各个字段的含义,请参阅 Canal 文档</p>
<p>MySQL products 表有4列(id,name,description 和 weight).<br>上面的 JSON 消息是 products 表上的一个更新事件,表示 id = 111 的行数据上 weight 字段值从5.15变更成为 5.18.<br>假设消息已经同步到了一个 Kafka 主题:products_binlog,那么就可以使用以下DDL来从这个主题消费消息并解析变更事件.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> topic_products (</span><br><span class="line">  <span class="comment">-- 元数据与 MySQL &quot;products&quot; 表完全相同</span></span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  description STRING,</span><br><span class="line">  weight <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;products_binlog&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;canal-json&#x27;</span>  <span class="comment">-- 使用 canal-json 格式</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>将 Kafka 主题注册成 Flink 表之后,就可以将 Canal 消息用作变更日志源.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 关于MySQL &quot;products&quot; 表的实时物化视图</span></span><br><span class="line"><span class="comment">-- 计算相同产品的最新平均重量</span></span><br><span class="line"><span class="keyword">SELECT</span> name, <span class="built_in">AVG</span>(weight) <span class="keyword">FROM</span> topic_products <span class="keyword">GROUP</span> <span class="keyword">BY</span> name;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将 MySQL &quot;products&quot; 表的所有数据和增量更改同步到</span></span><br><span class="line"><span class="comment">-- Elasticsearch &quot;products&quot; 索引以供将来搜索</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> elasticsearch_products</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> topic_products;</span><br></pre></td></tr></table></figure>

<h4 id="Available-Metadata-1"><a href="#Available-Metadata-1" class="headerlink" title="Available Metadata"></a>Available Metadata</h4><img src="/images/flgl141.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>The following example shows how to access Canal metadata fields in Kafka:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  origin_database STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.database&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_table STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.table&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_sql_type MAP<span class="operator">&lt;</span>STRING, <span class="type">INT</span><span class="operator">&gt;</span> METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.sql-type&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_pk_names <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span> METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.pk-names&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.ingestion-timestamp&#x27;</span> VIRTUAL,</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;canal-json&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-5"><a href="#Format-参数-5" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl142.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl143.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl144.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h4><h5 id="重复的变更事件-1"><a href="#重复的变更事件-1" class="headerlink" title="重复的变更事件"></a>重复的变更事件</h5><p>在正常的操作环境下,Canal 应用能以 exactly-once 的语义投递每条变更事件.<br>在这种情况下,Flink 消费 Canal 产生的变更事件能够工作得很好.<br>然而,当有故障发生时,Canal 应用只能保证 at-least-once 的投递语义.<br>这也意味着,在非正常情况下,Canal 可能会投递重复的变更事件到消息队列中,当 Flink 从消息队列中消费的时候就会得到重复的事件.<br>这可能会导致 Flink query 的运行得到错误的结果或者非预期的异常.<br>因此,建议在这种情况下,建议在这种情况下,将作业参数 table.exec.source.cdc-events-duplicate 设置成 true,并在该 source 上定义 PRIMARY KEY.<br>框架会生成一个额外的有状态算子,使用该 primary key 来对变更事件去重并生成一个规范化的 changelog 流.</p>
<h4 id="数据类型映射-4"><a href="#数据类型映射-4" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前,Canal Format 使用 JSON Format 进行序列化和反序列化.<br>有关数据类型映射的更多详细信息,请参阅 JSON Format 文档.</p>
<h3 id="Maxwell-Format"><a href="#Maxwell-Format" class="headerlink" title="Maxwell Format"></a>Maxwell Format</h3><p>Changelog-Data-Capture Format<br>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Maxwell是一个 CDC(Changelog Data Capture)工具,可以将 MySQL 的更改实时流式传输到 Kafka/Kinesis 和其他流式连接器.<br>Maxwell 为 changelog 提供统一的格式模式,并支持使用 JSON 序列化消息.</p>
<p>Flink 支持将 Maxwell JSON 消息解释为 INSERT/UPDATE/DELETE 消息到 Flink SQL 系统中.<br>在许多情况下,这对于利用此功能很有用,例如:</p>
<ol>
<li>将增量数据从数据库同步到其他系统</li>
<li>审核日志</li>
<li>数据库的实时物化视图</li>
<li>数据库表的临时连接更改历史记录等.</li>
</ol>
<p>Flink 还支持将 Flink SQL 中的 INSERT/UPDATE/DELETE 消息编码为 Maxwell JSON 消息,并发送到 Kafka 等外部系统.<br>但是,目前 Flink 无法将 UPDATE_BEFORE 和 UPDATE_AFTER 组合成单个 UPDATE 消息.<br>因此,Flink 将 UPDATE_BEFORE 和 UDPATE_AFTER 编码为 DELETE 和 INSERT Maxwell 消息.</p>
<h4 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a>依赖项</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>注意:请参阅Maxwell 文档,了解如何使用 Maxwell JSON 将更新日志同步到 Kafka 主题.</p>
<h4 id="如何使用Maxwell格式"><a href="#如何使用Maxwell格式" class="headerlink" title="如何使用Maxwell格式"></a>如何使用Maxwell格式</h4><p>Maxwell 为变更日志提供了统一的格式,下面是一个简单的示例,用于从productsJSON 格式的 MySQL 表中捕获更新操作:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">&quot;database&quot;</span>:<span class="string">&quot;test&quot;</span>,</span><br><span class="line">   <span class="attr">&quot;table&quot;</span>:<span class="string">&quot;e&quot;</span>,</span><br><span class="line">   <span class="attr">&quot;type&quot;</span>:<span class="string">&quot;insert&quot;</span>,</span><br><span class="line">   <span class="attr">&quot;ts&quot;</span>:<span class="number">1477053217</span>,</span><br><span class="line">   <span class="attr">&quot;xid&quot;</span>:<span class="number">23396</span>,</span><br><span class="line">   <span class="attr">&quot;commit&quot;</span>:<span class="literal">true</span>,</span><br><span class="line">   <span class="attr">&quot;position&quot;</span>:<span class="string">&quot;master.000006:800911&quot;</span>,</span><br><span class="line">   <span class="attr">&quot;server_id&quot;</span>:<span class="number">23042</span>,</span><br><span class="line">   <span class="attr">&quot;thread_id&quot;</span>:<span class="number">108</span>,</span><br><span class="line">   <span class="attr">&quot;primary_key&quot;</span>: [<span class="number">1</span>, <span class="string">&quot;2016-10-21 05:33:37.523000&quot;</span>],</span><br><span class="line">   <span class="attr">&quot;primary_key_columns&quot;</span>: [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;c&quot;</span>],</span><br><span class="line">   <span class="attr">&quot;data&quot;</span>:&#123;</span><br><span class="line">     <span class="attr">&quot;id&quot;</span>:<span class="number">111</span>,</span><br><span class="line">     <span class="attr">&quot;name&quot;</span>:<span class="string">&quot;scooter&quot;</span>,</span><br><span class="line">     <span class="attr">&quot;description&quot;</span>:<span class="string">&quot;Big 2-wheel scooter&quot;</span>,</span><br><span class="line">     <span class="attr">&quot;weight&quot;</span>:<span class="number">5.15</span></span><br><span class="line">   &#125;,</span><br><span class="line">   <span class="attr">&quot;old&quot;</span>:&#123;</span><br><span class="line">     <span class="attr">&quot;weight&quot;</span>:<span class="number">5.18</span>,</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意:有关各个字段的含义,请参阅Maxwell 文档.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> topic_products (</span><br><span class="line">  <span class="comment">-- schema is totally the same to the MySQL &quot;products&quot; table</span></span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  description STRING,</span><br><span class="line">  weight <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;products_binlog&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;maxwell-json&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>将主题注册为 Flink 表后,您可以使用 Maxwell 消息作为更改日志源.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- a real-time materialized view on the MySQL &quot;products&quot;</span></span><br><span class="line"><span class="comment">-- which calculate the latest average of weight for the same products</span></span><br><span class="line"><span class="keyword">SELECT</span> name, <span class="built_in">AVG</span>(weight) <span class="keyword">FROM</span> topic_products <span class="keyword">GROUP</span> <span class="keyword">BY</span> name;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- synchronize all the data and incremental changes of MySQL &quot;products&quot; table to</span></span><br><span class="line"><span class="comment">-- Elasticsearch &quot;products&quot; index for future searching</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> elasticsearch_products</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> topic_products;</span><br></pre></td></tr></table></figure>

<h4 id="可用元数据"><a href="#可用元数据" class="headerlink" title="可用元数据"></a>可用元数据</h4><p>以下格式元数据可以作为表定义中的只读 ( VIRTUAL) 列公开.</p>
<p>仅当相应的连接器转发格式元数据时,格式元数据字段才可用.<br>目前,只有 Kafka 连接器能够公开其值格式的元数据字段.</p>
<img src="/images/flgl145.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>以下示例显示了如何访问 Kafka 中的 Maxwell 元数据字段:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  origin_database STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.database&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_table STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.table&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_primary_key_columns <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span> METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.primary-key-columns&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.ingestion-timestamp&#x27;</span> VIRTUAL,</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;maxwell-json&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="格式选项"><a href="#格式选项" class="headerlink" title="格式选项"></a>格式选项</h4><img src="/images/flgl146.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl147.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h4><h5 id="重复的更改事件"><a href="#重复的更改事件" class="headerlink" title="重复的更改事件"></a>重复的更改事件</h5><p>Maxwell 应用程序允许一次性交付每个更改事件.<br>在这种情况下使用 Maxwell 生成的事件时,Flink 运行良好.<br>如果 Maxwell 应用程序工作在at-least-once交付,它可能会向 Kafka 交付重复的更改事件,而 Flink 将获得重复的事件.<br>这可能会导致 Flink 查询得到错误的结果或意外的异常.<br>因此,在这种情况下,建议将作业配置设置table.exec.source.cdc-events-duplicate为true并在源上定义 PRIMARY KEY.<br>框架将生成一个额外的有状态操作符,并使用主键对更改事件进行重复数据删除并生成规范化的更改日志流.</p>
<h4 id="数据类型映射-5"><a href="#数据类型映射-5" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前,Maxwell 格式使用 JSON 进行序列化和反序列化.<br>有关数据类型映射的更多详细信息,请参阅JSON 格式文档.</p>
<h3 id="Ogg-Format"><a href="#Ogg-Format" class="headerlink" title="Ogg Format"></a>Ogg Format</h3><p>Changelog-Data-Capture Format<br>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Oracle GoldenGate (a.k.a ogg) 是一个实现异构 IT 环境间数据实时数据集成和复制的综合软件包.<br>该产品集支持高可用性解决方案/实时数据集成/事务更改数据捕获/运营和分析企业系统之间的数据复制/转换和验证.<br>Ogg 为变更日志提供了统一的格式结构,并支持使用 JSON 序列化消息.</p>
<p>Flink 支持将 Ogg JSON 消息解析为 INSERT/UPDATE/DELETE 消息到 Flink SQL 系统中.<br>在很多情况下,利用这个特性非常有用,例如:</p>
<ol>
<li>将增量数据从数据库同步到其他系统</li>
<li>日志审计</li>
<li>数据库的实时物化视图</li>
<li>关联维度数据库的变更历史,等等</li>
</ol>
<p>Flink 还支持将 Flink SQL 中的 INSERT/UPDATE/DELETE 消息编码为 Ogg JSON 格式的消息, 输出到 Kafka 等存储中.<br>但需要注意, 目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息. 因此, Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Ogg 消息.</p>
<h4 id="依赖-7"><a href="#依赖-7" class="headerlink" title="依赖"></a>依赖</h4><h5 id="Ogg-Json"><a href="#Ogg-Json" class="headerlink" title="Ogg Json"></a>Ogg Json</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>注意: 请参考 Ogg Kafka Handler documentation, 了解如何设置 Ogg Kafka handler 来将变更日志同步到 Kafka 的 Topic.</p>
<h4 id="How-to-use-Ogg-format"><a href="#How-to-use-Ogg-format" class="headerlink" title="How to use Ogg format"></a>How to use Ogg format</h4><p>Ogg 为变更日志提供了统一的格式, 这是一个 JSON 格式的从 Oracle PRODUCTS 表捕获的更新操作的简单示例:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;before&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="number">111</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Big 2-wheel scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;weight&quot;</span>: <span class="number">5.18</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;after&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="number">111</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Big 2-wheel scooter&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;weight&quot;</span>: <span class="number">5.15</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;op_type&quot;</span>: <span class="string">&quot;U&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;op_ts&quot;</span>: <span class="string">&quot;2020-05-13 15:40:06.000000&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;current_ts&quot;</span>: <span class="string">&quot;2020-05-13 15:40:07.000000&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;primary_keys&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;id&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;pos&quot;</span>: <span class="string">&quot;00000000000000000000143&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;table&quot;</span>: <span class="string">&quot;PRODUCTS&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意:请参考 Debezium documentation 了解每个字段的含义.</p>
<p>Oracle PRODUCTS 表 有 4 列 (id, name, description and weight). 上面的 JSON 消息是 PRODUCTS 表上的一条更新事件,其中 id = 111 的行的 weight 值从 5.18 更改为 5.15. 假设此消息已同步到 Kafka 的 Topic products_ogg, 则可以使用以下 DDL 来使用该 Topic 并解析更新事件.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> topic_products (</span><br><span class="line">  <span class="comment">-- schema is totally the same to the Oracle &quot;products&quot; table</span></span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  description STRING,</span><br><span class="line">  weight <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;products_ogg&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;ogg-json&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>再将 Kafka Topic 注册为 Flink 表之后, 可以将 OGG 消息变为变更日志源.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- a real-time materialized view on the Oracle &quot;PRODUCTS&quot;</span></span><br><span class="line"><span class="comment">-- which calculate the latest average of weight for the same products</span></span><br><span class="line"><span class="keyword">SELECT</span> name, <span class="built_in">AVG</span>(weight)</span><br><span class="line"><span class="keyword">FROM</span> topic_products</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> name;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- synchronize all the data and incremental changes of Oracle &quot;PRODUCTS&quot; table to</span></span><br><span class="line"><span class="comment">-- Elasticsearch &quot;products&quot; index for future searching</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> elasticsearch_products</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> topic_products;</span><br></pre></td></tr></table></figure>

<h4 id="Available-Metadata-2"><a href="#Available-Metadata-2" class="headerlink" title="Available Metadata"></a>Available Metadata</h4><img src="/images/flgl148.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>The following example shows how to access Ogg metadata fields in Kafka:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  origin_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.ingestion-timestamp&#x27;</span> VIRTUAL,</span><br><span class="line">  event_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.event-timestamp&#x27;</span> VIRTUAL,</span><br><span class="line">  origin_table STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.table&#x27;</span> VIRTUAL,</span><br><span class="line">  primary_keys <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span> METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.primary_keys&#x27;</span> VIRTUAL,</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;ogg-json&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="Format-Options"><a href="#Format-Options" class="headerlink" title="Format Options"></a>Format Options</h4><img src="/images/flgl149.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl150.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="Data-Type-Mapping"><a href="#Data-Type-Mapping" class="headerlink" title="Data Type Mapping"></a>Data Type Mapping</h4><p>目前, Ogg format 使用 JSON format 进行序列化和反序列化.<br>有关数据类型映射的更多详细信息,请参考 JSON Format 文档.</p>
<h3 id="Parquet-格式"><a href="#Parquet-格式" class="headerlink" title="Parquet 格式"></a>Parquet 格式</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Apache Parquet 格式允许读写 Parquet 数据.</p>
<h4 id="依赖-8"><a href="#依赖-8" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-parquet<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何创建基于-Parquet-格式的表"><a href="#如何创建基于-Parquet-格式的表" class="headerlink" title="如何创建基于 Parquet 格式的表"></a>如何创建基于 Parquet 格式的表</h4><p>以下为用 Filesystem 连接器和 Parquet 格式创建表的示例,</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  category_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING,</span><br><span class="line">  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  dt STRING</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;filesystem&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;path&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/tmp/user_behavior&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;parquet&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-6"><a href="#Format-参数-6" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl151.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>Parquet 格式也支持 ParquetOutputFormat 的配置.<br>例如, 可以配置 parquet.compression=GZIP 来开启 gzip 压缩.</p>
<h4 id="数据类型映射-6"><a href="#数据类型映射-6" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>目前,Parquet 格式类型映射与 Apache Hive 兼容,但与 Apache Spark 有所不同:</p>
<ol>
<li>Timestamp:不论精度,映射 timestamp 类型至 int96.</li>
<li>Decimal:根据精度,映射 decimal 类型至固定长度字节的数组.</li>
</ol>
<p>下表列举了 Flink 中的数据类型与 JSON 中的数据类型的映射关系.</p>
<table>
<thead>
<tr>
<th align="left">Flink 数据类型</th>
<th align="left">Parquet 类型</th>
<th align="left">Parquet 逻辑类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR / VARCHAR / STRING</td>
<td align="left">BINARY</td>
<td align="left">UTF8</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">BOOLEAN</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">BINARY / VARBINARY</td>
<td align="left">BINARY</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">FIXED_LEN_BYTE_ARRAY</td>
<td align="left">DECIMAL</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">INT32</td>
<td align="left">INT_8</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">INT32</td>
<td align="left">INT_16</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">INT32</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">INT64</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">FLOAT</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">DOUBLE</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">INT32</td>
<td align="left">DATE</td>
</tr>
<tr>
<td align="left">TIME</td>
<td align="left">INT32</td>
<td align="left">TIME_MILLIS</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">INT96</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">ARRAY</td>
<td align="left"></td>
<td align="left">LIST</td>
</tr>
<tr>
<td align="left">MAP</td>
<td align="left"></td>
<td align="left">MAP</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left"></td>
<td align="left">STRUCT</td>
</tr>
</tbody></table>
<p>注意 复合数据类型暂只支持写不支持读(Array/Map 与 Row).</p>
<h3 id="Orc-Format"><a href="#Orc-Format" class="headerlink" title="Orc Format"></a>Orc Format</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Apache Orc Format 允许读写 ORC 数据.</p>
<h4 id="依赖-9"><a href="#依赖-9" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-orc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="如何用-Orc-格式创建一个表格"><a href="#如何用-Orc-格式创建一个表格" class="headerlink" title="如何用 Orc 格式创建一个表格"></a>如何用 Orc 格式创建一个表格</h4><p>下面是一个用 Filesystem connector 和 Orc format 创建表格的例子</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  item_id <span class="type">BIGINT</span>,</span><br><span class="line">  category_id <span class="type">BIGINT</span>,</span><br><span class="line">  behavior STRING,</span><br><span class="line">  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  dt STRING</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;filesystem&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;path&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/tmp/user_behavior&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;orc&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="Format-参数-7"><a href="#Format-参数-7" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl152.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>Orc 格式也支持来源于 Table properties 的表属性.<br>举个例子,你可以设置 orc.compress=SNAPPY 来允许spappy压缩.</p>
<h4 id="数据类型映射-7"><a href="#数据类型映射-7" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>Orc 格式类型的映射和 Apache Hive 是兼容的.<br>下面的表格列出了 Flink 类型的数据和 Orc 类型的数据的映射关系.</p>
<table>
<thead>
<tr>
<th align="left">Flink 数据类型</th>
<th align="left">Orc 物理类型</th>
<th align="left">Orc 逻辑类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR</td>
<td align="left">bytes</td>
<td align="left">CHAR</td>
</tr>
<tr>
<td align="left">VARCHAR</td>
<td align="left">bytes</td>
<td align="left">VARCHAR</td>
</tr>
<tr>
<td align="left">STRING</td>
<td align="left">bytes</td>
<td align="left">STRING</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">long</td>
<td align="left">BOOLEAN</td>
</tr>
<tr>
<td align="left">BYTES</td>
<td align="left">bytes</td>
<td align="left">BINARY</td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">decimal</td>
<td align="left">DECIMAL</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">long</td>
<td align="left">BYTE</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">long</td>
<td align="left">SHORT</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">long</td>
<td align="left">INT</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">long</td>
<td align="left">LONG</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">double</td>
<td align="left">FLOAT</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">double</td>
<td align="left">DOUBLE</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">long</td>
<td align="left">DATE</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">timestamp</td>
<td align="left">TIMESTAMP</td>
</tr>
<tr>
<td align="left">ARRAY</td>
<td align="left">-</td>
<td align="left">LIST</td>
</tr>
<tr>
<td align="left">MAP</td>
<td align="left">-</td>
<td align="left">MAP</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">-</td>
<td align="left">STRUCT</td>
</tr>
</tbody></table>
<h3 id="Raw-Format"><a href="#Raw-Format" class="headerlink" title="Raw Format"></a>Raw Format</h3><p>Format: Serialization Schema<br>Format: Deserialization Schema</p>
<p>Raw format 允许读写原始(基于字节)值作为单个列.</p>
<p>注意: 这种格式将 null 值编码成 byte[] 类型的 null.<br>这样在 upsert-kafka 中使用时可能会有限制,因为 upsert-kafka 将 null 值视为 墓碑消息(在键上删除).<br>因此,如果该字段可能具有 null 值,我们建议避免使用 upsert-kafka 连接器和 raw format 作为 value.format.</p>
<p>Raw format 连接器是内置的.</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>例如,你可能在 Kafka 中具有原始日志数据,并希望使用 Flink SQL 读取和分析此类数据.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">47.29.201.179 - - [28&#x2F;Feb&#x2F;2019:13:17:10 +0000] &quot;GET &#x2F;?p&#x3D;1 HTTP&#x2F;2.0&quot; 200 5316 &quot;https:&#x2F;&#x2F;domain.com&#x2F;?p&#x3D;1&quot; &quot;Mozilla&#x2F;5.0 (Windows NT 6.1) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;72.0.3626.119 Safari&#x2F;537.36&quot; &quot;2.75&quot;</span><br></pre></td></tr></table></figure>

<p>下面的代码创建了一张表,使用 raw format 以 UTF-8 编码的形式从中读取(也可以写入)底层的 Kafka topic 作为匿名字符串值:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> nginx_log (</span><br><span class="line">  log STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;nginx_log&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;raw&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>然后,你可以将原始数据读取为纯字符串,之后使用用户自定义函数将其分为多个字段进行进一步分析.<br>例如 示例中的 my_split.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> t.hostname, t.datetime, t.url, t.browser, ...</span><br><span class="line"><span class="keyword">FROM</span>(</span><br><span class="line">  <span class="keyword">SELECT</span> my_split(log) <span class="keyword">as</span> t <span class="keyword">FROM</span> nginx_log</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>相对应的,你也可以将一个 STRING 类型的列以 UTF-8 编码的匿名字符串值写入 Kafka topic.</p>
<h4 id="Format-参数-8"><a href="#Format-参数-8" class="headerlink" title="Format 参数"></a>Format 参数</h4><img src="/images/flgl153.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="数据类型映射-8"><a href="#数据类型映射-8" class="headerlink" title="数据类型映射"></a>数据类型映射</h4><p>下表详细说明了这种格式支持的 SQL 类型,包括用于编码和解码的序列化类和反序列化类的详细信息.</p>
<table>
<thead>
<tr>
<th align="left">Flink SQL 类型</th>
<th align="left">值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR / VARCHAR / STRING</td>
<td align="left">UTF-8(默认)编码的文本字符串.编码字符集可以通过 &#39;raw.charset&#39; 进行配置.</td>
</tr>
<tr>
<td align="left">BINARY / VARBINARY / BYTES</td>
<td align="left">字节序列本身.</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">表示布尔值的单个字节,0表示 false,1 表示 true.</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">有符号数字值的单个字节.</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">采用big-endian(默认)编码的两个字节.字节序可以通过 &#39;raw.endianness&#39; 配置.</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">采用 big-endian (默认)编码的四个字节.字节序可以通过 &#39;raw.endianness&#39; 配置.</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">采用 big-endian (默认)编码的八个字节.字节序可以通过 &#39;raw.endianness&#39; 配置.</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">采用 IEEE 754 格式和 big-endian (默认)编码的四个字节.字节序可以通过 &#39;raw.endianness&#39; 配置.</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">采用 IEEE 754 格式和 big-endian (默认)编码的八个字节.字节序可以通过 &#39;raw.endianness&#39; 配置.</td>
</tr>
<tr>
<td align="left">RAW</td>
<td align="left">通过 RAW 类型的底层 TypeSerializer 序列化的字节序列.</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">-</td>
</tr>
</tbody></table>
<h2 id="Kafka-1"><a href="#Kafka-1" class="headerlink" title="Kafka"></a>Kafka</h2><p>Scan Source: Unbounded<br>Sink: Streaming Append Mode</p>
<p>Kafka 连接器提供从 Kafka topic 中消费和写入数据的能力.</p>
<h3 id="依赖-10"><a href="#依赖-10" class="headerlink" title="依赖"></a>依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Kafka 连接器目前并不包含在 Flink 的二进制发行版中,请查阅这里了解如何在集群运行中引用 Kafka 连接器.</p>
<h3 id="如何创建-Kafka-表"><a href="#如何创建-Kafka-表" class="headerlink" title="如何创建 Kafka 表"></a>如何创建 Kafka 表</h3><p>以下示例展示了如何创建 Kafka 表:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING,</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="可用的元数据"><a href="#可用的元数据" class="headerlink" title="可用的元数据"></a>可用的元数据</h3><p>以下的连接器元数据可以在表定义中通过元数据列的形式获取.</p>
<p>R/W 列定义了一个元数据是可读的(R)还是可写的(W).<br>只读列必须声明为 VIRTUAL 以在 INSERT INTO 操作中排除它们.</p>
<img src="/images/flgl154.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>以下扩展的 CREATE TABLE 示例展示了使用这些元数据字段的语法:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `event_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">  `<span class="keyword">partition</span>` <span class="type">BIGINT</span> METADATA VIRTUAL,</span><br><span class="line">  `<span class="keyword">offset</span>` <span class="type">BIGINT</span> METADATA VIRTUAL,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="格式元信息"><a href="#格式元信息" class="headerlink" title="格式元信息"></a>格式元信息</h3><p>连接器可以读出消息格式的元数据.<br>格式元数据的配置键以 &#39;value.&#39; 作为前缀.</p>
<p>以下示例展示了如何获取 Kafka 和 Debezium 的元数据字段:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `event_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.timestamp&#x27;</span> VIRTUAL,  <span class="comment">-- from Debezium format</span></span><br><span class="line">  `origin_table` STRING METADATA <span class="keyword">FROM</span> <span class="string">&#x27;value.source.table&#x27;</span> VIRTUAL, <span class="comment">-- from Debezium format</span></span><br><span class="line">  `partition_id` <span class="type">BIGINT</span> METADATA <span class="keyword">FROM</span> <span class="string">&#x27;partition&#x27;</span> VIRTUAL,  <span class="comment">-- from Kafka connector</span></span><br><span class="line">  `<span class="keyword">offset</span>` <span class="type">BIGINT</span> METADATA VIRTUAL,  <span class="comment">-- from Kafka connector</span></span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;debezium-json&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="连接器参数"><a href="#连接器参数" class="headerlink" title="连接器参数"></a>连接器参数</h3><img src="/images/flgl155.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl156.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl157.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><h4 id="消息键-Key-与消息体-Value-的格式"><a href="#消息键-Key-与消息体-Value-的格式" class="headerlink" title="消息键(Key)与消息体(Value)的格式"></a>消息键(Key)与消息体(Value)的格式</h4><p>Kafka 消息的消息键和消息体部分都可以使用某种 格式 来序列化或反序列化成二进制数据.</p>
<h5 id="消息体格式"><a href="#消息体格式" class="headerlink" title="消息体格式"></a>消息体格式</h5><p>由于 Kafka 消息中消息键是可选的,以下语句将使用消息体格式读取和写入消息,但不使用消息键格式.<br>&#39;format&#39; 选项与 &#39;value.format&#39; 意义相同.<br>所有的格式配置使用格式识别符作为前缀.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;json.ignore-parse-errors&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>消息体格式将配置为以下的数据类型:<br>ROW&lt;<code>user_id</code> BIGINT, <code>item_id</code> BIGINT, <code>behavior</code> STRING&gt;</p>
<h5 id="消息键和消息体格式"><a href="#消息键和消息体格式" class="headerlink" title="消息键和消息体格式"></a>消息键和消息体格式</h5><p>以下示例展示了如何配置和使用消息键和消息体格式.<br>格式配置使用 &#39;key&#39; 或 &#39;value&#39; 加上格式识别符作为前缀.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.json.ignore-parse-errors&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.fields&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_id;item_id&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.json.fail-on-missing-field&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.fields-include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;ALL&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>消息键格式包含了在 &#39;key.fields&#39; 中列出的字段(使用 &#39;;&#39; 分隔)和字段顺序.<br>因此将配置为以下的数据类型:<br>ROW&lt;<code>user_id</code> BIGINT, <code>item_id</code> BIGINT&gt;</p>
<p>由于消息体格式配置为 &#39;value.fields-include&#39; = &#39;ALL&#39;,所以消息键字段也会出现在消息体格式的数据类型中:<br>ROW&lt;<code>user_id</code> BIGINT, <code>item_id</code> BIGINT, <code>behavior</code> STRING&gt;</p>
<h5 id="重名的格式字段"><a href="#重名的格式字段" class="headerlink" title="重名的格式字段"></a>重名的格式字段</h5><p>如果消息键字段和消息体字段重名,连接器无法根据表结构信息将这些列区分开.<br>&#39;key.fields-prefix&#39; 配置项可以在表结构中为消息键字段指定一个唯一名称,并在配置消息键格式的时候保留原名.</p>
<p>以下示例展示了在消息键和消息体中同时包含 version 字段的情况:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `k_version` <span class="type">INT</span>,</span><br><span class="line">  `k_user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `k_item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `version` <span class="type">INT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.fields-prefix&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;k_&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.fields&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;k_version;k_user_id;k_item_id&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.fields-include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;EXCEPT_KEY&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>消息体格式必须配置为 &#39;EXCEPT_KEY&#39; 模式.<br>格式将被配置为以下的数据类型:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">消息键格式:</span><br><span class="line">ROW&lt;&#96;version&#96; INT, &#96;user_id&#96; BIGINT, &#96;item_id&#96; BIGINT&gt;</span><br><span class="line"></span><br><span class="line">消息体格式:</span><br><span class="line">ROW&lt;&#96;version&#96; INT, &#96;behavior&#96; STRING&gt;</span><br></pre></td></tr></table></figure>

<h4 id="Topic-和-Partition-的探测"><a href="#Topic-和-Partition-的探测" class="headerlink" title="Topic 和 Partition 的探测"></a>Topic 和 Partition 的探测</h4><p>topic 和 topic-pattern 配置项决定了 source 消费的 topic 或 topic 的匹配规则.<br>topic 配置项可接受使用分号间隔的 topic 列表,例如 topic-1;topic-2.<br>topic-pattern 配置项使用正则表达式来探测匹配的 topic.<br>例如 topic-pattern 设置为 test-topic-[0-9],则在作业启动时,所有匹配该正则表达式的 topic(以 test-topic- 开头,以一位数字结尾)都将被 consumer 订阅.</p>
<p>为允许 consumer 在作业启动之后探测到动态创建的 topic,请将 scan.topic-partition-discovery.interval 配置为一个非负值.<br>这将使 consumer 能够探测匹配名称规则的 topic 中新的 partition.</p>
<p>请参阅 Kafka DataStream 连接器文档 以获取更多关于 topic 和 partition 探测的信息.</p>
<p>注意 topic 列表和 topic 匹配规则只适用于 source.<br>对于 sink 端,Flink 目前只支持单一 topic.</p>
<h4 id="起始消费位点-1"><a href="#起始消费位点-1" class="headerlink" title="起始消费位点"></a>起始消费位点</h4><p>scan.startup.mode 配置项决定了 Kafka consumer 的启动模式.<br>有效值为:</p>
<ol>
<li><code>group-offsets</code>:从 Zookeeper/Kafka 中某个指定的消费组已提交的偏移量开始.</li>
<li><code>earliest-offset</code>:从可能的最早偏移量开始.</li>
<li><code>latest-offset</code>:从最末尾偏移量开始.</li>
<li><code>timestamp</code>:从用户为每个 partition 指定的时间戳开始.</li>
<li><code>specific-offsets</code>:从用户为每个 partition 指定的偏移量开始.</li>
</ol>
<p>默认值 group-offsets 表示从 Zookeeper/Kafka 中最近一次已提交的偏移量开始消费.</p>
<p>如果使用了 timestamp,必须使用另外一个配置项 scan.startup.timestamp-millis 来指定一个从格林尼治标准时间 1970 年 1 月 1 日 00:00:00.000 开始计算的毫秒单位时间戳作为起始时间.</p>
<p>如果使用了 specific-offsets,必须使用另外一个配置项 scan.startup.specific-offsets 来为每个 partition 指定起始偏移量, 例如,选项值 partition:0,offset:42;partition:1,offset:300 表示 partition 0 从偏移量 42 开始,partition 1 从偏移量 300 开始.</p>
<h4 id="CDC-变更日志-Changelog-Source"><a href="#CDC-变更日志-Changelog-Source" class="headerlink" title="CDC 变更日志(Changelog) Source"></a>CDC 变更日志(Changelog) Source</h4><p>Flink 原生支持使用 Kafka 作为 CDC 变更日志(changelog) source.<br>如果 Kafka topic 中的消息是通过变更数据捕获(CDC)工具从其他数据库捕获的变更事件,则你可以使用 CDC 格式将消息解析为 Flink SQL 系统中的插入(INSERT)/更新(UPDATE)/删除(DELETE)消息.</p>
<p>在许多情况下,变更日志(changelog) source 都是非常有用的功能,例如将数据库中的增量数据同步到其他系统,审核日志,数据库的物化视图,时态表关联数据库表的更改历史等.</p>
<p>Flink 提供了几种 CDC 格式:<br>debezium<br>canal<br>maxwell</p>
<h4 id="Sink-分区"><a href="#Sink-分区" class="headerlink" title="Sink 分区"></a>Sink 分区</h4><p>配置项 sink.partitioner 指定了从 Flink 分区到 Kafka 分区的映射关系.<br>默认情况下,Flink 使用 Kafka 默认分区器 来对消息分区.<br>默认分区器对没有消息键的消息使用 粘性分区策略(sticky partition strategy) 进行分区,对含有消息键的消息使用 murmur2 哈希算法计算分区.</p>
<p>为了控制数据行到分区的路由,也可以提供一个自定义的 sink 分区器.<br>&#39;fixed&#39; 分区器会将同一个 Flink 分区中的消息写入同一个 Kafka 分区,从而减少网络连接的开销.</p>
<h4 id="一致性保证"><a href="#一致性保证" class="headerlink" title="一致性保证"></a>一致性保证</h4><p>默认情况下,如果查询在 启用 checkpoint 模式下执行时,Kafka sink 按照至少一次(at-lease-once)语义保证将数据写入到 Kafka topic 中.</p>
<p>当 Flink checkpoint 启用时,kafka 连接器可以提供精确一次(exactly-once)的语义保证.</p>
<p>除了启用 Flink checkpoint,还可以通过传入对应的 sink.semantic 选项来选择三种不同的运行模式:</p>
<ol>
<li>none:Flink 不保证任何语义.已经写出的记录可能会丢失或重复.</li>
<li>at-least-once (默认设置):保证没有记录会丢失(但可能会重复).</li>
<li>exactly-once:使用 Kafka 事务提供精确一次(exactly-once)语义.<br>当使用事务向 Kafka 写入数据时,请将所有从 Kafka 中消费记录的应用中的 isolation.level 配置项设置成实际所需的值(read_committed 或 read_uncommitted,后者为默认值).</li>
</ol>
<p>请参阅 Kafka 文档 以获取更多关于语义保证的信息.</p>
<h4 id="Source-按分区-Watermark"><a href="#Source-按分区-Watermark" class="headerlink" title="Source 按分区 Watermark"></a>Source 按分区 Watermark</h4><p>Flink 对于 Kafka 支持发送按分区的 watermark.<br>Watermark 在 Kafka consumer 中生成.<br>按分区 watermark 的合并方式和在流 shuffle 时合并 Watermark 的方式一致.<br>Source 输出的 watermark 由读取的分区中最小的 watermark 决定.<br>如果 topic 中的某些分区闲置,watermark 生成器将不会向前推进.<br>你可以在表配置中设置 &#39;table.exec.source.idle-timeout&#39; 选项来避免上述问题.</p>
<p>请参阅 Kafka watermark 策略 以获取更多细节.</p>
<h4 id="安全-1"><a href="#安全-1" class="headerlink" title="安全"></a>安全</h4><p>要启用加密和认证相关的安全配置,只需将安全配置加上 &quot;properties.&quot; 前缀配置在 Kafka 表上即可.<br>下面的代码片段展示了如何配置 Kafka 表以使用 PLAIN 作为 SASL 机制并提供 JAAS 配置:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING,</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line">  <span class="string">&#x27;properties.security.protocol&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;SASL_PLAINTEXT&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.sasl.mechanism&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;PLAIN&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.sasl.jaas.config&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;username\&quot; password=\&quot;password\&quot;;&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>另一个更复杂的例子,使用 SASL_SSL 作为安全协议并使用 SCRAM-SHA-256 作为 SASL 机制:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING,</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line">  <span class="string">&#x27;properties.security.protocol&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;SASL_SSL&#x27;</span>,</span><br><span class="line">  <span class="comment">/* SSL 配置 */</span></span><br><span class="line">  <span class="comment">/* 配置服务端提供的 truststore (CA 证书) 的路径 */</span></span><br><span class="line">  <span class="string">&#x27;properties.ssl.truststore.location&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/path/to/kafka.client.truststore.jks&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.ssl.truststore.password&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;test1234&#x27;</span>,</span><br><span class="line">  <span class="comment">/* 如果要求客户端认证,则需要配置 keystore (私钥) 的路径 */</span></span><br><span class="line">  <span class="string">&#x27;properties.ssl.keystore.location&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/path/to/kafka.client.keystore.jks&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.ssl.keystore.password&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;test1234&#x27;</span>,</span><br><span class="line">  <span class="comment">/* SASL 配置 */</span></span><br><span class="line">  <span class="comment">/* 将 SASL 机制配置为 as SCRAM-SHA-256 */</span></span><br><span class="line">  <span class="string">&#x27;properties.sasl.mechanism&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;SCRAM-SHA-256&#x27;</span>,</span><br><span class="line">  <span class="comment">/* 配置 JAAS */</span></span><br><span class="line">  <span class="string">&#x27;properties.sasl.jaas.config&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;org.apache.kafka.common.security.scram.ScramLoginModule required username=\&quot;username\&quot; password=\&quot;password\&quot;;&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>如果在作业 JAR 中 Kafka 客户端依赖的类路径被重置了(relocate class),登录模块(login module)的类路径可能会不同,因此请根据登录模块在 JAR 中实际的类路径来改写以上配置.<br>例如在 SQL client JAR 中,Kafka client 依赖被重置在了 org.apache.flink.kafka.shaded.org.apache.kafka 路径下, 因此 plain 登录模块的类路径应写为 org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule.</p>
<p>关于安全配置的详细描述,请参阅 Apache Kafka 文档中的&quot;安全&quot;一节.</p>
<h3 id="数据类型映射-9"><a href="#数据类型映射-9" class="headerlink" title="数据类型映射"></a>数据类型映射</h3><p>Kafka 将消息键值以二进制进行存储,因此 Kafka 并不存在 schema 或数据类型.<br>Kafka 消息使用格式配置进行序列化和反序列化,例如 csv,json,avro.<br>因此,数据类型映射取决于使用的格式.<br>请参阅 格式 页面以获取更多细节.</p>
<h2 id="Upsert-Kafka"><a href="#Upsert-Kafka" class="headerlink" title="Upsert Kafka"></a>Upsert Kafka</h2><p>Scan Source: Unbounded<br>Sink: Streaming Upsert Mode</p>
<p>Upsert Kafka 连接器支持以 upsert 方式从 Kafka topic 中读取数据并将数据写入 Kafka topic.</p>
<p>作为 source,upsert-kafka 连接器生产 changelog 流,其中每条数据记录代表一个更新或删除事件.<br>更准确地说,数据记录中的 value 被解释为同一 key 的最后一个 value 的 UPDATE,如果有这个 key(如果不存在相应的 key,则该更新被视为 INSERT).<br>用表来类比,changelog 流中的数据记录被解释为 UPSERT,也称为 INSERT/UPDATE,因为任何具有相同 key 的现有行都被覆盖.<br>另外,value 为空的消息将会被视作为 DELETE 消息.</p>
<p>作为 sink,upsert-kafka 连接器可以消费 changelog 流.<br>它会将 INSERT/UPDATE_AFTER 数据作为正常的 Kafka 消息写入,并将 DELETE 数据以 value 为空的 Kafka 消息写入(表示对应 key 的消息被删除).<br>Flink 将根据主键列的值对数据进行分区,从而保证主键上的消息有序,因此同一主键上的更新/删除消息将落在同一分区中.</p>
<h3 id="依赖-11"><a href="#依赖-11" class="headerlink" title="依赖"></a>依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Upsert Kafka 连接器不是二进制发行版的一部分,请查阅这里了解如何在集群运行中引用 Upsert Kafka 连接器.</p>
<h3 id="完整示例"><a href="#完整示例" class="headerlink" title="完整示例"></a>完整示例</h3><p>下面的示例展示了如何创建和使用 Upsert Kafka 表:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews_per_region (</span><br><span class="line">  user_region STRING,</span><br><span class="line">  pv <span class="type">BIGINT</span>,</span><br><span class="line">  uv <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (user_region) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;upsert-kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;pageviews_per_region&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  page_id <span class="type">BIGINT</span>,</span><br><span class="line">  viewtime <span class="type">TIMESTAMP</span>,</span><br><span class="line">  user_region STRING,</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> viewtime <span class="keyword">AS</span> viewtime <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;pageviews&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 计算 pv/uv 并插入到 upsert-kafka sink</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> pageviews_per_region</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  user_region,</span><br><span class="line">  <span class="built_in">COUNT</span>(<span class="operator">*</span>),</span><br><span class="line">  <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id)</span><br><span class="line"><span class="keyword">FROM</span> pageviews</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> user_region;</span><br></pre></td></tr></table></figure>

<p>注意 确保在 DDL 中定义主键.</p>
<h3 id="Available-Metadata-3"><a href="#Available-Metadata-3" class="headerlink" title="Available Metadata"></a>Available Metadata</h3><p>See the regular Kafka connector for a list of all available metadata fields.</p>
<img src="/images/flgl158.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl159.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="特性-1"><a href="#特性-1" class="headerlink" title="特性"></a>特性</h3><h4 id="Key-and-Value-Formats"><a href="#Key-and-Value-Formats" class="headerlink" title="Key and Value Formats"></a>Key and Value Formats</h4><p>See the regular Kafka connector for more explanation around key and value formats. However, note that this connector requires both a key and value format where the key fields are derived from the PRIMARY KEY constraint.</p>
<p>The following example shows how to specify and configure key and value formats. The format options are prefixed with either the &#39;key&#39; or &#39;value&#39; plus format identifier.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`user_id`) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;upsert-kafka&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.json.ignore-parse-errors&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.json.fail-on-missing-field&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.fields-include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;EXCEPT_KEY&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="主键约束"><a href="#主键约束" class="headerlink" title="主键约束"></a>主键约束</h4><p>Upsert Kafka 始终以 upsert 方式工作,并且需要在 DDL 中定义主键.<br>在具有相同主键值的消息按序存储在同一个分区的前提下,在 changelog source 定义主键意味着 在物化后的 changelog 上主键具有唯一性.<br>定义的主键将决定哪些字段出现在 Kafka 消息的 key 中.</p>
<h4 id="一致性保证-1"><a href="#一致性保证-1" class="headerlink" title="一致性保证"></a>一致性保证</h4><p>默认情况下,如果启用 checkpoint,Upsert Kafka sink 会保证至少一次将数据插入 Kafka topic.</p>
<p>这意味着,Flink 可以将具有相同 key 的重复记录写入 Kafka topic.<br>但由于该连接器以 upsert 的模式工作,该连接器作为 source 读入时,可以确保具有相同主键值下仅最后一条消息会生效.<br>因此,upsert-kafka 连接器可以像 HBase sink 一样实现幂等写入.</p>
<h4 id="为每个分区生成相应的-watermark"><a href="#为每个分区生成相应的-watermark" class="headerlink" title="为每个分区生成相应的 watermark"></a>为每个分区生成相应的 watermark</h4><p>Flink 支持根据 Upsert Kafka 的 每个分区的数据特性发送相应的 watermark.<br>当使用这个特性的时候,watermark 是在 Kafka consumer 内部生成的.<br>合并每个分区 生成的 watermark 的方式和 stream shuffle 的方式是一致的.<br>数据源产生的 watermark 是取决于该 consumer 负责的所有分区中当前最小的 watermark.<br>如果该 consumer 负责的部分分区是 idle 的,那么整体的 watermark 并不会前进.<br>在这种情况下,可以通过设置合适的 table.exec.source.idle-timeout 来缓解这个问题.</p>
<p>如想获得更多细节,请查阅 Kafka watermark strategies.</p>
<h3 id="数据类型映射-10"><a href="#数据类型映射-10" class="headerlink" title="数据类型映射"></a>数据类型映射</h3><p>Upsert Kafka 用字节存储消息的 key 和 value,因此没有 schema 或数据类型.<br>消息按格式进行序列化和反序列化,例如:csv/json/avro.<br>因此数据类型映射表由指定的格式确定.<br>请参考格式页面以获取更多详细信息.</p>
<h2 id="Firehose-1"><a href="#Firehose-1" class="headerlink" title="Firehose"></a>Firehose</h2><h2 id="Kinesis-1"><a href="#Kinesis-1" class="headerlink" title="Kinesis"></a>Kinesis</h2><h2 id="JDBC-1"><a href="#JDBC-1" class="headerlink" title="JDBC"></a>JDBC</h2><p>Scan Source: Bounded<br>Lookup Source: Sync Mode<br>Sink: Batch<br>Sink: Streaming Append &amp; Upsert Mode</p>
<p>JDBC 连接器允许使用 JDBC 驱动向任意类型的关系型数据库读取或者写入数据.<br>本文档描述了针对关系型数据库如何通过建立 JDBC 连接器来执行 SQL 查询.</p>
<p>如果在 DDL 中定义了主键,JDBC sink 将以 upsert 模式与外部系统交换 UPDATE/DELETE 消息；否则,它将以 append 模式与外部系统交换消息且不支持消费 UPDATE/DELETE 消息.</p>
<h3 id="依赖-12"><a href="#依赖-12" class="headerlink" title="依赖"></a>依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="如何创建-JDBC-表"><a href="#如何创建-JDBC-表" class="headerlink" title="如何创建 JDBC 表"></a>如何创建 JDBC 表</h3><p>JDBC table 可以按如下定义:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 在 Flink SQL 中注册一张 MySQL 表 &#x27;users&#x27;</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyUserTable (</span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  age <span class="type">INT</span>,</span><br><span class="line">  status <span class="type">BOOLEAN</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (id) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc:mysql://localhost:3306/mydatabase&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;table-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;users&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 从另一张表 &quot;T&quot; 将数据写入到 JDBC 表中</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> MyUserTable</span><br><span class="line"><span class="keyword">SELECT</span> id, name, age, status <span class="keyword">FROM</span> T;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看 JDBC 表中的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> id, name, age, status <span class="keyword">FROM</span> MyUserTable;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- JDBC 表在时态表关联中作为维表</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> myTopic</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> MyUserTable <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> myTopic.proctime</span><br><span class="line"><span class="keyword">ON</span> myTopic.key <span class="operator">=</span> MyUserTable.id;</span><br></pre></td></tr></table></figure>

<h3 id="连接器参数-1"><a href="#连接器参数-1" class="headerlink" title="连接器参数"></a>连接器参数</h3><img src="/images/flgl160.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl161.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="特性-2"><a href="#特性-2" class="headerlink" title="特性"></a>特性</h3><h4 id="键处理"><a href="#键处理" class="headerlink" title="键处理"></a>键处理</h4><p>当写入数据到外部数据库时,Flink 会使用 DDL 中定义的主键.<br>如果定义了主键,则连接器将以 upsert 模式工作,否则连接器将以 append 模式工作.</p>
<p>在 upsert 模式下,Flink 将根据主键判断插入新行或者更新已存在的行,这种方式可以确保幂等性.<br>为了确保输出结果是符合预期的,推荐为表定义主键并且确保主键是底层数据库中表的唯一键或主键.<br>在 append 模式下,Flink 会把所有记录解释为 INSERT 消息,如果违反了底层数据库中主键或者唯一约束,INSERT 插入可能会失败.</p>
<p>有关 PRIMARY KEY 语法的更多详细信息,请参见 CREATE TABLE DDL.</p>
<h4 id="分区扫描"><a href="#分区扫描" class="headerlink" title="分区扫描"></a>分区扫描</h4><p>为了在并行 Source task 实例中加速读取数据,Flink 为 JDBC table 提供了分区扫描的特性.</p>
<p>如果下述分区扫描参数中的任一项被指定,则下述所有的分区扫描参数必须都被指定.<br>这些参数描述了在多个 task 并行读取数据时如何对表进行分区.<br>scan.partition.column 必须是相关表中的数字/日期或时间戳列.<br>注意,scan.partition.lower-bound 和 scan.partition.upper-bound 用于决定分区的起始位置和过滤表中的数据.<br>如果是批处理作业,也可以在提交 flink 作业之前获取最大值和最小值.</p>
<ol>
<li>scan.partition.column:输入用于进行分区的列名.</li>
<li>scan.partition.num:分区数.</li>
<li>scan.partition.lower-bound:第一个分区的最小值.</li>
<li>scan.partition.upper-bound:最后一个分区的最大值.</li>
</ol>
<h4 id="Lookup-Cache"><a href="#Lookup-Cache" class="headerlink" title="Lookup Cache"></a>Lookup Cache</h4><p>JDBC 连接器可以用在时态表关联中作为一个可 lookup 的 source (又称为维表),当前只支持同步的查找模式.</p>
<p>默认情况下,lookup cache 是未启用的,你可以设置 lookup.cache.max-rows and lookup.cache.ttl 参数来启用.</p>
<p>lookup cache 的主要目的是用于提高时态表关联 JDBC 连接器的性能.<br>默认情况下,lookup cache 不开启,所以所有请求都会发送到外部数据库.<br>当 lookup cache 被启用时,每个进程(即 TaskManager)将维护一个缓存.<br>Flink 将优先查找缓存,只有当缓存未查找到时才向外部数据库发送请求,并使用返回的数据更新缓存.<br>当缓存命中最大缓存行 lookup.cache.max-rows 或当行超过最大存活时间 lookup.cache.ttl 时,缓存中最老的行将被设置为已过期.<br>缓存中的记录可能不是最新的,用户可以将 lookup.cache.ttl 设置为一个更小的值以获得更好的刷新数据,但这可能会增加发送到数据库的请求数.<br>所以要做好吞吐量和正确性之间的平衡.</p>
<p>默认情况下,flink 会缓存主键的空查询结果,你可以通过将 lookup.cache.caching-missing-key 设置为 false 来切换行为.</p>
<h4 id="幂等写入"><a href="#幂等写入" class="headerlink" title="幂等写入"></a>幂等写入</h4><p>如果在 DDL 中定义了主键,JDBC sink 将使用 upsert 语义而不是普通的 INSERT 语句.<br>upsert 语义指的是如果底层数据库中存在违反唯一性约束,则原子地添加新行或更新现有行,这种方式确保了幂等性.</p>
<p>如果出现故障,Flink 作业会从上次成功的 checkpoint 恢复并重新处理,这可能导致在恢复过程中重复处理消息.<br>强烈推荐使用 upsert 模式,因为如果需要重复处理记录,它有助于避免违反数据库主键约束和产生重复数据.</p>
<p>除了故障恢复场景外,数据源(kafka topic)也可能随着时间的推移自然地包含多个具有相同主键的记录,这使得 upsert 模式是用户期待的.</p>
<p>由于 upsert 没有标准的语法,因此下表描述了不同数据库的 DML 语法:</p>
<ol>
<li>MySQL<br>INSERT .. ON DUPLICATE KEY UPDATE ..</li>
<li>Oracle<br>MERGE INTO .. USING (..) ON (..)<br>WHEN MATCHED THEN UPDATE SET (..)<br>WHEN NOT MATCHED THEN INSERT (..)<br>VALUES (..)</li>
<li>PostgreSQL<br>INSERT .. ON CONFLICT .. DO UPDATE SET ..</li>
</ol>
<h3 id="JDBC-Catalog"><a href="#JDBC-Catalog" class="headerlink" title="JDBC Catalog"></a>JDBC Catalog</h3><p>JdbcCatalog 允许用户通过 JDBC 协议将 Flink 连接到关系数据库.</p>
<p>目前,JDBC Catalog 有两个实现,即 Postgres Catalog 和 MySQL Catalog.<br>目前支持如下 catalog 方法.<br>其他方法目前尚不支持.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Postgres Catalog &amp; MySQL Catalog 支持的方法</span><br><span class="line">databaseExists(String databaseName);</span><br><span class="line">listDatabases();</span><br><span class="line">getDatabase(String databaseName);</span><br><span class="line">listTables(String databaseName);</span><br><span class="line">getTable(ObjectPath tablePath);</span><br><span class="line">tableExists(ObjectPath tablePath);</span><br></pre></td></tr></table></figure>

<p>其他的 Catalog 方法现在尚不支持.</p>
<h4 id="JDBC-Catalog-的使用"><a href="#JDBC-Catalog-的使用" class="headerlink" title="JDBC Catalog 的使用"></a>JDBC Catalog 的使用</h4><p>本小节主要描述如果创建并使用 Postgres Catalog 或 MySQL Catalog.<br>请参阅 Dependencies 部分了解如何配置 JDBC 连接器和相应的驱动.</p>
<p>JDBC catalog 支持以下参数:</p>
<ol>
<li>name:必填,catalog 的名称.</li>
<li>default-database:必填,默认要连接的数据库.</li>
<li>username:必填,Postgres/MySQL 账户的用户名.</li>
<li>password:必填,账户的密码.</li>
<li>base-url:必填,(不应该包含数据库名)<br>对于 Postgres Catalog base-url 应为 &quot;jdbc:postgresql://<ip>:<port>&quot; 的格式.<br>对于 MySQL Catalog base-url 应为 &quot;jdbc:mysql://<ip>:<port>&quot; 的格式.</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> CATALOG my_catalog <span class="keyword">WITH</span>(</span><br><span class="line">  <span class="string">&#x27;type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;default-database&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;username&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;password&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;base-url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">USE CATALOG my_catalog;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">String name            = <span class="string">&quot;my_catalog&quot;</span>;</span><br><span class="line">String defaultDatabase = <span class="string">&quot;mydb&quot;</span>;</span><br><span class="line">String username        = <span class="string">&quot;...&quot;</span>;</span><br><span class="line">String password        = <span class="string">&quot;...&quot;</span>;</span><br><span class="line">String baseUrl         = <span class="string">&quot;...&quot;</span></span><br><span class="line"></span><br><span class="line">JdbcCatalog catalog = <span class="keyword">new</span> JdbcCatalog(name, defaultDatabase, username, password, baseUrl);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">&quot;my_catalog&quot;</span>, catalog);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 JdbcCatalog 为会话的当前 catalog</span></span><br><span class="line">tableEnv.useCatalog(<span class="string">&quot;my_catalog&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="JDBC-Catalog-for-PostgreSQL"><a href="#JDBC-Catalog-for-PostgreSQL" class="headerlink" title="JDBC Catalog for PostgreSQL"></a>JDBC Catalog for PostgreSQL</h4><h5 id="PostgreSQL-元空间映射"><a href="#PostgreSQL-元空间映射" class="headerlink" title="PostgreSQL 元空间映射"></a>PostgreSQL 元空间映射</h5><p>除了数据库之外,postgreSQL 还有一个额外的命名空间 schema.<br>一个 Postgres 实例可以拥有多个数据库,每个数据库可以拥有多个 schema,其中一个 schema 默认名为 &quot;public&quot;,每个 schema 可以包含多张表.<br>在 Flink 中,当查询由 Postgres catalog 注册的表时,用户可以使用 schema_name.table_name 或只有 table_name,其中 schema_name 是可选的,默认值为 &quot;public&quot;.</p>
<p>因此,Flink Catalog 和 Postgres 之间的元空间映射如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink Catalog Metaspace Structure Postgres Metaspace Structure</span><br><span class="line">catalog name (defined in Flink only)  N&#x2F;A</span><br><span class="line">database name database name</span><br><span class="line">table name  [schema_name.]table_name</span><br></pre></td></tr></table></figure>

<p>Flink 中的 Postgres 表的完整路径应该是 &quot;<catalog>.<db>.<code>&lt;schema.table&gt;</code>&quot;.<br>如果指定了 schema,请注意需要转义 &lt;schema.table&gt;.</p>
<p>这里提供了一些访问 Postgres 表的例子:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 扫描 &#x27;public&#x27; schema(即默认 schema)中的 &#x27;test_table&#x27; 表,schema 名称可以省略</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mypg.mydb.test_table;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mydb.test_table;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> test_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 扫描 &#x27;custom_schema&#x27; schema 中的 &#x27;test_table2&#x27; 表,</span></span><br><span class="line"><span class="comment">-- 自定义 schema 不能省略,并且必须与表一起转义.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mypg.mydb.`custom_schema.test_table2`</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mydb.`custom_schema.test_table2`;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> `custom_schema.test_table2`;</span><br></pre></td></tr></table></figure>

<h4 id="JDBC-Catalog-for-MySQL"><a href="#JDBC-Catalog-for-MySQL" class="headerlink" title="JDBC Catalog for MySQL"></a>JDBC Catalog for MySQL</h4><h5 id="MySQL-元空间映射"><a href="#MySQL-元空间映射" class="headerlink" title="MySQL 元空间映射"></a>MySQL 元空间映射</h5><p>MySQL 实例中的数据库与 MySQL Catalog 注册的 catalog 下的数据库处于同一个映射层级.<br>一个 MySQL 实例可以拥有多个数据库,每个数据库可以包含多张表.<br>在 Flink 中,当查询由 MySQL catalog 注册的表时,用户可以使用 database.table_name 或只使用 table_name,其中 database 是可选的,默认值为创建 MySQL Catalog 时指定的默认数据库.</p>
<p>因此,Flink Catalog 和 MySQL catalog 之间的元空间映射如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink Catalog Metaspace Structure MySQL Metaspace Structure</span><br><span class="line">catalog name (defined in Flink only)  N&#x2F;A</span><br><span class="line">database name database name</span><br><span class="line">table name  table_name</span><br></pre></td></tr></table></figure>

<p>Flink 中的 MySQL 表的完整路径应该是 &quot;<code>&lt;catalog&gt;</code>.<code>&lt;db&gt;</code>.<code>&lt;table&gt;</code>&quot;.</p>
<p>这里提供了一些访问 MySQL 表的例子:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 扫描 默认数据库中的 &#x27;test_table&#x27; 表</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mysql_catalog.mydb.test_table;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mydb.test_table;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> test_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 扫描 &#x27;given_database&#x27; 数据库中的 &#x27;test_table2&#x27; 表,</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> mysql_catalog.given_database.test_table2;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> given_database.test_table2;</span><br></pre></td></tr></table></figure>

<h3 id="数据类型映射-11"><a href="#数据类型映射-11" class="headerlink" title="数据类型映射"></a>数据类型映射</h3><p>Flink 支持连接到多个使用方言(dialect)的数据库,如 MySQL/Oracle/PostgreSQL/Derby 等.<br>其中,Derby 通常是用于测试目的.<br>下表列出了从关系数据库数据类型到 Flink SQL 数据类型的类型映射,映射表可以使得在 Flink 中定义 JDBC 表更加简单.</p>
<img src="/images/flgl162.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl163.png" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="Elasticsearch-1"><a href="#Elasticsearch-1" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><p>Sink: Batch<br>Sink: Streaming Append &amp; Upsert Mode</p>
<p>Elasticsearch 连接器允许将数据写入到 Elasticsearch 引擎的索引中.<br>本文档描述运行 SQL 查询时如何设置 Elasticsearch 连接器.</p>
<p>连接器可以工作在 upsert 模式,使用 DDL 中定义的主键与外部系统交换 UPDATE/DELETE 消息.</p>
<p>如果 DDL 中没有定义主键,那么连接器只能工作在 append 模式,只能与外部系统交换 INSERT 消息.</p>
<h3 id="依赖-13"><a href="#依赖-13" class="headerlink" title="依赖"></a>依赖</h3><p>6.x</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>7.x and later versions  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch7<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="如何创建-Elasticsearch-表"><a href="#如何创建-Elasticsearch-表" class="headerlink" title="如何创建 Elasticsearch 表"></a>如何创建 Elasticsearch 表</h3><p>以下示例展示了如何创建 Elasticsearch sink 表:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> myUserTable (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  user_name STRING,</span><br><span class="line">  uv <span class="type">BIGINT</span>,</span><br><span class="line">  pv <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (user_id) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;elasticsearch-7&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hosts&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;http://localhost:9200&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;index&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;users&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="连接器参数-2"><a href="#连接器参数-2" class="headerlink" title="连接器参数"></a>连接器参数</h3><img src="/images/flgl164.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl165.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl166.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="特性-3"><a href="#特性-3" class="headerlink" title="特性"></a>特性</h3><h4 id="Key-处理"><a href="#Key-处理" class="headerlink" title="Key 处理"></a>Key 处理</h4><p>Elasticsearch sink 可以根据是否定义了一个主键来确定是在 upsert 模式还是 append 模式下工作.<br>如果定义了主键,Elasticsearch sink 将以 upsert 模式工作,该模式可以消费包含 UPDATE/DELETE 消息的查询.<br>如果未定义主键,Elasticsearch sink 将以 append 模式工作,该模式只能消费包含 INSERT 消息的查询.</p>
<p>在 Elasticsearch 连接器中,主键用于计算 Elasticsearch 的文档 id,文档 id 为最多 512 字节且不包含空格的字符串.<br>Elasticsearch 连接器通过使用 document-id.key-delimiter 指定的键分隔符按照 DDL 中定义的顺序连接所有主键字段,为每一行记录生成一个文档 ID 字符串.<br>某些类型不允许作为主键字段,因为它们没有对应的字符串表示形式,例如,BYTES,ROW,ARRAY,MAP 等.<br>如果未指定主键,Elasticsearch 将自动生成文档 id.</p>
<p>有关 PRIMARY KEY 语法的更多详细信息,请参见 CREATE TABLE DDL.</p>
<h4 id="动态索引"><a href="#动态索引" class="headerlink" title="动态索引"></a>动态索引</h4><p>Elasticsearch sink 同时支持静态索引和动态索引.</p>
<p>如果你想使用静态索引,则 index 选项值应为纯字符串,例如 &#39;myusers&#39;,所有记录都将被写入到 &quot;myusers&quot; 索引中.</p>
<p>如果你想使用动态索引,你可以使用 {field_name} 来引用记录中的字段值来动态生成目标索引.<br>你也可以使用 &#39;{field_name|date_format_string}&#39; 将 TIMESTAMP/DATE/TIME 类型的字段值转换为 date_format_string 指定的格式.<br>date_format_string 与 Java 的 DateTimeFormatter 兼容.<br>例如,如果选项值设置为 &#39;myusers-{log_ts|yyyy-MM-dd}&#39;,则 log_ts 字段值为 2020-03-27 12:25:55 的记录将被写入到 &quot;myusers-2020-03-27&quot; 索引中.</p>
<p>你也可以使用 &#39;{now()|date_format_string}&#39; 将当前的系统时间转换为 date_format_string 指定的格式.<br>now() 对应的时间类型是 TIMESTAMP_WITH_LTZ .<br>在将系统时间格式化为字符串时会使用 session 中通过 table.local-time-zone 中配置的时区.<br>使用 NOW(), now(), CURRENT_TIMESTAMP, current_timestamp 均可以.</p>
<p>注意: 使用当前系统时间生成的动态索引时, 对于 changelog 的流,无法保证同一主键对应的记录能产生相同的索引名, 因此使用基于系统时间的动态索引,只能支持 append only 的流.</p>
<h3 id="数据类型映射-12"><a href="#数据类型映射-12" class="headerlink" title="数据类型映射"></a>数据类型映射</h3><p>Elasticsearch 将文档存储在 JSON 字符串中.<br>因此数据类型映射介于 Flink 数据类型和 JSON 数据类型之间.<br>Flink 为 Elasticsearch 连接器使用内置的 &#39;json&#39; 格式.<br>更多类型映射的详细信息,请参阅 JSON Format 页面.</p>
<h2 id="文件系统-1"><a href="#文件系统-1" class="headerlink" title="文件系统"></a>文件系统</h2><p>此连接器提供了对 Flink FileSystem abstraction 支持的文件系统中分区文件的访问.</p>
<p>在 Flink 中包含了该文件系统连接器,不需要添加额外的依赖.<br>相应的 jar 包可以在 Flink 工程项目的 /lib 目录下找到.<br>从文件系统中读取或者向文件系统中写入行时,需要指定相应的 format.</p>
<p>文件系统连接器允许从本地或分布式文件系统进行读写.<br>文件系统表可以定义为:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyUserTable (</span><br><span class="line">  column_name1 <span class="type">INT</span>,</span><br><span class="line">  column_name2 STRING,</span><br><span class="line">  ...</span><br><span class="line">  part_name1 <span class="type">INT</span>,</span><br><span class="line">  part_name2 STRING</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (part_name1, part_name2) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;filesystem&#x27;</span>,           <span class="comment">-- 必选:指定连接器类型</span></span><br><span class="line">  <span class="string">&#x27;path&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;file:///path/to/whatever&#x27;</span>,  <span class="comment">-- 必选:指定路径</span></span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,                     <span class="comment">-- 必选:文件系统连接器指定 format</span></span><br><span class="line">                                        <span class="comment">-- 有关更多详情,请参考 Table Formats</span></span><br><span class="line">  <span class="string">&#x27;partition.default-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,     <span class="comment">-- 可选:默认的分区名,动态分区模式下分区字段值是 null 或空字符串</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 可选:该属性开启了在 sink 阶段通过动态分区字段来 shuffle 数据,该功能可以大大减少文件系统 sink 的文件数,但是可能会导致数据倾斜,默认值是 false</span></span><br><span class="line">  <span class="string">&#x27;sink.shuffle-by-partition.enable&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>请确保包含 Flink File System specific dependencies.</p>
<p>基于流的文件系统 sources 仍在开发中.<br>未来,社区将增加对常见地流式用例的支持,例如,对分区和目录的监控等.</p>
<p>文件系统连接器的特性与 previous legacy filesystem connector 有很大不同: path 属性指定的是目录,而不是文件,该目录下的文件也不是肉眼可读的.</p>
<h3 id="分区文件"><a href="#分区文件" class="headerlink" title="分区文件"></a>分区文件</h3><p>Flink 的文件系统连接器支持分区,使用了标准的 hive.<br>但是,不需要预先注册分区到 table catalog,而是基于目录结构自动做了分区发现.<br>例如,根据下面的目录结构,分区表将被推断包含 datetime 和 hour 分区.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── datetime&#x3D;2019-08-25</span><br><span class="line">    └── hour&#x3D;11</span><br><span class="line">        ├── part-0.parquet</span><br><span class="line">        ├── part-1.parquet</span><br><span class="line">    └── hour&#x3D;12</span><br><span class="line">        ├── part-0.parquet</span><br><span class="line">└── datetime&#x3D;2019-08-26</span><br><span class="line">    └── hour&#x3D;6</span><br><span class="line">        ├── part-0.parquet</span><br></pre></td></tr></table></figure>

<p>文件系统表支持分区新增插入和分区覆盖插入.<br>请参考 INSERT Statement.<br>当对分区表进行分区覆盖插入时,只有相应的分区会被覆盖,而不是整个表.</p>
<h3 id="File-Formats"><a href="#File-Formats" class="headerlink" title="File Formats"></a>File Formats</h3><p>文件系统连接器支持多种 format:</p>
<ol>
<li>CSV:RFC-4180.<br>是非压缩的.</li>
<li>JSON:注意,文件系统连接器的 JSON format 与传统的标准的 JSON file 的不同,而是非压缩的.<br>换行符分割的 JSON.</li>
<li>Avro:Apache Avro.<br>通过配置 avro.codec 属性支持压缩.</li>
<li>Parquet:Apache Parquet.<br>兼容 hive.</li>
<li>Orc:Apache Orc.<br>兼容 hive.</li>
<li>Debezium-JSON:debezium-json.</li>
<li>Canal-JSON:canal-json.</li>
<li>Raw:raw.</li>
</ol>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>文件系统连接器可用于将单个文件或整个目录的数据读取到单个表中.</p>
<p>当使用目录作为 source 路径时,对目录中的文件进行 无序的读取.</p>
<h4 id="目录监控"><a href="#目录监控" class="headerlink" title="目录监控"></a>目录监控</h4><p>当运行模式为流模式时,文件系统连接器会自动监控输入目录.<br>可以使用以下属性修改监控时间间隔.</p>
<img src="/images/flgl167.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="可用的-Metadata"><a href="#可用的-Metadata" class="headerlink" title="可用的 Metadata"></a>可用的 Metadata</h4><p>以下连接器 metadata 可以在表定义时作为 metadata 列进行访问.<br>所有 metadata 都是只读的.</p>
<img src="/images/flgl168.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>扩展的 CREATE TABLE 示例演示了标识某个字段为 metadata 的语法:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyUserTableWithFilepath (</span><br><span class="line">  column_name1 <span class="type">INT</span>,</span><br><span class="line">  column_name2 STRING,</span><br><span class="line">  `file.path` STRING <span class="keyword">NOT</span> <span class="keyword">NULL</span> METADATA</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;filesystem&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;path&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;file:///path/to/whatever&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="Streaming-Sink"><a href="#Streaming-Sink" class="headerlink" title="Streaming Sink"></a>Streaming Sink</h3><p>文件系统连接器支持流写入,是基于 Flink 的 文件系统 写入文件的.<br>CSV 和 JSON 使用的是 Row-encoded Format.<br>Parquet/ORC 和 Avro 使用的是 Bulk-encoded Format.</p>
<p>可以直接编写 SQL,将流数据插入到非分区表.<br>如果是分区表,可以配置分区操作相关的属性.<br>请参考分区提交了解更多详情.</p>
<h4 id="滚动策略-1"><a href="#滚动策略-1" class="headerlink" title="滚动策略"></a>滚动策略</h4><p>分区目录下的数据被分割到 part 文件中.<br>每个分区对应的 sink 的收到的数据的 subtask 都至少会为该分区生成一个 part 文件.<br>根据可配置的滚动策略,当前 in-progress part 文件将被关闭,生成新的 part 文件.<br>该策略基于大小,和指定的文件可被打开的最大 timeout 时长,来滚动 part 文件.</p>
<img src="/images/flgl169.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>注意: 对于 bulk formats 数据 (parquet/orc/avro),滚动策略与 checkpoint 间隔(pending 状态的文件会在下个 checkpoint 完成)控制了 part 文件的大小和个数.</p>
<p>注意: 对于 row formats 数据 (csv/json),如果想使得分区文件更快在文件系统中可见,可以设置 sink.rolling-policy.file-size 或 sink.rolling-policy.rollover-interval 属性以及在 flink-conf.yaml 中的 execution.checkpointing.interval 属性.<br>对于其他 formats (avro/orc),可以只设置 flink-conf.yaml 中的 execution.checkpointing.interval 属性.</p>
<h4 id="文件合并-1"><a href="#文件合并-1" class="headerlink" title="文件合并"></a>文件合并</h4><p>file sink 支持文件合并,允许应用程序使用较小的 checkpoint 间隔而不产生大量小文件.</p>
<img src="/images/flgl170.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>如果启用文件合并功能,会根据目标文件大小,将多个小文件合并成大文件.<br>在生产环境中使用文件合并功能时,需要注意:</p>
<ol>
<li>只有 checkpoint 内部的文件才会被合并,至少生成的文件个数与 checkpoint 个数相同.</li>
<li>合并前文件是不可见的,那么文件的可见时间是:checkpoint 间隔时长 + 合并时长.</li>
<li>如果合并时间过长,将导致反压,延长 checkpoint 所需时间.</li>
</ol>
<h4 id="分区提交"><a href="#分区提交" class="headerlink" title="分区提交"></a>分区提交</h4><p>数据写入分区之后,通常需要通知下游应用.<br>例如,在 hive metadata 中新增分区或者在目录下生成 <code>_SUCCESS</code> 文件.<br>分区提交策略是可定制的.<br>具体分区提交行为是基于 triggers 和 policies 的组合.</p>
<ol>
<li>Trigger:分区提交时机,可以基于从分区中提取的时间对应的 watermark,或者基于处理时间.</li>
<li>Policy:分区提交策略,内置策略包括生成 <code>_SUCCESS</code> 文件和提交 hive metastore,也可以实现自定义策略,例如触发 hive 生成统计信息,合并小文件等.</li>
</ol>
<p>注意: 分区提交仅在动态分区插入模式下才有效.</p>
<h5 id="分区提交触发器"><a href="#分区提交触发器" class="headerlink" title="分区提交触发器"></a>分区提交触发器</h5><p>通过配置分区提交触发策略,来决定何时提交分区:</p>
<img src="/images/flgl171.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>Flink 提供了两种类型分区提交触发器:<br>第一种是根据分区的处理时间.<br>既不需要额外的分区时间,也不需要 watermark 生成.<br>这种分区提交触发器基于分区创建时间和当前系统时间.<br>这种触发器更具通用性,但不是很精确.<br>例如,数据延迟或故障将导致过早提交分区.</p>
<p>第二种是根据从分区字段提取的时间以及 watermark.<br>这需要 job 支持 watermark 生成,分区是根据时间来切割的,例如,按小时或按天分区.</p>
<p>不管分区数据是否完整而只想让下游尽快感知到分区:<br>&#39;sink.partition-commit.trigger&#39;=&#39;process-time&#39; (默认值)<br>&#39;sink.partition-commit.delay&#39;=&#39;0s&#39; (默认值) 一旦数据进入分区,将立即提交分区.<br>注意:这个分区可能会被提交多次.</p>
<p>如果想让下游只有在分区数据完整时才感知到分区,并且 job 中有 watermark 生成,也能从分区字段的值中提取到时间:<br>&#39;sink.partition-commit.trigger&#39;=&#39;partition-time&#39;<br>&#39;sink.partition-commit.delay&#39;=&#39;1h&#39; (根据分区类型指定,如果是按小时分区可配置为 &#39;1h&#39;) 该方式是最精准地提交分区的方式,尽力确保提交分区的数据完整.</p>
<p>如果想让下游系统只有在数据完整时才感知到分区,但是没有 watermark,或者无法从分区字段的值中提取时间:<br>&#39;sink.partition-commit.trigger&#39;=&#39;process-time&#39; (默认值)<br>&#39;sink.partition-commit.delay&#39;=&#39;1h&#39; (根据分区类型指定,如果是按小时分区可配置为 &#39;1h&#39;) 该方式尽量精确地提交分区,但是数据延迟或者故障将导致过早提交分区.</p>
<p>延迟数据的处理:延迟的记录会被写入到已经提交的对应分区中,且会再次触发该分区的提交.</p>
<h5 id="分区时间提取器"><a href="#分区时间提取器" class="headerlink" title="分区时间提取器"></a>分区时间提取器</h5><p>时间提取器从分区字段值中提取时间.</p>
<img src="/images/flgl172.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>默认情况下,提取器基于由分区字段组成的 timestamp pattern.<br>也可以指定一个实现接口 PartitionTimeExtractor 的自定义提取器.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HourPartTimeExtractor</span> <span class="keyword">implements</span> <span class="title">PartitionTimeExtractor</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LocalDateTime <span class="title">extract</span><span class="params">(List&lt;String&gt; keys, List&lt;String&gt; values)</span> </span>&#123;</span><br><span class="line">        String dt = values.get(<span class="number">0</span>);</span><br><span class="line">        String hour = values.get(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> Timestamp.valueOf(dt + <span class="string">&quot; &quot;</span> + hour + <span class="string">&quot;:00:00&quot;</span>).toLocalDateTime();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="分区提交策略"><a href="#分区提交策略" class="headerlink" title="分区提交策略"></a>分区提交策略</h5><p>分区提交策略定义了提交分区时的具体操作.<br>第一种是 metadata 存储(metastore),仅 hive 表支持该策略,该策略下文件系统通过目录层次结构来管理分区.<br>第二种是 success 文件,该策略下会在分区对应的目录下生成一个名为 <code>_SUCCESS</code> 的空文件.</p>
<img src="/images/flgl173.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>也可以自定义提交策略,例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AnalysisCommitPolicy</span> <span class="keyword">implements</span> <span class="title">PartitionCommitPolicy</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> HiveShell hiveShell;</span><br><span class="line">  </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commit</span><span class="params">(Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (hiveShell == <span class="keyword">null</span>) &#123;</span><br><span class="line">          hiveShell = createHiveShell(context.catalogName());</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">        hiveShell.execute(String.format(</span><br><span class="line">            <span class="string">&quot;ALTER TABLE %s ADD IF NOT EXISTS PARTITION (%s = &#x27;%s&#x27;) location &#x27;%s&#x27;&quot;</span>,</span><br><span class="line">          context.tableName(),</span><br><span class="line">          context.partitionKeys().get(<span class="number">0</span>),</span><br><span class="line">          context.partitionValues().get(<span class="number">0</span>),</span><br><span class="line">          context.partitionPath()));</span><br><span class="line">      hiveShell.execute(String.format(</span><br><span class="line">          <span class="string">&quot;ANALYZE TABLE %s PARTITION (%s = &#x27;%s&#x27;) COMPUTE STATISTICS FOR COLUMNS&quot;</span>,</span><br><span class="line">          context.tableName(),</span><br><span class="line">          context.partitionKeys().get(<span class="number">0</span>),</span><br><span class="line">          context.partitionValues().get(<span class="number">0</span>)));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Sink-Parallelism"><a href="#Sink-Parallelism" class="headerlink" title="Sink Parallelism"></a>Sink Parallelism</h3><p>在流模式和批模式下,向外部文件系统(包括 hive)写文件时的 parallelism 可以通过相应的 table 配置项指定.<br>默认情况下,该 sink parallelism 与上游 chained operator 的 parallelism 一样.<br>当配置了跟上游的 chained operator 不一样的 parallelism 时,写文件和合并文件的算子(如果开启的话)会使用指定的 sink parallelism.</p>
<img src="/images/flgl174.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>注意: 目前,当且仅当上游的 changelog 模式为 INSERT-ONLY 时,才支持配置 sink parallelism.<br>否则,程序将会抛出异常.</p>
<h3 id="完整示例-1"><a href="#完整示例-1" class="headerlink" title="完整示例"></a>完整示例</h3><p>以下示例展示了如何使用文件系统连接器编写流式查询语句,将数据从 Kafka 写入文件系统,然后运行批式查询语句读取数据.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  log_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> log_ts <span class="keyword">AS</span> log_ts <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> fs_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  dt STRING,</span><br><span class="line">  `<span class="keyword">hour</span>` STRING</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt, `<span class="keyword">hour</span>`) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span><span class="operator">=</span><span class="string">&#x27;filesystem&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;path&#x27;</span><span class="operator">=</span><span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span><span class="operator">=</span><span class="string">&#x27;parquet&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.delay&#x27;</span><span class="operator">=</span><span class="string">&#x27;1 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span><span class="operator">=</span><span class="string">&#x27;success-file&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 流式 sql,插入文件系统表</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> fs_table </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    user_id, </span><br><span class="line">    order_amount, </span><br><span class="line">    DATE_FORMAT(log_ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>),</span><br><span class="line">    DATE_FORMAT(log_ts, <span class="string">&#x27;HH&#x27;</span>) </span><br><span class="line"><span class="keyword">FROM</span> kafka_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 批式 sql,使用分区修剪进行选择</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> fs_table <span class="keyword">WHERE</span> dt<span class="operator">=</span><span class="string">&#x27;2020-05-20&#x27;</span> <span class="keyword">and</span> `<span class="keyword">hour</span>`<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>如果 watermark 被定义在 TIMESTAMP_LTZ 类型的列上并且使用 partition-time 模式进行提交,sink.partition-commit.watermark-time-zone 这个属性需要设置成会话时区,否则分区提交可能会延迟若干个小时.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  ts <span class="type">BIGINT</span>, <span class="comment">-- 以毫秒为单位的时间</span></span><br><span class="line">  ts_ltz <span class="keyword">AS</span> TO_TIMESTAMP_LTZ(ts, <span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_ltz <span class="keyword">AS</span> ts_ltz <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> <span class="comment">-- 在 TIMESTAMP_LTZ 列上定义 watermark</span></span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> fs_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  dt STRING,</span><br><span class="line">  `<span class="keyword">hour</span>` STRING</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt, `<span class="keyword">hour</span>`) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span><span class="operator">=</span><span class="string">&#x27;filesystem&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;path&#x27;</span><span class="operator">=</span><span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span><span class="operator">=</span><span class="string">&#x27;parquet&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span><span class="operator">=</span><span class="string">&#x27;$dt $hour:00:00&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.delay&#x27;</span><span class="operator">=</span><span class="string">&#x27;1 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.trigger&#x27;</span><span class="operator">=</span><span class="string">&#x27;partition-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.watermark-time-zone&#x27;</span><span class="operator">=</span><span class="string">&#x27;Asia/Shanghai&#x27;</span>, <span class="comment">-- 假设用户配置的时区为 &#x27;Asia/Shanghai&#x27;</span></span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span><span class="operator">=</span><span class="string">&#x27;success-file&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 流式 sql,插入文件系统表</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> fs_table </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    user_id, </span><br><span class="line">    order_amount, </span><br><span class="line">    DATE_FORMAT(ts_ltz, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>),</span><br><span class="line">    DATE_FORMAT(ts_ltz, <span class="string">&#x27;HH&#x27;</span>) </span><br><span class="line"><span class="keyword">FROM</span> kafka_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 批式 sql,使用分区修剪进行选择</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> fs_table <span class="keyword">WHERE</span> dt<span class="operator">=</span><span class="string">&#x27;2020-05-20&#x27;</span> <span class="keyword">and</span> `<span class="keyword">hour</span>`<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>Scan Source: Bounded<br>Lookup Source: Sync Mode<br>Sink: Batch<br>Sink: Streaming Upsert Mode</p>
<p>HBase 连接器支持读取和写入 HBase 集群.<br>本文档介绍如何使用 HBase 连接器基于 HBase 进行 SQL 查询.</p>
<p>HBase 连接器在 upsert 模式下运行,可以使用 DDL 中定义的主键与外部系统交换更新操作消息.<br>但是主键只能基于 HBase 的 rowkey 字段定义.<br>如果没有声明主键,HBase 连接器默认取 rowkey 作为主键.</p>
<h3 id="依赖-14"><a href="#依赖-14" class="headerlink" title="依赖"></a>依赖</h3><p>1.4.x</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hbase-1.4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2.2.x </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hbase-2.2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="如何使用-HBase-表"><a href="#如何使用-HBase-表" class="headerlink" title="如何使用 HBase 表"></a>如何使用 HBase 表</h3><p>所有 HBase 表的列簇必须定义为 ROW 类型,字段名对应列簇名(column family),嵌套的字段名对应列限定符名(column qualifier).<br>用户只需在表结构中声明查询中使用的的列簇和列限定符.<br>除了 ROW 类型的列,剩下的原子数据类型字段(比如,STRING, BIGINT)将被识别为 HBase 的 rowkey,一张表中只能声明一个 rowkey.<br>rowkey 字段的名字可以是任意的,如果是保留关键字,需要用反引号.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 在 Flink SQL 中注册 HBase 表 &quot;mytable&quot;</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hTable (</span><br><span class="line"> rowkey <span class="type">INT</span>,</span><br><span class="line"> family1 <span class="type">ROW</span><span class="operator">&lt;</span>q1 <span class="type">INT</span><span class="operator">&gt;</span>,</span><br><span class="line"> family2 <span class="type">ROW</span><span class="operator">&lt;</span>q2 STRING, q3 <span class="type">BIGINT</span><span class="operator">&gt;</span>,</span><br><span class="line"> family3 <span class="type">ROW</span><span class="operator">&lt;</span>q4 <span class="keyword">DOUBLE</span>, q5 <span class="type">BOOLEAN</span>, q6 STRING<span class="operator">&gt;</span>,</span><br><span class="line"> <span class="keyword">PRIMARY</span> KEY (rowkey) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;hbase-1.4&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;table-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;mytable&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;zookeeper.quorum&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:2181&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 用 ROW(...) 构造函数构造列簇,并往 HBase 表写数据.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 假设 &quot;T&quot; 的表结构是 [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6]</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> hTable</span><br><span class="line"><span class="keyword">SELECT</span> rowkey, <span class="type">ROW</span>(f1q1), <span class="type">ROW</span>(f2q2, f2q3), <span class="type">ROW</span>(f3q4, f3q5, f3q6) <span class="keyword">FROM</span> T;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 从 HBase 表扫描数据</span></span><br><span class="line"><span class="keyword">SELECT</span> rowkey, family1, family3.q4, family3.q6 <span class="keyword">FROM</span> hTable;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- temporal join HBase 表,将 HBase 表作为维表</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> myTopic</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> hTable <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> myTopic.proctime</span><br><span class="line"><span class="keyword">ON</span> myTopic.key <span class="operator">=</span> hTable.rowkey;</span><br></pre></td></tr></table></figure>

<h3 id="连接器参数-3"><a href="#连接器参数-3" class="headerlink" title="连接器参数"></a>连接器参数</h3><img src="/images/flgl175.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/flgl176.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="数据类型映射表"><a href="#数据类型映射表" class="headerlink" title="数据类型映射表"></a>数据类型映射表</h3><p>HBase 以字节数组存储所有数据.<br>在读和写过程中要序列化和反序列化数据.</p>
<p>Flink 的 HBase 连接器利用 HBase(Hadoop) 的工具类 org.apache.hadoop.hbase.util.Bytes 进行字节数组和 Flink 数据类型转换.</p>
<p>Flink 的 HBase 连接器将所有数据类型(除字符串外)null 值编码成空字节.<br>对于字符串类型,null 值的字面值由null-string-literal选项值决定.</p>
<p>数据类型映射表如下:</p>
<table>
<thead>
<tr>
<th align="left">Flink 数据类型</th>
<th align="left">HBase 转换</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR / VARCHAR / STRING</td>
<td align="left">byte[] toBytes(String s)<br>String toString(byte[] b)</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">byte[] toBytes(boolean b)<br>boolean toBoolean(byte[] b)</td>
</tr>
<tr>
<td align="left">BINARY / VARBINARY</td>
<td align="left">返回byte[]</td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">byte[] toBytes(BigDecimal v)<br>BigDecimal toBigDecimal(byte[] b)</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">new byte[] { val }<br>bytes[0] // returns first and only byte from bytes</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">byte[] toBytes(short val)<br>short toShort(byte[] bytes)</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">byte[] toBytes(int val)<br>int toInt(byte[] bytes)</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">byte[] toBytes(long val)<br>long toLong(byte[] bytes)</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">byte[] toBytes(float val)<br>float toFloat(byte[] bytes)</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">byte[] toBytes(double val)<br>double toDouble(byte[] bytes)</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">从 1970-01-01 00:00:00 UTC 开始的天数,int 值</td>
</tr>
<tr>
<td align="left">TIME</td>
<td align="left">从 1970-01-01 00:00:00 UTC 开始天的毫秒数,int 值</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">从 1970-01-01 00:00:00 UTC 开始的毫秒数,long 值</td>
</tr>
<tr>
<td align="left">ARRAY</td>
<td align="left">不支持</td>
</tr>
<tr>
<td align="left">MAP / MULTISET</td>
<td align="left">不支持</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">不支持</td>
</tr>
</tbody></table>
<h2 id="DataGen"><a href="#DataGen" class="headerlink" title="DataGen"></a>DataGen</h2><h2 id="Print"><a href="#Print" class="headerlink" title="Print"></a>Print</h2><p>Sink</p>
<p>Print 连接器允许将每一行写入标准输出流或者标准错误流.<br>设计目的:</p>
<ol>
<li>简单的流作业测试.</li>
<li>对生产调试带来极大便利.</li>
</ol>
<p>四种 format 选项:</p>
<table>
<thead>
<tr>
<th align="left">打印内容</th>
<th align="left">条件 1</th>
<th align="left">条件 2</th>
</tr>
</thead>
<tbody><tr>
<td align="left">标识符:任务 ID&gt; 输出数据</td>
<td align="left">需要提供前缀打印标识符</td>
<td align="left">parallelism &gt; 1</td>
</tr>
<tr>
<td align="left">标识符&gt; 输出数据</td>
<td align="left">需要提供前缀打印标识符</td>
<td align="left">parallelism == 1</td>
</tr>
<tr>
<td align="left">任务 ID&gt; 输出数据</td>
<td align="left">不需要提供前缀打印标识符</td>
<td align="left">parallelism &gt; 1</td>
</tr>
<tr>
<td align="left">输出数据</td>
<td align="left">不需要提供前缀打印标识符</td>
<td align="left">parallelism == 1</td>
</tr>
</tbody></table>
<p>输出字符串格式为 &quot;$row_kind(f0,f1,f2…)&quot;,row_kind是一个 RowKind 类型的短字符串,例如:&quot;+I(1,1)&quot;.</p>
<p>Print 连接器是内置的.</p>
<p>注意 在任务运行时使用 Print Sinks 打印记录,你需要注意观察任务日志.</p>
<h3 id="如何创建一张基于-Print-的表"><a href="#如何创建一张基于-Print-的表" class="headerlink" title="如何创建一张基于 Print 的表"></a>如何创建一张基于 Print 的表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> print_table (</span><br><span class="line"> f0 <span class="type">INT</span>,</span><br><span class="line"> f1 <span class="type">INT</span>,</span><br><span class="line"> f2 STRING,</span><br><span class="line"> f3 <span class="keyword">DOUBLE</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>或者,也可以通过 LIKE子句 基于已有表的结构去创建新表.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> print_table <span class="keyword">WITH</span> (<span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span>)</span><br><span class="line"><span class="keyword">LIKE</span> source_table (EXCLUDING <span class="keyword">ALL</span>)</span><br></pre></td></tr></table></figure>

<h3 id="连接器参数-4"><a href="#连接器参数-4" class="headerlink" title="连接器参数"></a>连接器参数</h3><img src="/images/flgl177.png" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="BlackHole"><a href="#BlackHole" class="headerlink" title="BlackHole"></a>BlackHole</h2><p>Sink: Bounded<br>Sink: UnBounded</p>
<p>BlackHole 连接器允许接收所有输入记录.<br>它被设计用于:</p>
<ol>
<li>高性能测试.</li>
<li>UDF 输出,而不是实质性 sink.</li>
</ol>
<p>就像类 Unix 操作系统上的 /dev/null.</p>
<p>BlackHole 连接器是内置的.</p>
<h3 id="如何创建-BlackHole-表"><a href="#如何创建-BlackHole-表" class="headerlink" title="如何创建 BlackHole 表"></a>如何创建 BlackHole 表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> blackhole_table (</span><br><span class="line">  f0 <span class="type">INT</span>,</span><br><span class="line">  f1 <span class="type">INT</span>,</span><br><span class="line">  f2 STRING,</span><br><span class="line">  f3 <span class="keyword">DOUBLE</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;blackhole&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>或者,可以基于现有模式使用 LIKE 子句 创建.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> blackhole_table <span class="keyword">WITH</span> (<span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;blackhole&#x27;</span>)</span><br><span class="line"><span class="keyword">LIKE</span> source_table (EXCLUDING <span class="keyword">ALL</span>)</span><br></pre></td></tr></table></figure>

<h3 id="连接器选项"><a href="#连接器选项" class="headerlink" title="连接器选项"></a>连接器选项</h3><img src="/images/flgl178.png" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="概览-2"><a href="#概览-2" class="headerlink" title="概览"></a>概览</h3><p>Apache Hive 已经成为了数据仓库生态系统中的核心.<br>它不仅仅是一个用于大数据分析和ETL场景的SQL引擎,同样它也是一个数据管理平台,可用于发现,定义,和演化数据.</p>
<p>Flink 与 Hive 的集成包含两个层面:<br>一是利用了 Hive 的 MetaStore 作为持久化的 Catalog,用户可通过HiveCatalog将不同会话中的 Flink 元数据存储到 Hive Metastore 中.<br>例如,用户可以使用HiveCatalog将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中,并后续在 SQL 查询中重新使用它们.</p>
<p>二是利用 Flink 来读写 Hive 的表.</p>
<p>HiveCatalog的设计提供了与 Hive 良好的兼容性,用户可以&quot;开箱即用&quot;的访问其已有的 Hive 数仓.<br>您不需要修改现有的 Hive Metastore,也不需要更改表的数据位置或分区.</p>
<h4 id="支持的Hive版本"><a href="#支持的Hive版本" class="headerlink" title="支持的Hive版本"></a>支持的Hive版本</h4><p>Flink 支持一下的 Hive 版本.<br>1.0<br>1.0.0<br>1.0.1</p>
<p>1.1<br>1.1.0<br>1.1.1</p>
<p>1.2<br>1.2.0<br>1.2.1<br>1.2.2</p>
<p>2.0<br>2.0.0<br>2.0.1</p>
<p>2.1<br>2.1.0<br>2.1.1</p>
<p>2.2<br>2.2.0</p>
<p>2.3<br>2.3.0<br>2.3.1<br>2.3.2<br>2.3.3<br>2.3.4<br>2.3.5<br>2.3.6</p>
<p>3.1<br>3.1.0<br>3.1.1<br>3.1.2</p>
<p>请注意,某些功能是否可用取决于您使用的 Hive 版本,这些限制不是由 Flink 所引起的:</p>
<ol>
<li>Hive 内置函数在使用 Hive-1.2.0 及更高版本时支持.</li>
<li>列约束,也就是 PRIMARY KEY 和 NOT NULL,在使用 Hive-3.1.0 及更高版本时支持.</li>
<li>更改表的统计信息,在使用 Hive-1.2.0 及更高版本时支持.</li>
<li>DATE列统计信息,在使用 Hive-1.2.0 及更高版时支持.</li>
<li>使用 Hive-2.0.x 版本时不支持写入 ORC 表.</li>
</ol>
<h5 id="依赖项-1"><a href="#依赖项-1" class="headerlink" title="依赖项"></a>依赖项</h5><p>要与 Hive 集成,您需要在 Flink 下的/lib/目录中添加一些额外的依赖包, 以便通过 Table API 或 SQL Client 与 Hive 进行交互.<br>或者,您可以将这些依赖项放在专用文件夹中,并分别使用 Table API 程序或 SQL Client 的-C或-l选项将它们添加到 classpath 中.</p>
<p>Apache Hive 是基于 Hadoop 之上构建的, 首先您需要 Hadoop 的依赖,请参考 Providing Hadoop classes:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH&#x3D;hadoop classpath</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;$($&#123;HADOOP_HOME&#125;&#x2F;sbin&#x2F;hadoop classpath)</span><br></pre></td></tr></table></figure>

<p>有两种添加 Hive 依赖项的方法.<br>第一种是使用 Flink 提供的 Hive Jar包.<br>您可以根据使用的 Metastore 的版本来选择对应的 Hive jar.<br>第二个方式是分别添加每个所需的 jar 包.<br>如果您使用的 Hive 版本尚未在此处列出,则第二种方法会更适合.</p>
<p>注意:建议您优先使用 Flink 提供的 Hive jar 包.<br>仅在 Flink 提供的 Hive jar 不满足您的需求时,再考虑使用分开添加 jar 包的方式.</p>
<p>使用 Flink 提供的 Hive jar<br>下表列出了所有可用的 Hive jar.<br>您可以选择一个并放在 Flink 发行版的/lib/ 目录中.</p>
<p>用户定义的依赖项<br>您可以在下方找到不同Hive主版本所需要的依赖项.</p>
<h5 id="Maven-依赖"><a href="#Maven-依赖" class="headerlink" title="Maven 依赖"></a>Maven 依赖</h5><p>如果您在构建自己的应用程序,则需要在 mvn 文件中添加以下依赖项.<br>您应该在运行时添加以上的这些依赖项,而不要在已生成的 jar 文件中去包含它们.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="连接到Hive"><a href="#连接到Hive" class="headerlink" title="连接到Hive"></a>连接到Hive</h4><p>通过 TableEnvironment 或者 YAML 配置,使用 Catalog 接口 和 HiveCatalog连接到现有的 Hive 集群.</p>
<p>以下是如何连接到 Hive 的示例:</p>
<h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">String name            = <span class="string">&quot;myhive&quot;</span>;</span><br><span class="line">String defaultDatabase = <span class="string">&quot;mydatabase&quot;</span>;</span><br><span class="line">String hiveConfDir     = <span class="string">&quot;/opt/hive-conf&quot;</span>;</span><br><span class="line"></span><br><span class="line">HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">&quot;myhive&quot;</span>, hive);</span><br><span class="line"></span><br><span class="line"><span class="comment">// set the HiveCatalog as the current catalog of the session</span></span><br><span class="line">tableEnv.useCatalog(<span class="string">&quot;myhive&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="sql"><a href="#sql" class="headerlink" title="sql"></a>sql</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> CATALOG myhive <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;hive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;default-database&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;mydatabase&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;hive-conf-dir&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/opt/hive-conf&#x27;</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- set the HiveCatalog as the current catalog of the session</span></span><br><span class="line">USE CATALOG myhive;</span><br></pre></td></tr></table></figure>

<h5 id="yaml"><a href="#yaml" class="headerlink" title="yaml"></a>yaml</h5><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">execution:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">current-catalog:</span> <span class="string">myhive</span>  <span class="comment"># set the HiveCatalog as the current catalog of the session</span></span><br><span class="line">  <span class="attr">current-database:</span> <span class="string">mydatabase</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">catalogs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myhive</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">hive</span></span><br><span class="line">    <span class="attr">hive-conf-dir:</span> <span class="string">/opt/hive-conf</span></span><br></pre></td></tr></table></figure>

<p>下表列出了通过 YAML 文件或 DDL 定义 HiveCatalog 时所支持的参数.</p>
<img src="/images/flgl179.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h4><p>在 Flink 中执行 DDL 操作 Hive 的表/视图/分区/函数等元数据时,建议使用 Hive 方言</p>
<h4 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h4><p>Flink 支持 DML 写入 Hive 表,请参考读写 Hive 表</p>
<h3 id="Hive-Catalog"><a href="#Hive-Catalog" class="headerlink" title="Hive Catalog"></a>Hive Catalog</h3><p>多年来,Hive Metastore 已发展成为 Hadoop 生态系统中事实上的元数据中心.<br>许多公司在其生产中使用单个 Hive Metastore 服务实例来管理其所有元数据,无论是 Hive 元数据还是非 Hive 元数据,作为事实来源.</p>
<p>对于同时拥有 Hive 和 Flink 部署的用户,HiveCatalog使他们能够使用 Hive Metastore 来管理 Flink 的元数据.</p>
<p>对于刚刚部署 Flink 的用户,是 FlinkHiveCatalog提供的唯一一个开箱即用的持久化目录.<br>如果没有持久化的目录,使用Flink SQL CREATE DDL的用户必须在每个会话中重复创建像 Kafka 表这样的元对象,这会浪费大量时间.<br>HiveCatalog通过授权用户只创建一次表和其他元对象,并在以后跨会话方便地引用和管理它们来填补这一空白.</p>
<h4 id="设置-HiveCatalog"><a href="#设置-HiveCatalog" class="headerlink" title="设置 HiveCatalog"></a>设置 HiveCatalog</h4><h5 id="依赖项-2"><a href="#依赖项-2" class="headerlink" title="依赖项"></a>依赖项</h5><p>在 Flink 中设置需要与 整体 Flink-Hive 集成HiveCatalog相同的依赖项.</p>
<h5 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h5><p>在 Flink 中设置HiveCatalog需要与 整体 Flink-Hive 集成相同的配置.</p>
<h4 id="如何使用-HiveCatalog"><a href="#如何使用-HiveCatalog" class="headerlink" title="如何使用 HiveCatalog"></a>如何使用 HiveCatalog</h4><p>一旦配置正确,HiveCatalog应该可以开箱即用.<br>用户可以使用 DDL 创建 Flink 元对象,然后应该立即看到它们.</p>
<p>HiveCatalog可用于处理两种表:Hive 兼容表和通用表.<br>Hive 兼容表是那些以 Hive 兼容方式存储的表,就存储层中的元数据和数据而言.<br>因此,通过 Flink 创建的 Hive 兼容表可以从 Hive 端查询.</p>
<p>另一方面,通用表是 Flink 特有的.<br>使用 来创建通用表时HiveCatalog,我们只是使用 HMS 来持久化元数据.<br>虽然这些表对 Hive 可见,但 Hive 不太可能理解元数据.<br>因此,在 Hive 中使用此类表会导致未定义的行为.</p>
<p>建议切换到Hive 方言来创建 Hive 兼容的表.<br>如果要使用默认方言创建 Hive 兼容表,请确保&#39;connector&#39;=&#39;hive&#39;在表属性中进行设置,否则默认情况下,表在HiveCatalog. 请注意,connector如果您使用 Hive 方言,则不需要该属性.</p>
<h5 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h5><p>我们将在这里介绍一个简单的示例.</p>
<p>第 1 步:设置 Hive Metastore<br>运行 Hive Metastore.</p>
<p>hive-site.xml在这里,我们在本地路径中设置了一个本地 Hive Metastore 和我们的文件/opt/hive-conf/hive-site.xml.<br>我们有一些配置,如下所示:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>metadata is stored in a MySQL server<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>MySQL JDBC driver class<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>...<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>user name for connecting to mysql server<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>...<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password for connecting to mysql server<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://localhost:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>IP address (or fully-qualified domain name) and port of the metastore host<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>使用 Hive Cli 测试与 HMS 的连接.<br>运行一些命令,我们可以看到我们有一个名为default的数据库,其中没有表.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br></pre></td></tr></table></figure>

<p>步骤 2:启动 SQL Client,并使用 Flink SQL DDL 创建 Hive 目录<br>将所有 Hive 依赖项添加到Flink 发行版中/lib的 dir,并在 Flink SQL CLI 中创建 Hive 目录,如下所示:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> CATALOG myhive <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;hive&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hive-conf-dir&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/opt/hive-conf&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>第 3 步:设置 Kafka 集群<br>使用名为&quot;test&quot;的主题引导本地 Kafka 集群,并为该主题生成一些简单的数据作为名称和年龄的元组.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">tom,15</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">john,21</span></span><br></pre></td></tr></table></figure>

<p>通过启动 Kafka 控制台消费者可以看到这些消息.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line"></span><br><span class="line">tom,15</span><br><span class="line">john,21</span><br></pre></td></tr></table></figure>

<p>第 4 步:使用 Flink SQL DDL 创建 Kafka 表<br>使用 Flink SQL DDL 创建一个简单的 Kafka 表,并验证其架构.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">USE CATALOG myhive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mykafka (name String, age <span class="type">Int</span>) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">&#x27;connector.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;connector.version&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;universal&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;connector.topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;connector.properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;format.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;update-mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;append&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">DESCRIBE</span> mykafka;</span><br><span class="line">root</span><br><span class="line"> <span class="operator">|</span><span class="comment">-- name: STRING</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- age: INT</span></span><br></pre></td></tr></table></figure>

<p>通过 Hive Cli 验证表对 Hive 也可见:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> tables;</span><br></pre></td></tr></table></figure>

<p>step 5:运行 Flink SQL 查询 Kafka 表<br>从 Flink 集群中的 Flink SQL 客户端运行一个简单的选择查询,无论是独立的还是 yarn-session.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mykafka;</span><br></pre></td></tr></table></figure>

<p>在 Kafka 主题中产生更多消息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin<span class="operator">/</span>kafka<span class="operator">-</span>console<span class="operator">-</span>consumer.sh <span class="comment">--bootstrap-server localhost:9092 --topic test --from-beginning</span></span><br><span class="line"></span><br><span class="line">tom,<span class="number">15</span></span><br><span class="line">john,<span class="number">21</span></span><br><span class="line">kitty,<span class="number">30</span></span><br><span class="line">amy,<span class="number">24</span></span><br><span class="line">kaiky,<span class="number">18</span></span><br></pre></td></tr></table></figure>

<p>您现在应该在 SQL 客户端中看到 Flink 生成的结果,如下所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">name                       age</span><br><span class="line">tom                        15</span><br><span class="line">john                        21</span><br><span class="line">kitty                        30</span><br><span class="line">amy                        24</span><br><span class="line">kaiky                        18</span><br></pre></td></tr></table></figure>

<h4 id="支持的类型"><a href="#支持的类型" class="headerlink" title="支持的类型"></a>支持的类型</h4><p>HiveCatalog支持通用表的所有 Flink 类型.<br>对于 Hive 兼容的表,HiveCatalog需要将 Flink 数据类型映射到对应的 Hive 类型,如下表所述:</p>
<table>
<thead>
<tr>
<th align="left">Flink Data Type</th>
<th align="left">Hive Data Type</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CHAR(p)</td>
<td align="left">CHAR(p)</td>
</tr>
<tr>
<td align="left">VARCHAR(p)</td>
<td align="left">VARCHAR(p)</td>
</tr>
<tr>
<td align="left">STRING</td>
<td align="left">STRING</td>
</tr>
<tr>
<td align="left">BOOLEAN</td>
<td align="left">BOOLEAN</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">TINYINT</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">SMALLINT</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">INT</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">LONG</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">FLOAT</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">DOUBLE</td>
</tr>
<tr>
<td align="left">DECIMAL(p, s)</td>
<td align="left">DECIMAL(p, s)</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">DATE</td>
</tr>
<tr>
<td align="left">TIMESTAMP(9)</td>
<td align="left">TIMESTAMP</td>
</tr>
<tr>
<td align="left">BYTES</td>
<td align="left">BINARY</td>
</tr>
<tr>
<td align="left">ARRAY<T></td>
<td align="left">LIST<T></td>
</tr>
<tr>
<td align="left">MAP</td>
<td align="left">MAP</td>
</tr>
<tr>
<td align="left">ROW</td>
<td align="left">STRUCT</td>
</tr>
</tbody></table>
<p>关于类型映射的一些注意事项:</p>
<ol>
<li>Hive 的CHAR(p)最大长度为 255</li>
<li>Hive 的VARCHAR(p)最大长度为 65535</li>
<li>Hive MAP仅支持原始键类型,而 FlinkMAP可以是任何数据类型</li>
<li>UNION不支持Hive 的类型</li>
<li>Hive 的TIMESTAMP精度始终为 9,不支持其他精度.<br>另一方面,Hive UDF 可以处理TIMESTAMP精度 &lt;= 9 的值.</li>
<li>Hive 不支持 Flink 的TIMESTAMP_WITH_TIME_ZONE, TIMESTAMP_WITH_LOCAL_TIME_ZONE, 和MULTISET</li>
<li>Flink 的INTERVAL类型还不能映射到 HiveINTERVAL类型</li>
</ol>
<h3 id="Hive-方言"><a href="#Hive-方言" class="headerlink" title="Hive 方言"></a>Hive 方言</h3><p>从 1.11.0 开始,在使用 Hive 方言时,Flink 允许用户用 Hive 语法来编写 SQL 语句.<br>通过提供与 Hive 语法的兼容性,我们旨在改善与 Hive 的互操作性,并减少用户需要在 Flink 和 Hive 之间切换来执行不同语句的情况.</p>
<h4 id="使用-Hive-方言"><a href="#使用-Hive-方言" class="headerlink" title="使用 Hive 方言"></a>使用 Hive 方言</h4><p>Flink 目前支持两种 SQL 方言: default 和 hive.<br>你需要先切换到 Hive 方言,然后才能使用 Hive 语法编写.<br>下面介绍如何使用 SQL 客户端和 Table API 设置方言.<br>还要注意,你可以为执行的每个语句动态切换方言.<br>无需重新启动会话即可使用其他方言.</p>
<h5 id="SQL-客户端"><a href="#SQL-客户端" class="headerlink" title="SQL 客户端"></a>SQL 客户端</h5><p>SQL 方言可以通过 table.sql-dialect 属性指定.<br>因此你可以通过 SQL 客户端 yaml 文件中的 configuration 部分来设置初始方言.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">execution:</span><br><span class="line">  type: batch</span><br><span class="line">  result-mode: table</span><br><span class="line"></span><br><span class="line">configuration:</span><br><span class="line">  table.sql-dialect: hive</span><br></pre></td></tr></table></figure>

<p>你同样可以在 SQL 客户端启动后设置方言.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive; <span class="comment">-- to use hive dialect</span></span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span><span class="keyword">default</span>; <span class="comment">-- to use default dialect</span></span><br><span class="line">[INFO] Session property has been set.</span><br></pre></td></tr></table></figure>

<h5 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h5><p>你可以使用 Table API 为 TableEnvironment 设置方言.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"><span class="comment">// to use hive dialect</span></span><br><span class="line">tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);</span><br><span class="line"><span class="comment">// to use default dialect</span></span><br><span class="line">tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);</span><br></pre></td></tr></table></figure>

<h4 id="DDL-1"><a href="#DDL-1" class="headerlink" title="DDL"></a>DDL</h4><p>本章节列出了 Hive 方言支持的 DDL 语句.<br>我们主要关注语法.<br>你可以参考 Hive 文档 了解每个 DDL 语句的语义.</p>
<h5 id="CATALOG"><a href="#CATALOG" class="headerlink" title="CATALOG"></a>CATALOG</h5><h6 id="Show"><a href="#Show" class="headerlink" title="Show"></a>Show</h6><p>SHOW CURRENT CATALOG;</p>
<h5 id="DATABASE"><a href="#DATABASE" class="headerlink" title="DATABASE"></a>DATABASE</h5><h6 id="Show-1"><a href="#Show-1" class="headerlink" title="Show"></a>Show</h6><p>SHOW DATABASES;</p>
<h6 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h6><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION fs_path]</span><br><span class="line">  [<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)];</span><br></pre></td></tr></table></figure>

<h6 id="Alter"><a href="#Alter" class="headerlink" title="Alter"></a>Alter</h6><p>Update Properties<br>ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);</p>
<p>Update Owner<br>ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;</p>
<p>Update Location<br>ALTER (DATABASE|SCHEMA) database_name SET LOCATION fs_path;</p>
<h6 id="Drop"><a href="#Drop" class="headerlink" title="Drop"></a>Drop</h6><p>DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</p>
<h6 id="Use"><a href="#Use" class="headerlink" title="Use"></a>Use</h6><p>USE database_name;</p>
<h5 id="TABLE"><a href="#TABLE" class="headerlink" title="TABLE"></a>TABLE</h5><h6 id="Show-2"><a href="#Show-2" class="headerlink" title="Show"></a>Show</h6><p>SHOW TABLES;</p>
<h6 id="Create-1"><a href="#Create-1" class="headerlink" title="Create"></a>Create</h6><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">  [(col_name data_type [column_constraint] [COMMENT col_comment], ... [table_constraint])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [</span><br><span class="line">    [<span class="type">ROW</span> FORMAT row_format]</span><br><span class="line">    [STORED <span class="keyword">AS</span> file_format]</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION fs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line"></span><br><span class="line">row_format:</span><br><span class="line">  : DELIMITED [FIELDS TERMINATED <span class="keyword">BY</span> <span class="type">char</span> [ESCAPED <span class="keyword">BY</span> <span class="type">char</span>]] [COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>]</span><br><span class="line">      [MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [LINES TERMINATED <span class="keyword">BY</span> <span class="type">char</span>]</span><br><span class="line">      [<span class="keyword">NULL</span> DEFINED <span class="keyword">AS</span> <span class="type">char</span>]</span><br><span class="line">  <span class="operator">|</span> SERDE serde_name [<span class="keyword">WITH</span> SERDEPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line"></span><br><span class="line">file_format:</span><br><span class="line">  : SEQUENCEFILE</span><br><span class="line">  <span class="operator">|</span> TEXTFILE</span><br><span class="line">  <span class="operator">|</span> RCFILE</span><br><span class="line">  <span class="operator">|</span> ORC</span><br><span class="line">  <span class="operator">|</span> PARQUET</span><br><span class="line">  <span class="operator">|</span> AVRO</span><br><span class="line">  <span class="operator">|</span> INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br><span class="line"></span><br><span class="line">column_constraint:</span><br><span class="line">  : <span class="keyword">NOT</span> <span class="keyword">NULL</span> [[ENABLE<span class="operator">|</span>DISABLE] [VALIDATE<span class="operator">|</span>NOVALIDATE] [RELY<span class="operator">|</span>NORELY]]</span><br><span class="line"></span><br><span class="line">table_constraint:</span><br><span class="line">  : [<span class="keyword">CONSTRAINT</span> constraint_name] <span class="keyword">PRIMARY</span> KEY (col_name, ...) [[ENABLE<span class="operator">|</span>DISABLE] [VALIDATE<span class="operator">|</span>NOVALIDATE] [RELY<span class="operator">|</span>NORELY]]</span><br></pre></td></tr></table></figure>

<h6 id="Alter-1"><a href="#Alter-1" class="headerlink" title="Alter"></a>Alter</h6><p>Rename<br>ALTER TABLE table_name RENAME TO new_table_name;</p>
<p>Update Properties<br>ALTER TABLE table_name SET TBLPROPERTIES (property_name = property_value, property_name = property_value, ... );</p>
<p>Update Location<br>ALTER TABLE table_name [PARTITION partition_spec] SET LOCATION fs_path;<br>如果指定了 partition_spec,那么必须完整,即具有所有分区列的值.<br>如果指定了,该操作将作用在对应分区上而不是表上.</p>
<p>Update File Format<br>ALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format;<br>如果指定了 partition_spec,那么必须完整,即具有所有分区列的值.<br>如果指定了,该操作将作用在对应分区上而不是表上.</p>
<p>Update SerDe Properties<br>ALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties];</p>
<p>ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties;</p>
<p>serde_properties:<br>  : (property_name = property_value, property_name = property_value, ... )<br>如果指定了 partition_spec,那么必须完整,即具有所有分区列的值.<br>如果指定了,该操作将作用在对应分区上而不是表上.</p>
<p>Add Partitions<br>ALTER TABLE table_name ADD [IF NOT EXISTS] (PARTITION partition_spec [LOCATION fs_path])+;</p>
<p>Drop Partitions<br>ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...];</p>
<p>Add/Replace Columns</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name</span><br><span class="line">  <span class="keyword">ADD</span><span class="operator">|</span>REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br><span class="line">  [CASCADE<span class="operator">|</span>RESTRICT]</span><br></pre></td></tr></table></figure>

<p>Change Column </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type</span><br><span class="line">  [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name] [CASCADE<span class="operator">|</span>RESTRICT];</span><br></pre></td></tr></table></figure>

<p>Drop<br>DROP TABLE [IF EXISTS] table_name;</p>
<h6 id="VIEW"><a href="#VIEW" class="headerlink" title="VIEW"></a>VIEW</h6><p>Create</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] view_name [(column_name, ...) ]</span><br><span class="line">  [COMMENT view_comment]</span><br><span class="line">  [TBLPROPERTIES (property_name <span class="operator">=</span> property_value, ...)]</span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> ...;</span><br></pre></td></tr></table></figure>

<p>Alter<br>注意: 变更视图只在 Table API 中有效,SQL 客户端不支持.</p>
<p>Rename<br>ALTER VIEW view_name RENAME TO new_view_name;</p>
<p>Update Properties<br>ALTER VIEW view_name SET TBLPROPERTIES (property_name = property_value, ... );</p>
<p>Update As Select<br>ALTER VIEW view_name AS select_statement;</p>
<p>Drop<br>DROP VIEW [IF EXISTS] view_name;</p>
<h6 id="FUNCTION"><a href="#FUNCTION" class="headerlink" title="FUNCTION"></a>FUNCTION</h6><p>Show<br>SHOW FUNCTIONS;</p>
<p>Create<br>CREATE FUNCTION function_name AS class_name;</p>
<p>Drop<br>DROP FUNCTION [IF EXISTS] function_name;</p>
<h4 id="DML-amp-DQL-Beta"><a href="#DML-amp-DQL-Beta" class="headerlink" title="DML &amp; DQL Beta"></a>DML &amp; DQL Beta</h4><p>Hive 方言支持常用的 Hive DML 和 DQL .<br>下表列出了一些 Hive 方言支持的语法.<br>SORT/CLUSTER/DISTRIBUTE BY<br>Group By<br>Join<br>Union<br>LATERAL VIEW<br>Window Functions<br>SubQueries<br>CTE<br>INSERT INTO dest schema<br>Implicit type conversions</p>
<p>为了实现更好的语法和语义的兼容,强烈建议使用 HiveModule 并将其放在 Module 列表的首位,以便在函数解析时优先使用 Hive 内置函数.</p>
<p>Hive 方言不再支持 Flink SQL 语法 .<br>若需使用 Flink 语法,请切换到 default 方言.</p>
<p>以下是一个使用 Hive 方言的示例.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> catalog myhive <span class="keyword">with</span> (<span class="string">&#x27;type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;hive&#x27;</span>, <span class="string">&#x27;hive-conf-dir&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;/opt/hive-conf&#x27;</span>);</span><br><span class="line">[INFO] <span class="keyword">Execute</span> statement succeed.</span><br><span class="line"></span><br><span class="line">use catalog myhive;</span><br><span class="line">[INFO] <span class="keyword">Execute</span> statement succeed.</span><br><span class="line"></span><br><span class="line">load <span class="keyword">module</span> hive;</span><br><span class="line">[INFO] <span class="keyword">Execute</span> statement succeed.</span><br><span class="line"></span><br><span class="line">use modules hive,core;</span><br><span class="line">[INFO] <span class="keyword">Execute</span> statement succeed.</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive;</span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> explode(<span class="keyword">array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)); <span class="comment">-- call hive udtf</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+</span></span><br><span class="line"><span class="operator">|</span> col <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">2</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">3</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tbl (key <span class="type">int</span>,<span class="keyword">value</span> string);</span><br><span class="line">[INFO] <span class="keyword">Execute</span> statement succeed.</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tbl <span class="keyword">values</span> (<span class="number">5</span>,<span class="string">&#x27;e&#x27;</span>),(<span class="number">1</span>,<span class="string">&#x27;a&#x27;</span>),(<span class="number">1</span>,<span class="string">&#x27;a&#x27;</span>),(<span class="number">3</span>,<span class="string">&#x27;c&#x27;</span>),(<span class="number">2</span>,<span class="string">&#x27;b&#x27;</span>),(<span class="number">3</span>,<span class="string">&#x27;c&#x27;</span>),(<span class="number">3</span>,<span class="string">&#x27;c&#x27;</span>),(<span class="number">4</span>,<span class="string">&#x27;d&#x27;</span>);</span><br><span class="line">[INFO] Submitting <span class="keyword">SQL</span> update statement <span class="keyword">to</span> the cluster...</span><br><span class="line">[INFO] <span class="keyword">SQL</span> update statement has been successfully submitted <span class="keyword">to</span> the cluster:</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tbl cluster <span class="keyword">by</span> key; <span class="comment">-- run cluster by</span></span><br><span class="line"><span class="number">2021</span><span class="number">-04</span><span class="number">-22</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">57</span>,<span class="number">005</span> INFO  org.apache.hadoop.mapred.FileInputFormat                     [] <span class="operator">-</span> Total input paths <span class="keyword">to</span> process : <span class="number">1</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+-------+</span></span><br><span class="line"><span class="operator">|</span> key <span class="operator">|</span> <span class="keyword">value</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+-------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">1</span> <span class="operator">|</span>     a <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">1</span> <span class="operator">|</span>     a <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">5</span> <span class="operator">|</span>     e <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">2</span> <span class="operator">|</span>     b <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">3</span> <span class="operator">|</span>     c <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">3</span> <span class="operator">|</span>     c <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">3</span> <span class="operator">|</span>     c <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="number">4</span> <span class="operator">|</span>     d <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+-------+</span></span><br><span class="line"><span class="number">8</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span></span><br></pre></td></tr></table></figure>

<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>以下是使用 Hive 方言的一些注意事项.</p>
<p>Hive 方言只能用于操作 Hive 对象,并要求当前 Catalog 是一个 HiveCatalog .</p>
<p>Hive 方言只支持 db.table 这种两级的标识符,不支持带有 Catalog 名字的标识符.</p>
<p>虽然所有 Hive 版本支持相同的语法,但是一些特定的功能是否可用仍取决于你使用的Hive 版本.<br>例如,更新数据库位置 只在 Hive-2.4.0 或更高版本支持.</p>
<p>执行 DML 和 DQL 时应该使用 HiveModule .</p>
<h3 id="Hive-Read-amp-Write"><a href="#Hive-Read-amp-Write" class="headerlink" title="Hive Read &amp; Write"></a>Hive Read &amp; Write</h3><p>Apache Flink使用HiveCatalog 可以对 Apache Hive Tables 进行统一BATCH STREAM处理.<br>这意味着 Flink 可以用作 Hive 批处理引擎的性能更高的替代品,或者连续读写 Hive 表中的数据,以支持实时数据仓库应用程序.</p>
<h4 id="read"><a href="#read" class="headerlink" title="read"></a>read</h4><p>Flink 支持在BATCH和STREAMING模式下从 Hive 读取数据.<br>当作为BATCH 应用程序运行时,Flink 将在执行查询的时间点对表的状态执行查询.<br>STREAMINGreads 将持续监视表并在新数据可用时增量获取它.<br>Flink 会默认读取有界的表.</p>
<p>STREAMING读取支持同时使用分区表和非分区表.<br>对于分区表,Flink 会监控新分区的生成,并在可用时增量读取.<br>对于非分区表,Flink 会监控文件夹中新文件的生成,增量读取新文件.</p>
<p><code>streaming-source.enable</code><br>FALSE<br>Boolean<br>是否启用流媒体源.<br>注意:请确保每个分区/文件都应该以原子方式写入,否则读者可能会得到不完整的数据.</p>
<p><code>streaming-source.partition.include</code><br>all<br>String<br>设置读取分区的选项,支持的选项是<code>all</code>和<code>latest</code>,<code>all</code>表示读取所有分区；<code>latest</code> 表示按 &#39;streaming-source.partition.order&#39; 的顺序读取最新分区,<code>latest</code> 仅在流式配置单元源表用作临时表时才有效.<br>默认情况下,该选项是&quot;全部&quot;.<br>Flink 支持通过启用 &#39;streaming-source.enable&#39; 并将 &#39;streaming-source.partition.include&#39; 设置为 &#39;latest&#39; 来临时加入最新的 Hive 分区,同时用户可以通过分配分区比较顺序和数据更新间隔配置以下与分区相关的选项.</p>
<p><code>streaming-source.monitor-interval</code><br>None<br>Duration<br>连续监控分区/文件的时间间隔.<br>注意:hive 流读取的默认间隔是&quot;1 分钟&quot;,hive 流时间连接的默认间隔是&quot;60 分钟&quot;,这是因为有一个框架限制,每个 TM 都会在当前的 Hive 流时间连接中访问 Hive 元存储可能会对 metaStore 产生压力的实现,这将在未来得到改善.</p>
<p><code>streaming-source.partition-order</code><br>partition-name<br>String<br>流源的分区顺序,支持create-time、partition-time和partition-name.<br>create-time 比较分区/文件创建时间,这不是 Hive MetaStore 中的分区创建时间,而是文件系统中的文件夹/文件修改时间,如果分区文件夹以某种方式更新,例如将新文件添加到文件夹中,它会影响如何数据被消耗.<br>partition-time 比较从分区名称中提取的时间.<br>partition-name 比较分区名称的字母顺序.<br>对于非分区表,此值应始终为&quot;create-time&quot;.<br>默认情况下,该值为分区名称.<br>该选项与已弃用的选项&quot;streaming-source.consume-order&quot;相等.</p>
<p><code>streaming-source.consume-start-offset</code><br>None<br>String<br>流式消费的开始偏移量.<br>如何解析和比较偏移量取决于您的订单.<br>对于创建时间和分区时间,应该是时间戳字符串 (yyyy-[m]m-[d]d [hh:mm:ss]).<br>对于分区时间,将使用分区时间提取器从分区中提取时间.<br>对于分区名称,是分区名称字符串(例如 pt_year=2020/pt_mon=10/pt_day=01).</p>
<p>SQL 提示可用于将配置应用于 Hive 表,而无需更改其在 Hive 元存储中的定义.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> </span><br><span class="line"><span class="keyword">FROM</span> hive_table </span><br><span class="line"><span class="comment">/*+ OPTIONS(&#x27;streaming-source.enable&#x27;=&#x27;true&#x27;, &#x27;streaming-source.consume-start-offset&#x27;=&#x27;2020-05-20&#x27;) */</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>笔记</p>
</blockquote>
<ol>
<li><p>监控策略是扫描当前位置路径中的所有目录/文件.<br>许多分区可能会导致性能下降.</p>
</li>
<li><p>非分区表的流式读取要求将每个文件原子地写入目标目录.</p>
</li>
<li><p>分区表的流式读取要求在 hive 元存储的视图中原子地添加每个分区.<br>否则,将使用添加到现有分区的新数据.</p>
</li>
<li><p>流式读取不支持 Flink DDL 中的水印语法.<br>这些表不能用于窗口运算符.</p>
</li>
</ol>
<h5 id="阅读-Hive-视图"><a href="#阅读-Hive-视图" class="headerlink" title="阅读 Hive 视图"></a>阅读 Hive 视图</h5><p>Flink 能够从 Hive 定义的视图中读取,但存在一些限制:</p>
<ol>
<li>必须先将 Hive 目录设置为当前目录,然后才能查询视图.<br>这可以通过tableEnv.useCatalog(...)Table API 或USE CATALOG ...SQL 客户端来完成.</li>
<li>Hive 和 Flink SQL 有不同的语法,例如不同的保留关键字和文字.<br>确保视图的查询与 Flink 语法兼容.</li>
</ol>
<h5 id="读取时的矢量化优化"><a href="#读取时的矢量化优化" class="headerlink" title="读取时的矢量化优化"></a>读取时的矢量化优化</h5><p>当满足以下条件时,Flink 会自动使用 Hive 表的向量化读取:</p>
<ol>
<li>格式:ORC 或 Parquet.</li>
<li>没有复杂数据类型的列,例如配置单元类型:List/Map/Struct/Union.</li>
</ol>
<p>默认情况下启用此功能.<br>可以使用以下配置禁用它.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table.exec.hive.fallback<span class="operator">-</span>mapred<span class="operator">-</span>reader<span class="operator">=</span><span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h5 id="源并行推理"><a href="#源并行推理" class="headerlink" title="源并行推理"></a>源并行推理</h5><p>默认情况下,Flink 将根据文件数和每个文件中的块数为其 Hive 读取器推断最佳并行度.</p>
<p>Flink 允许您灵活配置并行推理的策略.<br>您可以在中配置以下参数TableConfig(注意这些参数会影响作业的所有来源):</p>
<img src="/images/flgl180.png" style="margin-left: 0px; padding-bottom: 10px;">

<h5 id="加载分区拆分"><a href="#加载分区拆分" class="headerlink" title="加载分区拆分"></a>加载分区拆分</h5><p>多线程用于拆分 hive 的分区.<br>您可以使用table.exec.hive.load-partition-splits.thread-num来配置线程数.<br>默认值为 3,配置值应大于 0.</p>
<h4 id="临时表连接"><a href="#临时表连接" class="headerlink" title="临时表连接"></a>临时表连接</h4><p>您可以将 Hive 表用作时态表,然后流可以通过时态连接关联 Hive 表.<br>有关时间连接的更多信息,请参阅时间连接.</p>
<p>Flink 支持 processing-time temporal join Hive Table,processing-time temporal join 总是加入最新版本的 temporal table.<br>Flink 支持对分区表和 Hive 非分区表进行 temporal join,对于分区表,Flink 支持自动跟踪 Hive 表的最新分区.</p>
<p>注意:Flink 还不支持 event-time temporal join Hive 表.</p>
<h5 id="临时加入最新分区"><a href="#临时加入最新分区" class="headerlink" title="临时加入最新分区"></a>临时加入最新分区</h5><p>对于随时间变化的分区表,我们可以将其读取为无界流,如果每个分区都包含一个版本的完整数据,则该分区可以作为时态表的一个版本,时态表的版本保存数据的分区.</p>
<p>Flink 支持在处理时间 temporal join 时自动跟踪 temporal table 的最新分区(version),最新 partition(version)由 &#39;streaming-source.partition-order&#39; 选项定义,这是使用 Hive table 的最常见用例作为 Flink 流应用作业中的维度表.</p>
<p>注意:此功能仅在 FlinkSTREAMING模式下支持.</p>
<p>下面的demo是一个经典的业务流水线,维度表来自Hive,每天通过批处理流水线作业或Flink作业更新一次,kafka流来自实时在线业务数据或日志,需要加入维度表来丰富流.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Assume the data in hive table is updated per day, every day contains the latest and complete dimension data</span></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dimension_table (</span><br><span class="line">  product_id STRING,</span><br><span class="line">  product_name STRING,</span><br><span class="line">  unit_price <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">4</span>),</span><br><span class="line">  pv_count <span class="type">BIGINT</span>,</span><br><span class="line">  like_count <span class="type">BIGINT</span>,</span><br><span class="line">  comment_count <span class="type">BIGINT</span>,</span><br><span class="line">  update_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  update_user STRING,</span><br><span class="line">  ...</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (pt_year STRING, pt_month STRING, pt_day STRING) TBLPROPERTIES (</span><br><span class="line">  <span class="comment">-- using default partition-name order to load the latest partition every 12h (the most recommended and convenient way)</span></span><br><span class="line">  <span class="string">&#x27;streaming-source.enable&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.partition.include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.monitor-interval&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;12 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.partition-order&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;partition-name&#x27;</span>,  <span class="comment">-- option with default value, can be ignored.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">-- using partition file create-time order to load the latest partition every 12h</span></span><br><span class="line">  <span class="string">&#x27;streaming-source.enable&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.partition.include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.partition-order&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;create-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.monitor-interval&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;12 h&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">-- using partition-time order to load the latest partition every 12h</span></span><br><span class="line">  <span class="string">&#x27;streaming-source.enable&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.partition.include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.monitor-interval&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;12 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;streaming-source.partition-order&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;partition-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.kind&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;default&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;$pt_year-$pt_month-$pt_day 00:00:00&#x27;</span> </span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span><span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders_table (</span><br><span class="line">  order_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  product_id STRING,</span><br><span class="line">  log_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  proctime <span class="keyword">as</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- streaming sql, kafka temporal join a hive dimension table. Flink will automatically reload data from the</span></span><br><span class="line"><span class="comment">-- configured latest partition in the interval of &#x27;streaming-source.monitor-interval&#x27;.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> orders_table <span class="keyword">AS</span> o </span><br><span class="line"><span class="keyword">JOIN</span> dimension_table <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> o.proctime <span class="keyword">AS</span> dim</span><br><span class="line"><span class="keyword">ON</span> o.product_id <span class="operator">=</span> dim.product_id;</span><br></pre></td></tr></table></figure>

<h5 id="临时加入最新表"><a href="#临时加入最新表" class="headerlink" title="临时加入最新表"></a>临时加入最新表</h5><p>对于 Hive 表,我们可以将其作为有界流读取.<br>在这种情况下,Hive 表只能在我们查询时跟踪其最新版本.<br>最新版本的 table 保留了 Hive 表的所有数据.</p>
<p>在对最新的 Hive 表执行时间连接时,Hive 表将缓存在 Slot 内存中,并且流中的每条记录都通过键与表进行连接,以确定是否找到匹配项.<br>使用最新的 Hive 表作为临时表不需要任何额外的配置.<br>或者,您可以使用以下属性配置 Hive 表缓存的 TTL.<br>缓存过期后,会再次扫描 Hive 表以加载最新数据.</p>
<p><code>lookup.join.cache.ttl</code><br>60 min<br>Duration<br>查找连接中构建表的缓存 TTL(例如 10 分钟).<br>默认情况下,TTL 为 60 分钟.<br>注意:该选项仅在查找有界 Hive 表源时有效,如果您使用流式 Hive 源作为临时表,请使用 &#39;streaming-source.monitor-interval&#39; 配置数据更新的间隔.</p>
<p>以下演示显示将配置单元表的所有数据加载为时态表.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Assume the data in hive table is overwrite by batch pipeline.</span></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dimension_table (</span><br><span class="line">  product_id STRING,</span><br><span class="line">  product_name STRING,</span><br><span class="line">  unit_price <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">4</span>),</span><br><span class="line">  pv_count <span class="type">BIGINT</span>,</span><br><span class="line">  like_count <span class="type">BIGINT</span>,</span><br><span class="line">  comment_count <span class="type">BIGINT</span>,</span><br><span class="line">  update_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  update_user STRING,</span><br><span class="line">  ...</span><br><span class="line">) TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;streaming-source.enable&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;false&#x27;</span>,           <span class="comment">-- option with default value, can be ignored.</span></span><br><span class="line">  <span class="string">&#x27;streaming-source.partition.include&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;all&#x27;</span>,  <span class="comment">-- option with default value, can be ignored.</span></span><br><span class="line">  <span class="string">&#x27;lookup.join.cache.ttl&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;12 h&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span><span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders_table (</span><br><span class="line">  order_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  product_id STRING,</span><br><span class="line">  log_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  proctime <span class="keyword">as</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- streaming sql, kafka join a hive dimension table. Flink will reload all data from dimension_table after cache ttl is expired.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> orders_table <span class="keyword">AS</span> o </span><br><span class="line"><span class="keyword">JOIN</span> dimension_table <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> o.proctime <span class="keyword">AS</span> dim</span><br><span class="line"><span class="keyword">ON</span> o.product_id <span class="operator">=</span> dim.product_id;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>笔记</p>
</blockquote>
<ol>
<li>每个加入子任务都需要保留自己的 Hive 表缓存.<br>请确保 Hive 表可以放入 TM 任务槽的内存中.</li>
<li>鼓励为streaming-source.monitor-interval(最新的分区作为时态表)或lookup.join.cache.ttl(所有的分区作为时态表)设置一个相对较大的值.<br>否则,作业很容易出现性能问题,因为表需要过于频繁地更新和重新加载.</li>
<li>目前,只要缓存需要刷新,我们只需加载整个 Hive 表.<br>没有办法区分新数据和旧数据.</li>
</ol>
<h4 id="write"><a href="#write" class="headerlink" title="write"></a>write</h4><p>Flink 支持在BATCH和STREAMING模式下从 Hive 写入数据.<br>当作为BATCH 应用程序运行时,Flink 将写入 Hive 表,仅在 Job 完成时才使这些记录可见.<br>BATCHwrites 支持追加和覆盖现有表.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># <span class="comment">------ INSERT INTO will append to the table or partition, keeping the existing data intact ------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mytable <span class="keyword">SELECT</span> <span class="string">&#x27;Tom&#x27;</span>, <span class="number">25</span>;</span><br><span class="line"></span><br><span class="line"># <span class="comment">------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE mytable <span class="keyword">SELECT</span> <span class="string">&#x27;Tom&#x27;</span>, <span class="number">25</span>;</span><br></pre></td></tr></table></figure>

<p>数据也可以插入特定的分区.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># <span class="comment">------ Insert with static partition ------</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE myparttable <span class="keyword">PARTITION</span> (my_type<span class="operator">=</span><span class="string">&#x27;type_1&#x27;</span>, my_date<span class="operator">=</span><span class="string">&#x27;2019-08-08&#x27;</span>) <span class="keyword">SELECT</span> <span class="string">&#x27;Tom&#x27;</span>, <span class="number">25</span>;</span><br><span class="line"></span><br><span class="line"># <span class="comment">------ Insert with dynamic partition ------</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE myparttable <span class="keyword">SELECT</span> <span class="string">&#x27;Tom&#x27;</span>, <span class="number">25</span>, <span class="string">&#x27;type_1&#x27;</span>, <span class="string">&#x27;2019-08-08&#x27;</span>;</span><br><span class="line"></span><br><span class="line"># <span class="comment">------ Insert with static(my_type) and dynamic(my_date) partition ------</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE myparttable <span class="keyword">PARTITION</span> (my_type<span class="operator">=</span><span class="string">&#x27;type_1&#x27;</span>) <span class="keyword">SELECT</span> <span class="string">&#x27;Tom&#x27;</span>, <span class="number">25</span>, <span class="string">&#x27;2019-08-08&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>STREAMING不断写入将新数据添加到 Hive,提交记录 - 使它们可见 - 增量.<br>用户使用多个属性控制何时/如何触发提交.<br>流式写入不支持插入覆盖.</p>
<p>下面的示例展示了如何使用流式接收器编写流式查询以将数据从 Kafka 写入具有分区提交的 Hive 表,并运行批处理查询以将该数据读回.</p>
<p>请参阅流式接收器以获取可用配置的完整列表.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span></span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt STRING, hr STRING) STORED <span class="keyword">AS</span> parquet TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span><span class="operator">=</span><span class="string">&#x27;$dt $hr:00:00&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.trigger&#x27;</span><span class="operator">=</span><span class="string">&#x27;partition-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.delay&#x27;</span><span class="operator">=</span><span class="string">&#x27;1 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span><span class="operator">=</span><span class="string">&#x27;metastore,success-file&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span><span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  log_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> log_ts <span class="keyword">AS</span> log_ts <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> <span class="comment">-- Define watermark on TIMESTAMP column</span></span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- streaming sql, insert into hive table</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> hive_table </span><br><span class="line"><span class="keyword">SELECT</span> user_id, order_amount, DATE_FORMAT(log_ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), DATE_FORMAT(log_ts, <span class="string">&#x27;HH&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> kafka_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- batch sql, select with partition pruning</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> hive_table <span class="keyword">WHERE</span> dt<span class="operator">=</span><span class="string">&#x27;2020-05-20&#x27;</span> <span class="keyword">and</span> hr<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>如果在 TIMESTAMP_LTZ 列上定义了水印并用于partition-time提交,sink.partition-commit.watermark-time-zone则需要设置为会话时区,否则可能会在几个小时后发生提交的分区.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span></span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt STRING, hr STRING) STORED <span class="keyword">AS</span> parquet TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span><span class="operator">=</span><span class="string">&#x27;$dt $hr:00:00&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.trigger&#x27;</span><span class="operator">=</span><span class="string">&#x27;partition-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.delay&#x27;</span><span class="operator">=</span><span class="string">&#x27;1 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.watermark-time-zone&#x27;</span><span class="operator">=</span><span class="string">&#x27;Asia/Shanghai&#x27;</span>, <span class="comment">-- Assume user configured time zone is &#x27;Asia/Shanghai&#x27;</span></span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span><span class="operator">=</span><span class="string">&#x27;metastore,success-file&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span><span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  ts <span class="type">BIGINT</span>, <span class="comment">-- time in epoch milliseconds</span></span><br><span class="line">  ts_ltz <span class="keyword">AS</span> TO_TIMESTAMP_LTZ(ts, <span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_ltz <span class="keyword">AS</span> ts_ltz <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> <span class="comment">-- Define watermark on TIMESTAMP_LTZ column</span></span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- streaming sql, insert into hive table</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> hive_table </span><br><span class="line"><span class="keyword">SELECT</span> user_id, order_amount, DATE_FORMAT(ts_ltz, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), DATE_FORMAT(ts_ltz, <span class="string">&#x27;HH&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> kafka_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- batch sql, select with partition pruning</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> hive_table <span class="keyword">WHERE</span> dt<span class="operator">=</span><span class="string">&#x27;2020-05-20&#x27;</span> <span class="keyword">and</span> hr<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>默认情况下,对于流式写入,Flink 仅支持重命名提交者,这意味着 S3 文件系统不支持完全一次性的流式写入.<br>通过将以下参数配置为 false 可以实现对 S3 的一次性写入.<br>这将指示接收器使用 Flink 的本机编写器,但仅适用于 parquet 和 orc 文件类型.<br>此配置在 中设置,TableConfig并将影响作业的所有接收器.</p>
<p><code>table.exec.hive.fallback-mapred-writer</code><br>true<br>Boolean<br>如果为false,使用flink native writer写parquet和orc文件；如果是真的,使用 hadoop mapred record writer 写 parquet 和 orc 文件.</p>
<h4 id="Formats-2"><a href="#Formats-2" class="headerlink" title="Formats"></a>Formats</h4><p>Flink 的 Hive 集成已经针对以下文件格式进行了测试:<br>Text<br>CSV<br>SequenceFile<br>ORC<br>Parquet</p>
<h3 id="Hive-Functions"><a href="#Hive-Functions" class="headerlink" title="Hive Functions"></a>Hive Functions</h3><h4 id="通过-HiveModule-使用-Hive-内置函数"><a href="#通过-HiveModule-使用-Hive-内置函数" class="headerlink" title="通过 HiveModule 使用 Hive 内置函数"></a>通过 HiveModule 使用 Hive 内置函数</h4><p>将HiveModuleHive 内置函数作为 Flink 系统(内置)函数提供给 Flink SQL 和 Table API 用户.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String name            = <span class="string">&quot;myhive&quot;</span>;</span><br><span class="line">String version         = <span class="string">&quot;2.3.4&quot;</span>;</span><br><span class="line"></span><br><span class="line">tableEnv.loadModue(name, <span class="keyword">new</span> HiveModule(version));</span><br></pre></td></tr></table></figure>

<p>旧版本中的一些 Hive 内置函数存在线程安全问题.<br>我们建议用户修补他们自己的 Hive 来修复它们.</p>
<h4 id="Hive-用户定义函数"><a href="#Hive-用户定义函数" class="headerlink" title="Hive 用户定义函数"></a>Hive 用户定义函数</h4><p>用户可以在 Flink 中使用他们现有的 Hive 用户定义函数.<br>支持的 UDF 类型包括:<br>UDF<br>GenericUDF<br>GenericUDTF<br>UDAF<br>GenericUDAFResolver2</p>
<p>在查询规划和执行时,Hive 的 UDF 和 GenericUDF 会自动翻译成 Flink 的 ScalarFunction,Hive 的 GenericUDTF 会自动翻译成 Flink 的 TableFunction,Hive 的 UDAF 和 GenericUDAFResolver2 会自动翻译成 Flink 的 AggregateFunction.</p>
<p>要使用 Hive 用户定义函数,用户必须:</p>
<ol>
<li>设置由 Hive Metastore 支持的 HiveCatalog,其中包含该功能作为会话的当前目录</li>
<li>在 Flink 的类路径中包含一个包含该函数的 jar</li>
</ol>
<h4 id="使用-Hive-用户定义的函数"><a href="#使用-Hive-用户定义的函数" class="headerlink" title="使用 Hive 用户定义的函数"></a>使用 Hive 用户定义的函数</h4><p>假设我们在 Hive Metastore 中注册了以下 Hive 函数:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Test simple udf. Registered under name &#x27;myudf&#x27;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestHiveSimpleUDF</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> IntWritable <span class="title">evaluate</span><span class="params">(IntWritable i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> IntWritable(i.get());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(Text text)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Text(text.toString());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Test generic udf. Registered under name &#x27;mygenericudf&#x27;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestHiveGenericUDF</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    checkArgument(arguments.length == <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    checkArgument(arguments[<span class="number">1</span>] <span class="keyword">instanceof</span> ConstantObjectInspector);</span><br><span class="line">    Object constant = ((ConstantObjectInspector) arguments[<span class="number">1</span>]).getWritableConstantValue();</span><br><span class="line">    checkArgument(constant <span class="keyword">instanceof</span> IntWritable);</span><br><span class="line">    checkArgument(((IntWritable) constant).get() == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (arguments[<span class="number">0</span>] <span class="keyword">instanceof</span> IntObjectInspector ||</span><br><span class="line">        arguments[<span class="number">0</span>] <span class="keyword">instanceof</span> StringObjectInspector) &#123;</span><br><span class="line">      <span class="keyword">return</span> arguments[<span class="number">0</span>];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Not support argument: &quot;</span> + arguments[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> arguments[<span class="number">0</span>].get();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;TestHiveGenericUDF&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Test split udtf. Registered under name &#x27;mygenericudtf&#x27;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestHiveUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    checkArgument(argOIs.length == <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TEST for constant arguments</span></span><br><span class="line">    checkArgument(argOIs[<span class="number">1</span>] <span class="keyword">instanceof</span> ConstantObjectInspector);</span><br><span class="line">    Object constant = ((ConstantObjectInspector) argOIs[<span class="number">1</span>]).getWritableConstantValue();</span><br><span class="line">    checkArgument(constant <span class="keyword">instanceof</span> IntWritable);</span><br><span class="line">    checkArgument(((IntWritable) constant).get() == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(</span><br><span class="line">      Collections.singletonList(<span class="string">&quot;col1&quot;</span>),</span><br><span class="line">      Collections.singletonList(PrimitiveObjectInspectorFactory.javaStringObjectInspector));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    String str = (String) args[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (String s : str.split(<span class="string">&quot;,&quot;</span>)) &#123;</span><br><span class="line">      forward(s);</span><br><span class="line">      forward(s);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从 Hive CLI 中,我们可以看到它们已注册:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line">OK</span><br><span class="line">......</span><br><span class="line">mygenericudf</span><br><span class="line">myudf</span><br><span class="line">myudtf</span><br></pre></td></tr></table></figure>

<p>然后,用户可以在 SQL 中将它们用作:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> mygenericudf(myudf(name), <span class="number">1</span>) <span class="keyword">as</span> a, mygenericudf(myudf(age), <span class="number">1</span>) <span class="keyword">as</span> b, s <span class="keyword">from</span> mysourcetable, <span class="keyword">lateral</span> <span class="keyword">table</span>(myudtf(name, <span class="number">1</span>)) <span class="keyword">as</span> T(s);</span><br></pre></td></tr></table></figure>

<h1 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h1><h2 id="读取和写入文件系统"><a href="#读取和写入文件系统" class="headerlink" title="读取和写入文件系统"></a>读取和写入文件系统</h2><p>Apache Flink 项目支持多个文件系统,可用作输入和输出连接器的后备存储.</p>
<h2 id="使用-Hadoop-的-Input-OutputFormat-包装器连接到其他系统"><a href="#使用-Hadoop-的-Input-OutputFormat-包装器连接到其他系统" class="headerlink" title="使用 Hadoop 的 Input/OutputFormat 包装器连接到其他系统"></a>使用 Hadoop 的 Input/OutputFormat 包装器连接到其他系统</h2><p>Apache Flink 允许用户访问许多不同的系统作为数据源或接收器.<br>该系统的设计非常易于扩展.<br>与 Apache Hadoop 类似,Flink 也有所谓的InputFormats和OutputFormats 的概念.</p>
<p>这些InputFormats 的一种实现是HadoopInputFormat. 这是一个包装器,允许用户在 Flink 中使用所有现有的 Hadoop 输入格式.</p>
<p>本节展示了一些将 Flink 连接到其他系统的示例.<br>在 Flink 中阅读有关 Hadoop 兼容性的更多信息.</p>
<h2 id="Flink-中的-Avro-支持"><a href="#Flink-中的-Avro-支持" class="headerlink" title="Flink 中的 Avro 支持"></a>Flink 中的 Avro 支持</h2><p>Flink 为Apache Avro提供了广泛的内置支持.<br>这允许使用 Flink 从 Avro 文件中轻松读取.<br>此外,Flink 的序列化框架能够处理从 Avro 模式生成的类.<br>确保在项目的 pom.xml 中包含 Flink Avro 依赖项.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>为了从 Avro 文件中读取数据,您必须指定一个AvroInputFormat.<br>示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AvroInputFormat&lt;User&gt; users = <span class="keyword">new</span> AvroInputFormat&lt;User&gt;(in, User.class);</span><br><span class="line">DataSet&lt;User&gt; usersDS = env.createInput(users);</span><br></pre></td></tr></table></figure>

<p>请注意,这User是 Avro 生成的 POJO.<br>Flink 还允许对这些 POJO 执行基于字符串的键选择.<br>例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usersDS.groupBy(<span class="string">&quot;name&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>请注意,GenericData.RecordFlink 可以使用该类型,但不推荐使用.<br>由于记录包含完整的模式,它的数据非常密集,因此使用起来可能很慢.</p>
<p>Flink 的 POJO 字段选择也适用于从 Avro 生成的 POJO.<br>但是,只有将字段类型正确写入生成的类时,才能使用.<br>如果字段属于类型Object,则不能将该字段用作连接键或分组键.<br>像这样在 Avro 中指定一个字段{&quot;name&quot;: &quot;type_double_test&quot;, &quot;type&quot;: &quot;double&quot;},可以正常工作,但是将其指定为只有一个字段 ( {&quot;name&quot;: &quot;type_double_test&quot;, &quot;type&quot;: [&quot;double&quot;]},) 的 UNION 类型将生成一个类型为 的字段Object.<br>请注意,指定可为空的类型 ( {&quot;name&quot;: &quot;type_double_test&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;double&quot;]},) 是可能的！</p>
<h3 id="访问-Microsoft-Azure-表存储"><a href="#访问-Microsoft-Azure-表存储" class="headerlink" title="访问 Microsoft Azure 表存储"></a>访问 Microsoft Azure 表存储</h3><h2 id="访问-MongoDB"><a href="#访问-MongoDB" class="headerlink" title="访问 MongoDB"></a>访问 MongoDB</h2>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/flink/" rel="tag"># flink</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/09/08/clickhouse%20try/" rel="prev" title="clickhouse try">
                  <i class="fa fa-chevron-left"></i> clickhouse try
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/14/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%A4%9A%E7%A7%8D%E5%AE%9E%E7%8E%B0/" rel="next" title="分布式锁的多种实现">
                  分布式锁的多种实现 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">maoeryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  





</body>
</html>
