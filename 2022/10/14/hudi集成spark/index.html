<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maoeryu.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","width":200,"display":"post","padding":7,"offset":5},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="通过Spark保存数据到Hudi表,使用Spark加载Hudi表数据进行分析,不仅支持批处理和流计算,还可以集成Hive进行数据分析. 下载对应的jar包hudi-spark2.4-bundle_2.11-0.12.0.jarspark-avro_2.11-2.4.8.jarhudi-utilities-bundle_2.11-0.12.0.jar放入${SPARK_HOME}&#x2F;jars文件夹下.">
<meta property="og:type" content="article">
<meta property="og:title" content="hudi集成spark">
<meta property="og:url" content="https://maoeryu.github.io/2022/10/14/hudi%E9%9B%86%E6%88%90spark/index.html">
<meta property="og:site_name" content="FlyingPig">
<meta property="og:description" content="通过Spark保存数据到Hudi表,使用Spark加载Hudi表数据进行分析,不仅支持批处理和流计算,还可以集成Hive进行数据分析. 下载对应的jar包hudi-spark2.4-bundle_2.11-0.12.0.jarspark-avro_2.11-2.4.8.jarhudi-utilities-bundle_2.11-0.12.0.jar放入${SPARK_HOME}&#x2F;jars文件夹下.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1180.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1022.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1018.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1019.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1020.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1021.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1176.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1177.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1178.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1179.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1181.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1182.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1183.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1184.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1185.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1046.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1047.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1048.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1049.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1050.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1051.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1052.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1053.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1054.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1055.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1056.png">
<meta property="article:published_time" content="2022-10-13T16:00:00.000Z">
<meta property="article:modified_time" content="2022-10-26T02:48:05.488Z">
<meta property="article:author" content="maoeryu">
<meta property="article:tag" content="hudi">
<meta property="article:tag" content="hive">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maoeryu.github.io/images/fly1180.png">


<link rel="canonical" href="https://maoeryu.github.io/2022/10/14/hudi%E9%9B%86%E6%88%90spark/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>hudi集成spark | FlyingPig</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlyingPig</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"><span class="nav-number">1.</span> <span class="nav-text">快速开始</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sql"><span class="nav-number">1.1.</span> <span class="nav-text">sql</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.1.1.</span> <span class="nav-text">设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="nav-number">1.1.2.</span> <span class="nav-text">创建表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A8%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">表概念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A1%A8%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.1.2.1.1.</span> <span class="nav-text">表类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E9%9D%9E%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="nav-number">1.1.2.1.2.</span> <span class="nav-text">分区表和非分区表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%98%E7%AE%A1%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="nav-number">1.1.2.1.3.</span> <span class="nav-text">托管和外部表</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%9D%9E%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">创建非分区表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">创建分区表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E7%8E%B0%E6%9C%89-Hudi-%E8%A1%A8%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">为现有 Hudi 表创建表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CTAS"><span class="nav-number">1.1.2.5.</span> <span class="nav-text">CTAS</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.1.2.5.1.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8%E5%B1%9E%E6%80%A7"><span class="nav-number">1.1.2.6.</span> <span class="nav-text">创建表属性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">1.1.3.</span> <span class="nav-text">插入数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE"><span class="nav-number">1.1.4.</span> <span class="nav-text">查询数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%97%85%E8%A1%8C%E6%9F%A5%E8%AF%A2"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">时间旅行查询</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE-Update-data"><span class="nav-number">1.1.5.</span> <span class="nav-text">更新数据(Update data)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Update"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">Update</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Merge-Into"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">Merge-Into</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE-Delete-data"><span class="nav-number">1.1.6.</span> <span class="nav-text">删除数据(Delete data)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AC%E5%88%A0%E9%99%A4-Hard-Deletes"><span class="nav-number">1.1.6.1.</span> <span class="nav-text">硬删除(Hard Deletes)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%92%E5%85%A5%E8%A6%86%E7%9B%96-Insert-Overwrite"><span class="nav-number">1.1.7.</span> <span class="nav-text">插入覆盖(Insert Overwrite)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E5%AE%83%E5%91%BD%E4%BB%A4"><span class="nav-number">1.1.8.</span> <span class="nav-text">其它命令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%94%B9%E8%A1%A8-Alter-Table"><span class="nav-number">1.1.8.1.</span> <span class="nav-text">更改表(Alter Table)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA-SQL%E5%91%BD%E4%BB%A4"><span class="nav-number">1.1.8.2.</span> <span class="nav-text">分区 SQL命令</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shell"><span class="nav-number">1.2.</span> <span class="nav-text">shell</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE"><span class="nav-number">1.2.2.</span> <span class="nav-text">模拟数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8-1"><span class="nav-number">1.2.3.</span> <span class="nav-text">创建表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE-1"><span class="nav-number">1.2.4.</span> <span class="nav-text">插入数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE-1"><span class="nav-number">1.2.5.</span> <span class="nav-text">查询数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%97%85%E8%A1%8C%E6%9F%A5%E8%AF%A2-1"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">时间旅行查询</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE-Update-data-1"><span class="nav-number">1.2.6.</span> <span class="nav-text">更新数据(Update data)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E6%9F%A5%E8%AF%A2-Incremental-query"><span class="nav-number">1.2.7.</span> <span class="nav-text">增量查询(Incremental query)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E7%82%B9%E6%9F%A5%E8%AF%A2-Point-in-time-query"><span class="nav-number">1.2.8.</span> <span class="nav-text">时间点查询(Point in time query)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE-Delete-data-1"><span class="nav-number">1.2.9.</span> <span class="nav-text">删除数据(Delete data)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AF%E5%88%A0%E9%99%A4-Soft-Deletes"><span class="nav-number">1.2.9.1.</span> <span class="nav-text">软删除(Soft Deletes)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AC%E5%88%A0%E9%99%A4-Hard-Deletes-1"><span class="nav-number">1.2.9.2.</span> <span class="nav-text">硬删除(Hard Deletes)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%92%E5%85%A5%E8%A6%86%E7%9B%96-Insert-Overwrite-1"><span class="nav-number">1.2.10.</span> <span class="nav-text">插入覆盖(Insert Overwrite)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%89%B9%E5%A4%84%E7%90%86-hive"><span class="nav-number">2.</span> <span class="nav-text">批处理&#x2F;hive</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.</span> <span class="nav-text">需求说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="nav-number">2.2.</span> <span class="nav-text">环境准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7%E7%B1%BBSparkUtils"><span class="nav-number">2.2.1.</span> <span class="nav-text">工具类SparkUtils</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A5%E6%9C%9F%E8%BD%AC%E6%8D%A2%E6%98%9F%E6%9C%9F"><span class="nav-number">2.2.2.</span> <span class="nav-text">日期转换星期</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AEETL%E4%BF%9D%E5%AD%98"><span class="nav-number">2.3.</span> <span class="nav-text">数据ETL保存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4"><span class="nav-number">2.3.1.</span> <span class="nav-text">开发步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BDCSV%E6%95%B0%E6%8D%AE"><span class="nav-number">2.3.2.</span> <span class="nav-text">加载CSV数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AEETL%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.3.3.</span> <span class="nav-text">数据ETL转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E8%87%B3Hudi"><span class="nav-number">2.3.4.</span> <span class="nav-text">保存数据至Hudi</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hudi-%E8%A1%A8%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84"><span class="nav-number">2.3.5.</span> <span class="nav-text">Hudi 表存储结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="nav-number">2.4.</span> <span class="nav-text">指标查询分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4-1"><span class="nav-number">2.4.1.</span> <span class="nav-text">开发步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BDHudi%E8%A1%A8%E6%95%B0%E6%8D%AE"><span class="nav-number">2.4.2.</span> <span class="nav-text">加载Hudi表数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%B8%80-%E8%AE%A2%E5%8D%95%E7%B1%BB%E5%9E%8B%E7%BB%9F%E8%AE%A1"><span class="nav-number">2.4.3.</span> <span class="nav-text">指标一:订单类型统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%BA%8C-%E8%AE%A2%E5%8D%95%E6%97%B6%E6%95%88%E6%80%A7%E7%BB%9F%E8%AE%A1"><span class="nav-number">2.4.4.</span> <span class="nav-text">指标二:订单时效性统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%B8%89-%E8%AE%A2%E5%8D%95%E4%BA%A4%E9%80%9A%E7%B1%BB%E5%9E%8B%E7%BB%9F%E8%AE%A1"><span class="nav-number">2.4.5.</span> <span class="nav-text">指标三:订单交通类型统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E5%9B%9B-%E8%AE%A2%E5%8D%95%E4%BB%B7%E6%A0%BC%E7%BB%9F%E8%AE%A1"><span class="nav-number">2.4.6.</span> <span class="nav-text">指标四:订单价格统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%BA%94-%E8%AE%A2%E5%8D%95%E8%B7%9D%E7%A6%BB%E7%BB%9F%E8%AE%A1"><span class="nav-number">2.4.7.</span> <span class="nav-text">指标五:订单距离统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E5%85%AD-%E8%AE%A2%E5%8D%95%E6%98%9F%E6%9C%9F%E7%BB%9F%E8%AE%A1"><span class="nav-number">2.4.8.</span> <span class="nav-text">指标六:订单星期统计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E6%88%90Hive%E6%9F%A5%E8%AF%A2"><span class="nav-number">2.5.</span> <span class="nav-text">集成Hive查询</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8%E5%8F%8A%E6%9F%A5%E8%AF%A2"><span class="nav-number">2.5.1.</span> <span class="nav-text">创建表及查询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HiveQL-%E5%88%86%E6%9E%90"><span class="nav-number">2.5.2.</span> <span class="nav-text">HiveQL 分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%B8%80-%E8%AE%A2%E5%8D%95%E7%B1%BB%E5%9E%8B%E7%BB%9F%E8%AE%A1-1"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">指标一:订单类型统计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%BA%8C-%E8%AE%A2%E5%8D%95%E6%97%B6%E6%95%88%E6%80%A7%E7%BB%9F%E8%AE%A1-1"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">指标二:订单时效性统计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E4%B8%89-%E8%AE%A2%E5%8D%95%E4%BA%A4%E9%80%9A%E7%B1%BB%E5%9E%8B%E7%BB%9F%E8%AE%A1-1"><span class="nav-number">2.5.2.3.</span> <span class="nav-text">指标三:订单交通类型统计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E5%9B%9B-%E8%AE%A2%E5%8D%95%E4%BB%B7%E6%A0%BC%E7%BB%9F%E8%AE%A1-1"><span class="nav-number">2.5.2.4.</span> <span class="nav-text">指标四:订单价格统计</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B5%81%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">流处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E4%BA%A4%E6%98%93%E8%AE%A2%E5%8D%95"><span class="nav-number">3.1.</span> <span class="nav-text">模拟交易订单</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%BC%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91"><span class="nav-number">3.2.</span> <span class="nav-text">流式程序开发</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="nav-number">3.3.</span> <span class="nav-text">Spark 查询分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeltaStreamer-%E5%B7%A5%E5%85%B7%E7%B1%BB"><span class="nav-number">3.4.</span> <span class="nav-text">DeltaStreamer 工具类</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">maoeryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">222</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://maoeryu.github.io/2022/10/14/hudi%E9%9B%86%E6%88%90spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="maoeryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlyingPig">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          hudi集成spark
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-10-14 00:00:00" itemprop="dateCreated datePublished" datetime="2022-10-14T00:00:00+08:00">2022-10-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-10-26 10:48:05" itemprop="dateModified" datetime="2022-10-26T10:48:05+08:00">2022-10-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8D%8F%E5%90%8C%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">协同框架</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8D%8F%E5%90%8C%E6%A1%86%E6%9E%B6/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>通过Spark保存数据到Hudi表,使用Spark加载Hudi表数据进行分析,不仅支持批处理和流计算,还可以集成Hive进行数据分析.</p>
<p>下载对应的jar包<br><code>hudi-spark2.4-bundle_2.11-0.12.0.jar</code><br><code>spark-avro_2.11-2.4.8.jar</code><br><code>hudi-utilities-bundle_2.11-0.12.0.jar</code><br>放入${SPARK_HOME}/jars文件夹下.</p>
<p>hudi 0.12.0<br>spark 2.4.8<br>scala 2.11</p>
<span id="more"></span>
<h1 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h1><blockquote>
<p>Hudi 如何管理数据?</p>
<p>使用表Table形式组织数据,并且每张表中数据类似Hive分区表,<br>按照分区字段划分数据到不同目录中,<br>每条数据有主键PrimaryKey,标识数据唯一性.</p>
</blockquote>
<h2 id="sql"><a href="#sql" class="headerlink" title="sql"></a>sql</h2><h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><p>对于 Spark 3.2 及更高版本,需要额外的 spark_catalog 配置.<br>Hudi 支持使用 Spark SQL 通过HoodieSparkSessionExtension sql 扩展来写入和读取数据.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Spark 2.4</span></span><br><span class="line">spark-sql \</span><br><span class="line">--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \</span><br><span class="line">--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Spark 3.2</span></span><br><span class="line">spark-sql \</span><br><span class="line">--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \</span><br><span class="line">--conf &#x27;spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog&#x27; \</span><br><span class="line">--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27;</span><br></pre></td></tr></table></figure>

<p>Hudi默认upsert/insert/delete的并发度是200,对于演示小规模数据集设置更小的并发度.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set hoodie.upsert.shuffle.parallelism &#x3D; 1;</span><br><span class="line">set hoodie.insert.shuffle.parallelism &#x3D; 1;</span><br><span class="line">set hoodie.bulkinsert.shuffle.parallelism &#x3D; 1;</span><br><span class="line">set hoodie.delete.shuffle.parallelism &#x3D; 1;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;设置不同步Hudi表元数据,默认false.</span><br><span class="line">set hoodie.datasource.meta.sync.enable&#x3D;false;</span><br></pre></td></tr></table></figure>

<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>Spark SQL 需要一个显式的创建表命令.<br>在实际应用使用时,合理选择创建表的方式,建议创建外部及分区表,便于数据管理和安全.</p>
<h4 id="表概念"><a href="#表概念" class="headerlink" title="表概念"></a>表概念</h4><h5 id="表类型"><a href="#表类型" class="headerlink" title="表类型"></a>表类型</h5><p>Hudi 的两种表类型,Copy-On-Write (COW) 和 Merge-On-Read (MOR),都可以使用 Spark SQL 创建.<br>创建表时,可以使用type选项指定表类型:<code>type = &#39;cow&#39;</code>或<code>type = &#39;mor&#39;</code>.</p>
<h5 id="分区表和非分区表"><a href="#分区表和非分区表" class="headerlink" title="分区表和非分区表"></a>分区表和非分区表</h5><p>用户可以在 Spark SQL 中创建分区表或非分区表.<br>要创建分区表,需要使用<code>partitioned by</code>语句指定分区列来创建分区表.<br>当 create table 命令没有partitioned by语句时,table 被认为是非分区表.</p>
<h5 id="托管和外部表"><a href="#托管和外部表" class="headerlink" title="托管和外部表"></a>托管和外部表</h5><p>一般来说,Spark SQL 支持两种表,即托管表和外部表.<br>如果使用 <code>location</code>语句指定位置或用于<code>create external table</code>显式创建表,则它是外部表,否则将其视为托管表.</p>
<blockquote>
<p>由于 Hudi 0.10.0,<code>primaryKey</code>是必需的.不再支持Non-primary-key表.<br>任何在 0.10.0 之前创建的没有 primaryKey 的 Hudi 表都需要使用 0.10.0 的 primaryKey 字段重新创建.<br>与<code>hoodie.datasource.write.recordkey.field</code>类似,uuid默认用作主键.如果table是这种情况,您可以跳过在tblproperties中设置primaryKey.</p>
</blockquote>
<blockquote>
<p>primaryKey, preCombineField和type区分大小写.<br>MOR 表需要preCombineField.</p>
</blockquote>
<blockquote>
<p>当设置primaryKey/preCombineField/type,或其他 Hudi 配置时,tblproperties优先于options.<br>由 Spark SQL 创建的新 Hudi 表将默认设置<code>hoodie.table.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator</code>和 <code>hoodie.datasource.write.hive_style_partitioning=true</code>.</p>
</blockquote>
<h4 id="创建非分区表"><a href="#创建非分区表" class="headerlink" title="创建非分区表"></a>创建非分区表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- create a cow table, with default primaryKey &#x27;uuid&#x27; and without preCombineField provided</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_cow_nonpcf_tbl (</span><br><span class="line">  uuid <span class="type">int</span>,</span><br><span class="line">  name string,</span><br><span class="line">  price <span class="keyword">double</span></span><br><span class="line">) <span class="keyword">using</span> hudi;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- create a mor non-partitioned table without preCombineField provided</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_mor_tbl (</span><br><span class="line">  id <span class="type">int</span>,</span><br><span class="line">  name string,</span><br><span class="line">  price <span class="keyword">double</span>,</span><br><span class="line">  ts <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">using</span> hudi</span><br><span class="line">tblproperties (</span><br><span class="line">  type <span class="operator">=</span> <span class="string">&#x27;mor&#x27;</span>,</span><br><span class="line">  primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>,</span><br><span class="line">  preCombineField <span class="operator">=</span> <span class="string">&#x27;ts&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="创建分区表"><a href="#创建分区表" class="headerlink" title="创建分区表"></a>创建分区表</h4><p>下面是创建外部 COW 分区表的示例.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- create a partitioned, preCombineField-provided cow table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_cow_pt_tbl (</span><br><span class="line">  id <span class="type">bigint</span>,</span><br><span class="line">  name string,</span><br><span class="line">  ts <span class="type">bigint</span>,</span><br><span class="line">  dt string,</span><br><span class="line">  hh string</span><br><span class="line">) <span class="keyword">using</span> hudi</span><br><span class="line">tblproperties (</span><br><span class="line">  type <span class="operator">=</span> <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">  primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>,</span><br><span class="line">  preCombineField <span class="operator">=</span> <span class="string">&#x27;ts&#x27;</span></span><br><span class="line"> )</span><br><span class="line">partitioned <span class="keyword">by</span> (dt, hh)</span><br><span class="line">location <span class="string">&#x27;/hudi/hudi_cow_pt_tbl&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h4 id="为现有-Hudi-表创建表"><a href="#为现有-Hudi-表创建表" class="headerlink" title="为现有 Hudi 表创建表"></a>为现有 Hudi 表创建表</h4><p>我们可以在现有的 hudi 表上创建一个表(使用 spark-shell 或 deltastreamer 创建).<br>这对于读取/写入预先存在的 hudi 表很有用.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_existing_tbl <span class="keyword">using</span> hudi</span><br><span class="line">location <span class="string">&#x27;/hudi/hudi_existing_table&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>除了分区列(如果存在)之外,您不需要指定架构和任何属性.<br>Hudi 可以自动识别架构和配置.</p>
<h4 id="CTAS"><a href="#CTAS" class="headerlink" title="CTAS"></a>CTAS</h4><p>Hudi 在 Spark SQL 上支持 CTAS(Create Table As Select).</p>
<blockquote>
<p>为了更好的将数据加载到 hudi 表中,CTAS 使用批量插入作为写入操作.</p>
</blockquote>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>用于创建不带 preCombineField 的非分区 COW 表CTAS 命令.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- CTAS: create a non-partitioned cow table without preCombineField</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_ctas_cow_nonpcf_tbl</span><br><span class="line"><span class="keyword">using</span> hudi</span><br><span class="line">tblproperties (primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>)</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id, <span class="string">&#x27;a1&#x27;</span> <span class="keyword">as</span> name, <span class="number">10</span> <span class="keyword">as</span> price;</span><br></pre></td></tr></table></figure>

<p>用于创建分区的主键 COW 表的示例 CTAS 命令.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- CTAS: create a partitioned, preCombineField-provided cow table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_ctas_cow_pt_tbl</span><br><span class="line"><span class="keyword">using</span> hudi</span><br><span class="line">tblproperties (type <span class="operator">=</span> <span class="string">&#x27;cow&#x27;</span>, primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>, preCombineField <span class="operator">=</span> <span class="string">&#x27;ts&#x27;</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (dt)</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id, <span class="string">&#x27;a1&#x27;</span> <span class="keyword">as</span> name, <span class="number">10</span> <span class="keyword">as</span> price, <span class="number">1000</span> <span class="keyword">as</span> ts, <span class="string">&#x27;2021-12-01&#x27;</span> <span class="keyword">as</span> dt;</span><br></pre></td></tr></table></figure>

<p>从另一个表加载数据的示例 CTAS 命令.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">create</span> managed parquet <span class="keyword">table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> parquet_mngd <span class="keyword">using</span> parquet location <span class="string">&#x27;/parquet_dataset/*.parquet&#x27;</span>;</span><br><span class="line"></span><br><span class="line"># CTAS <span class="keyword">by</span> loading data <span class="keyword">into</span> hudi <span class="keyword">table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_ctas_cow_pt_tbl2 <span class="keyword">using</span> hudi location <span class="string">&#x27;/hudi/hudi_tbl/&#x27;</span> options (</span><br><span class="line">  type <span class="operator">=</span> <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">  primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>,</span><br><span class="line">  preCombineField <span class="operator">=</span> <span class="string">&#x27;ts&#x27;</span></span><br><span class="line"> )</span><br><span class="line">partitioned <span class="keyword">by</span> (datestr) <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> parquet_mngd;</span><br></pre></td></tr></table></figure>

<h4 id="创建表属性"><a href="#创建表属性" class="headerlink" title="创建表属性"></a>创建表属性</h4><table>
<thead>
<tr>
<th align="left">属性</th>
<th align="left">默认值</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">primaryKey</td>
<td align="left">uuid</td>
<td align="left">表的主键名称,多个字段以逗号分隔.同<code>hoodie.datasource.write.recordkey.field</code></td>
</tr>
<tr>
<td align="left">preCombineField</td>
<td align="left">&quot;&quot;</td>
<td align="left">表的预组合字段.同<code>hoodie.datasource.write.precombine.field</code></td>
</tr>
<tr>
<td align="left">type</td>
<td align="left">cow</td>
<td align="left">要创建的表类型,type = &#39;cow&#39; 表示 COPY-ON-WRITE 表,而 type = &#39;mor&#39; 表示 MERGE-ON-READ 表.同<code>hoodie.datasource.write.table.type</code></td>
</tr>
</tbody></table>
<h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- insert into non-partitioned table</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_cow_nonpcf_tbl <span class="keyword">select</span> <span class="number">1</span>, <span class="string">&#x27;a1&#x27;</span>, <span class="number">20</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_mor_tbl <span class="keyword">select</span> <span class="number">1</span>, <span class="string">&#x27;a1&#x27;</span>, <span class="number">20</span>, <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- insert dynamic partition</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_cow_pt_tbl <span class="keyword">partition</span> (dt, hh)</span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id, <span class="string">&#x27;a1&#x27;</span> <span class="keyword">as</span> name, <span class="number">1000</span> <span class="keyword">as</span> ts, <span class="string">&#x27;2021-12-09&#x27;</span> <span class="keyword">as</span> dt, <span class="string">&#x27;10&#x27;</span> <span class="keyword">as</span> hh;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- insert static partition</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_cow_pt_tbl <span class="keyword">partition</span>(dt <span class="operator">=</span> <span class="string">&#x27;2021-12-09&#x27;</span>, hh<span class="operator">=</span><span class="string">&#x27;11&#x27;</span>) <span class="keyword">select</span> <span class="number">2</span>, <span class="string">&#x27;a2&#x27;</span>, <span class="number">1000</span>;</span><br></pre></td></tr></table></figure>

<p>默认情况下,如果提供preCombineKey, 则insert into使用upsert作写操作的类型,否则使用insert.<br>我们支持bulk_insert用作写操作的类型,只需要设置两个配置:<br><code>hoodie.sql.bulk.insert.enable</code>和<code>hoodie.sql.insert.mode</code>.<br>示例如下:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- upsert mode for preCombineField-provided table</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_mor_tbl <span class="keyword">select</span> <span class="number">1</span>, <span class="string">&#x27;a1_1&#x27;</span>, <span class="number">20</span>, <span class="number">1001</span>;</span><br><span class="line"><span class="keyword">select</span> id, name, price, ts <span class="keyword">from</span> hudi_mor_tbl;</span><br><span class="line"><span class="number">1</span>   a1_1    <span class="number">20.0</span>    <span class="number">1001</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- bulk_insert mode for preCombineField-provided table</span></span><br><span class="line"><span class="keyword">set</span> hoodie.sql.bulk.insert.enable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hoodie.sql.insert.mode<span class="operator">=</span>non<span class="operator">-</span>strict;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_mor_tbl <span class="keyword">select</span> <span class="number">1</span>, <span class="string">&#x27;a1_2&#x27;</span>, <span class="number">20</span>, <span class="number">1002</span>;</span><br><span class="line"><span class="keyword">select</span> id, name, price, ts <span class="keyword">from</span> hudi_mor_tbl;</span><br><span class="line"><span class="number">1</span>   a1_1    <span class="number">20.0</span>    <span class="number">1001</span></span><br><span class="line"><span class="number">1</span>   a1_2    <span class="number">20.0</span>    <span class="number">1002</span></span><br></pre></td></tr></table></figure>

<h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><p>将数据文件加载到 DataFrame 中.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> fare, begin_lon, begin_lat, ts <span class="keyword">from</span> hudi_trips_snapshot <span class="keyword">where</span> fare <span class="operator">&gt;</span> <span class="number">20.0</span>;</span><br></pre></td></tr></table></figure>

<h4 id="时间旅行查询"><a href="#时间旅行查询" class="headerlink" title="时间旅行查询"></a>时间旅行查询</h4><p>需要 Spark 3.2+.<br>目前支持三种查询时间格式,如下所示.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hudi_cow_pt_tbl (</span><br><span class="line">  id <span class="type">bigint</span>,</span><br><span class="line">  name string,</span><br><span class="line">  ts <span class="type">bigint</span>,</span><br><span class="line">  dt string,</span><br><span class="line">  hh string</span><br><span class="line">) <span class="keyword">using</span> hudi</span><br><span class="line">tblproperties (</span><br><span class="line">  type <span class="operator">=</span> <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">  primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>,</span><br><span class="line">  preCombineField <span class="operator">=</span> <span class="string">&#x27;ts&#x27;</span></span><br><span class="line"> )</span><br><span class="line">partitioned <span class="keyword">by</span> (dt, hh)</span><br><span class="line">location <span class="string">&#x27;/hudi/hudi_cow_pt_tbl&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_cow_pt_tbl <span class="keyword">select</span> <span class="number">1</span>, <span class="string">&#x27;a0&#x27;</span>, <span class="number">1000</span>, <span class="string">&#x27;2021-12-09&#x27;</span>, <span class="string">&#x27;10&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hudi_cow_pt_tbl;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- record id=1 changes `name`</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hudi_cow_pt_tbl <span class="keyword">select</span> <span class="number">1</span>, <span class="string">&#x27;a1&#x27;</span>, <span class="number">1001</span>, <span class="string">&#x27;2021-12-09&#x27;</span>, <span class="string">&#x27;10&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hudi_cow_pt_tbl;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- time travel based on first commit time, assume `20220307091628793`</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hudi_cow_pt_tbl <span class="type">timestamp</span> <span class="keyword">as</span> <span class="keyword">of</span> <span class="string">&#x27;20220307091628793&#x27;</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- time travel based on different timestamp formats</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hudi_cow_pt_tbl <span class="type">timestamp</span> <span class="keyword">as</span> <span class="keyword">of</span> <span class="string">&#x27;2022-03-07 09:16:28.100&#x27;</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hudi_cow_pt_tbl <span class="type">timestamp</span> <span class="keyword">as</span> <span class="keyword">of</span> <span class="string">&#x27;2022-03-08&#x27;</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<h3 id="更新数据-Update-data"><a href="#更新数据-Update-data" class="headerlink" title="更新数据(Update data)"></a>更新数据(Update data)</h3><p>Spark SQL 支持两种 DML 更新 hudi 表:Merge-Into 和 Update.</p>
<h4 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h4><p>Update操作需要preCombineField指定.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE tableIdentifier <span class="keyword">SET</span> <span class="keyword">column</span> <span class="operator">=</span> EXPRESSION(,<span class="keyword">column</span> <span class="operator">=</span> EXPRESSION) [ <span class="keyword">WHERE</span> boolExpression]</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">update hudi_mor_tbl <span class="keyword">set</span> price <span class="operator">=</span> price <span class="operator">*</span> <span class="number">2</span>, ts <span class="operator">=</span> <span class="number">1111</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">update hudi_cow_pt_tbl <span class="keyword">set</span> name <span class="operator">=</span> <span class="string">&#x27;a1_1&#x27;</span>, ts <span class="operator">=</span> <span class="number">1001</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- update using non-PK field</span></span><br><span class="line">update hudi_cow_pt_tbl <span class="keyword">set</span> ts <span class="operator">=</span> <span class="number">1001</span> <span class="keyword">where</span> name <span class="operator">=</span> <span class="string">&#x27;a1&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h4 id="Merge-Into"><a href="#Merge-Into" class="headerlink" title="Merge-Into"></a>Merge-Into</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> tableIdentifier <span class="keyword">AS</span> target_alias</span><br><span class="line"><span class="keyword">USING</span> (sub_query <span class="operator">|</span> tableIdentifier) <span class="keyword">AS</span> source_alias</span><br><span class="line"><span class="keyword">ON</span> <span class="operator">&lt;</span>merge_condition<span class="operator">&gt;</span></span><br><span class="line">[ <span class="keyword">WHEN</span> MATCHED [ <span class="keyword">AND</span> <span class="operator">&lt;</span><span class="keyword">condition</span><span class="operator">&gt;</span> ] <span class="keyword">THEN</span> <span class="operator">&lt;</span>matched_action<span class="operator">&gt;</span> ]</span><br><span class="line">[ <span class="keyword">WHEN</span> MATCHED [ <span class="keyword">AND</span> <span class="operator">&lt;</span><span class="keyword">condition</span><span class="operator">&gt;</span> ] <span class="keyword">THEN</span> <span class="operator">&lt;</span>matched_action<span class="operator">&gt;</span> ]</span><br><span class="line">[ <span class="keyword">WHEN</span> <span class="keyword">NOT</span> MATCHED [ <span class="keyword">AND</span> <span class="operator">&lt;</span><span class="keyword">condition</span><span class="operator">&gt;</span> ]  <span class="keyword">THEN</span> <span class="operator">&lt;</span>not_matched_action<span class="operator">&gt;</span> ]</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>merge_condition<span class="operator">&gt;</span> <span class="operator">=</span>A equal bool <span class="keyword">condition</span> </span><br><span class="line"><span class="operator">&lt;</span>matched_action<span class="operator">&gt;</span>  <span class="operator">=</span></span><br><span class="line">  <span class="keyword">DELETE</span>  <span class="operator">|</span></span><br><span class="line">  UPDATE <span class="keyword">SET</span> <span class="operator">*</span>  <span class="operator">|</span></span><br><span class="line">  UPDATE <span class="keyword">SET</span> column1 <span class="operator">=</span> expression1 [, column2 <span class="operator">=</span> expression2 ...]</span><br><span class="line"><span class="operator">&lt;</span>not_matched_action<span class="operator">&gt;</span>  <span class="operator">=</span></span><br><span class="line">  <span class="keyword">INSERT</span> <span class="operator">*</span>  <span class="operator">|</span></span><br><span class="line">  <span class="keyword">INSERT</span> (column1 [, column2 ...]) <span class="keyword">VALUES</span> (value1 [, value2 ...])</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- source table using hudi for testing merging into non-partitioned table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> merge_source (id <span class="type">int</span>, name string, price <span class="keyword">double</span>, ts <span class="type">bigint</span>) <span class="keyword">using</span> hudi</span><br><span class="line">tblproperties (primaryKey <span class="operator">=</span> <span class="string">&#x27;id&#x27;</span>, preCombineField <span class="operator">=</span> <span class="string">&#x27;ts&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> merge_source <span class="keyword">values</span> (<span class="number">1</span>, &quot;old_a1&quot;, <span class="number">22</span>, <span class="number">900</span>), (<span class="number">2</span>, &quot;new_a2&quot;, <span class="number">33</span>, <span class="number">2000</span>), (<span class="number">3</span>, &quot;new_a3&quot;, <span class="number">44</span>, <span class="number">2000</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">merge</span> <span class="keyword">into</span> hudi_mor_tbl <span class="keyword">as</span> target</span><br><span class="line"><span class="keyword">using</span> merge_source <span class="keyword">as</span> source</span><br><span class="line"><span class="keyword">on</span> target.id <span class="operator">=</span> source.id</span><br><span class="line"><span class="keyword">when</span> matched <span class="keyword">then</span> update <span class="keyword">set</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">not</span> matched <span class="keyword">then</span> <span class="keyword">insert</span> <span class="operator">*</span></span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- source table using parquet for testing merging into partitioned table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> merge_source2 (id <span class="type">int</span>, name string, flag string, dt string, hh string) <span class="keyword">using</span> parquet;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> merge_source2 <span class="keyword">values</span> (<span class="number">1</span>, &quot;new_a1&quot;, <span class="string">&#x27;update&#x27;</span>, <span class="string">&#x27;2021-12-09&#x27;</span>, <span class="string">&#x27;10&#x27;</span>), (<span class="number">2</span>, &quot;new_a2&quot;, <span class="string">&#x27;delete&#x27;</span>, <span class="string">&#x27;2021-12-09&#x27;</span>, <span class="string">&#x27;11&#x27;</span>), (<span class="number">3</span>, &quot;new_a3&quot;, <span class="string">&#x27;insert&#x27;</span>, <span class="string">&#x27;2021-12-09&#x27;</span>, <span class="string">&#x27;12&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">merge</span> <span class="keyword">into</span> hudi_cow_pt_tbl <span class="keyword">as</span> target</span><br><span class="line"><span class="keyword">using</span> (</span><br><span class="line">  <span class="keyword">select</span> id, name, <span class="string">&#x27;1000&#x27;</span> <span class="keyword">as</span> ts, flag, dt, hh <span class="keyword">from</span> merge_source2</span><br><span class="line">) source</span><br><span class="line"><span class="keyword">on</span> target.id <span class="operator">=</span> source.id</span><br><span class="line"><span class="keyword">when</span> matched <span class="keyword">and</span> flag <span class="operator">!=</span> <span class="string">&#x27;delete&#x27;</span> <span class="keyword">then</span></span><br><span class="line"> update <span class="keyword">set</span> id <span class="operator">=</span> source.id, name <span class="operator">=</span> source.name, ts <span class="operator">=</span> source.ts, dt <span class="operator">=</span> source.dt, hh <span class="operator">=</span> source.hh</span><br><span class="line"><span class="keyword">when</span> matched <span class="keyword">and</span> flag <span class="operator">=</span> <span class="string">&#x27;delete&#x27;</span> <span class="keyword">then</span> <span class="keyword">delete</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">not</span> matched <span class="keyword">then</span></span><br><span class="line"> <span class="keyword">insert</span> (id, name, ts, dt, hh) <span class="keyword">values</span>(source.id, source.name, source.ts, source.dt, source.hh)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

<p>当不满足条件时(关联条件不匹配),插入数据到Hudi表中.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">merge</span> <span class="keyword">into</span> test_hudi_table <span class="keyword">as</span> t0</span><br><span class="line"><span class="keyword">using</span> (</span><br><span class="line"> <span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id, <span class="string">&#x27;hadoop&#x27;</span> <span class="keyword">as</span> name, <span class="number">1</span> <span class="keyword">as</span> price, <span class="number">9000</span> <span class="keyword">as</span> ts, <span class="string">&#x27;2021-11-02&#x27;</span> <span class="keyword">as</span> dt</span><br><span class="line">) <span class="keyword">as</span> s0</span><br><span class="line"><span class="keyword">on</span> t0.id <span class="operator">=</span> s0.id</span><br><span class="line"><span class="keyword">when</span> <span class="keyword">not</span> matched <span class="keyword">then</span> <span class="keyword">insert</span> <span class="operator">*</span> ;</span><br></pre></td></tr></table></figure>

<p>当满足条件时(关联条件匹配),对数据进行更新操作.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">merge</span> <span class="keyword">into</span> test_hudi_table <span class="keyword">as</span> t0</span><br><span class="line"><span class="keyword">using</span> (</span><br><span class="line"> <span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id, <span class="string">&#x27;hadoop3&#x27;</span> <span class="keyword">as</span> name, <span class="number">1000</span> <span class="keyword">as</span> price, <span class="number">9999</span> <span class="keyword">as</span> ts, <span class="string">&#x27;2021-11-02&#x27;</span> <span class="keyword">as</span> dt</span><br><span class="line">) <span class="keyword">as</span> s0</span><br><span class="line"><span class="keyword">on</span> t0.id <span class="operator">=</span> s0.id</span><br><span class="line"><span class="keyword">when</span> matched <span class="keyword">then</span> update <span class="keyword">set</span> <span class="operator">*</span> ;</span><br></pre></td></tr></table></figure>

<p>当满足条件时(关联条件匹配),对数据进行删除操作.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">merge</span> <span class="keyword">into</span> test_hudi_table t0</span><br><span class="line"><span class="keyword">using</span> (</span><br><span class="line"> <span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> s_id, <span class="string">&#x27;hadoop3&#x27;</span> <span class="keyword">as</span> s_name, <span class="number">8888</span> <span class="keyword">as</span> s_price, <span class="number">9999</span> <span class="keyword">as</span> s_ts, <span class="string">&#x27;2021-11-02&#x27;</span> <span class="keyword">as</span> dt</span><br><span class="line">) s0</span><br><span class="line"><span class="keyword">on</span> t0.id <span class="operator">=</span> s0.s_id</span><br><span class="line"><span class="keyword">when</span> matched <span class="keyword">and</span> s_ts <span class="operator">=</span> <span class="number">9999</span> <span class="keyword">then</span> <span class="keyword">delete</span> ;</span><br></pre></td></tr></table></figure>

<h3 id="删除数据-Delete-data"><a href="#删除数据-Delete-data" class="headerlink" title="删除数据(Delete data)"></a>删除数据(Delete data)</h3><h4 id="硬删除-Hard-Deletes"><a href="#硬删除-Hard-Deletes" class="headerlink" title="硬删除(Hard Deletes)"></a>硬删除(Hard Deletes)</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> tableIdentifier [ <span class="keyword">WHERE</span> BOOL_EXPRESSION]</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> hudi_cow_nonpcf_tbl <span class="keyword">where</span> uuid <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> hudi_mor_tbl <span class="keyword">where</span> id <span class="operator">%</span> <span class="number">2</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- delete using non-PK field</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> hudi_cow_pt_tbl <span class="keyword">where</span> name <span class="operator">=</span> <span class="string">&#x27;a1&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="插入覆盖-Insert-Overwrite"><a href="#插入覆盖-Insert-Overwrite" class="headerlink" title="插入覆盖(Insert Overwrite)"></a>插入覆盖(Insert Overwrite)</h3><p>insert overwrite一个分区表使用INSERT_OVERWRITE写操作的类型,而一个非分区表为INSERT_OVERWRITE_TABLE.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- insert overwrite non-partitioned table</span></span><br><span class="line"><span class="keyword">insert</span> overwrite hudi_mor_tbl <span class="keyword">select</span> <span class="number">99</span>, <span class="string">&#x27;a99&#x27;</span>, <span class="number">20.0</span>, <span class="number">900</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite hudi_cow_nonpcf_tbl <span class="keyword">select</span> <span class="number">99</span>, <span class="string">&#x27;a99&#x27;</span>, <span class="number">20.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- insert overwrite partitioned table with dynamic partition</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> hudi_cow_pt_tbl <span class="keyword">select</span> <span class="number">10</span>, <span class="string">&#x27;a10&#x27;</span>, <span class="number">1100</span>, <span class="string">&#x27;2021-12-09&#x27;</span>, <span class="string">&#x27;10&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- insert overwrite partitioned table with static partition</span></span><br><span class="line"><span class="keyword">insert</span> overwrite hudi_cow_pt_tbl <span class="keyword">partition</span>(dt <span class="operator">=</span> <span class="string">&#x27;2021-12-09&#x27;</span>, hh<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>) <span class="keyword">select</span> <span class="number">13</span>, <span class="string">&#x27;a13&#x27;</span>, <span class="number">1100</span>;</span><br></pre></td></tr></table></figure>

<h3 id="其它命令"><a href="#其它命令" class="headerlink" title="其它命令"></a>其它命令</h3><h4 id="更改表-Alter-Table"><a href="#更改表-Alter-Table" class="headerlink" title="更改表(Alter Table)"></a>更改表(Alter Table)</h4><p>模式演变可以通过ALTER TABLE命令来实现.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Alter table name</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> oldTableName RENAME <span class="keyword">TO</span> newTableName</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Alter table add columns</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> tableIdentifier <span class="keyword">ADD</span> COLUMNS(colAndType (,colAndType)<span class="operator">*</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Alter table column type</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> tableIdentifier CHANGE <span class="keyword">COLUMN</span> colName colName colType</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Alter table properties</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> tableIdentifier <span class="keyword">SET</span> TBLPROPERTIES (key <span class="operator">=</span> <span class="string">&#x27;value&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--rename to:</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> hudi_cow_nonpcf_tbl RENAME <span class="keyword">TO</span> hudi_cow_nonpcf_tbl2;</span><br><span class="line"></span><br><span class="line"><span class="comment">--add column:</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> hudi_cow_nonpcf_tbl2 <span class="keyword">add</span> columns(remark string);</span><br><span class="line"></span><br><span class="line"><span class="comment">--change column:</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> hudi_cow_nonpcf_tbl2 change <span class="keyword">column</span> uuid uuid <span class="type">bigint</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--set properties;</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hudi_cow_nonpcf_tbl2 <span class="keyword">set</span> tblproperties (hoodie.keep.max.commits <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="分区-SQL命令"><a href="#分区-SQL命令" class="headerlink" title="分区 SQL命令"></a>分区 SQL命令</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Drop Partition</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> tableIdentifier <span class="keyword">DROP</span> <span class="keyword">PARTITION</span> ( partition_col_name <span class="operator">=</span> partition_col_val [ , ... ] )</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Show Partitions</span></span><br><span class="line"><span class="keyword">SHOW</span> PARTITIONS tableIdentifier</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--show partition:</span></span><br><span class="line"><span class="keyword">show</span> partitions hudi_cow_pt_tbl;</span><br><span class="line"></span><br><span class="line"><span class="comment">--drop partition:</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hudi_cow_pt_tbl <span class="keyword">drop</span> <span class="keyword">partition</span> (dt<span class="operator">=</span><span class="string">&#x27;2021-12-09&#x27;</span>, hh<span class="operator">=</span><span class="string">&#x27;10&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>目前,结果show partitions基于文件系统表路径.<br>删除整个分区数据或直接删除某个分区是不准确的.</p>
<h2 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h2><h3 id="设置-1"><a href="#设置-1" class="headerlink" title="设置"></a>设置</h3><p>对于 Spark 3.2 及更高版本,需要额外的 spark_catalog 配置.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Spark 2.4</span></span><br><span class="line">spark-shell \</span><br><span class="line">--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \</span><br><span class="line">--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Spark 3.2</span></span><br><span class="line">spark-shell \</span><br><span class="line">--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \</span><br><span class="line">--conf &#x27;spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog&#x27; \</span><br><span class="line">--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27;</span><br></pre></td></tr></table></figure>

<h3 id="模拟数据"><a href="#模拟数据" class="headerlink" title="模拟数据"></a>模拟数据</h3><p>导入Spark及Hudi相关包和定义变量(表的名称和数据存储路径).<br>其中构建DataGenerator对象,用于模拟生成Trip乘车数据.<br>模拟产生10条Trip乘车数据,为JSON格式.<br>将模拟数据List转换为DataFrame数据集.<br>查看转换后DataFrame数据集的Schema信息.<br>选择相关字段,查看模拟样本数据.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hudi.<span class="type">QuickstartUtils</span>._</span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceReadOptions</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceWriteOptions</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.hudi.config.<span class="type">HoodieWriteConfig</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.hudi.common.model.<span class="type">HoodieRecord</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tableName = <span class="string">&quot;hudi_trips_cow&quot;</span></span><br><span class="line"><span class="keyword">val</span> basePath = <span class="string">&quot;/hudi_trips_cow&quot;</span></span><br><span class="line"><span class="keyword">val</span> dataGen = <span class="keyword">new</span> <span class="type">DataGenerator</span></span><br><span class="line"><span class="keyword">val</span> inserts = convertToStringList(dataGen.generateInserts(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(inserts, <span class="number">2</span>))</span><br><span class="line">df.printSchema()</span><br><span class="line">df.select(<span class="string">&quot;rider&quot;</span>, <span class="string">&quot;begin_lat&quot;</span>, <span class="string">&quot;begin_lon&quot;</span>, <span class="string">&quot;driver&quot;</span>, <span class="string">&quot;fare&quot;</span>, <span class="string">&quot;uuid&quot;</span>, <span class="string">&quot;ts&quot;</span>).show(<span class="number">10</span>, truncate=<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.printSchema()</span><br><span class="line">root</span><br><span class="line"> |-- begin_lat: double (nullable &#x3D; true)</span><br><span class="line"> |-- begin_lon: double (nullable &#x3D; true)</span><br><span class="line"> |-- driver: string (nullable &#x3D; true)</span><br><span class="line"> |-- end_lat: double (nullable &#x3D; true)</span><br><span class="line"> |-- end_lon: double (nullable &#x3D; true)</span><br><span class="line"> |-- fare: double (nullable &#x3D; true)</span><br><span class="line"> |-- partitionpath: string (nullable &#x3D; true)</span><br><span class="line"> |-- rider: string (nullable &#x3D; true)</span><br><span class="line"> |-- ts: long (nullable &#x3D; true)</span><br><span class="line"> |-- uuid: string (nullable &#x3D; true)</span><br></pre></td></tr></table></figure>

<img src="/images/fly1180.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="创建表-1"><a href="#创建表-1" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; scala</span><br><span class="line">&#x2F;&#x2F; No separate create table command required in spark. First batch of write to a table will create the table if not exists. </span><br></pre></td></tr></table></figure>

<h3 id="插入数据-1"><a href="#插入数据-1" class="headerlink" title="插入数据"></a>插入数据</h3><p>将上述模拟产生Trip数据,保存到Hudi表中.<br>直接通过format指定数据源Source,设置相关属性保存数据即可,命令如下.</p>
<p>使用Scala交互式命令行中paste模式(:paste)粘贴代码,按ctrl+d退出并执行代码.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line">df.write.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  options(getQuickstartWriteConfigs).</span><br><span class="line">  option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">&quot;ts&quot;</span>).</span><br><span class="line">  option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">&quot;uuid&quot;</span>).</span><br><span class="line">  option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">&quot;partitionpath&quot;</span>).</span><br><span class="line">  option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">  mode(<span class="type">Overwrite</span>).</span><br><span class="line">  save(basePath)</span><br></pre></td></tr></table></figure>

<img src="/images/fly1022.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>其中相关参数说明如下:<br>getQuickstartWriteConfigs<br>设置写入/更新数据至Hudi时,Shuffle时分区数目<br><img src="/images/fly1018.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<p>PRECOMBINE_FIELD_OPT_KEY<br>数据合并时,依据主键字段<br><img src="/images/fly1019.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<p>RECORDKEY_FIELD_OPT_KEY<br>每条记录的唯一id,支持多个字段<br><img src="/images/fly1020.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<p>PARTITIONPATH_FIELD_OPT_KEY<br>用于存放数据的分区字段<br><img src="/images/fly1021.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<p>数据保存成功以后,查看HDFS文件系统目录<code>/hudi_trips_cow</code>,结构如下.<br>可以发现Hudi表数据存储在HDFS上,以PARQUET列式方式存储的.</p>
<img src="/images/fly1176.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1177.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1178.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1179.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1181.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>mode(Overwrite):如果表已经存在,则覆盖并重新创建表.<br>您可以检查下生成的数据<code>/hudi_trips_cow/&lt;region&gt;/&lt;country&gt;/&lt;city&gt;/</code>.<br>我们提供了记录键(uuid)、分区字段(region/country/city)和组合逻辑(ts),以确保行程记录在每个分区中都是唯一的.<br>这里我们使用默认的写操作:upsert.<br>如果您的工作负载没有更新,您还可以发布 insert或bulk_insert操作可能更快.</p>
<p>通过外部化配置文件<code>hudi-default.conf</code>,您可以将配置设置集中在一个配置文件中,而不是直接将配置设置传递给每个 Hudi 作业.</p>
<h3 id="查询数据-1"><a href="#查询数据-1" class="headerlink" title="查询数据"></a>查询数据</h3><p>将数据文件加载到 DataFrame 中.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line"><span class="keyword">val</span> tripsSnapshotDF = spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(basePath)</span><br><span class="line"><span class="comment">//打印获取Hudi表数据的Schema信息.</span></span><br><span class="line">tripsSnapshotDF.printSchema()</span><br></pre></td></tr></table></figure>

<p>比原先保存到Hudi表中数据多5个字段,这些字段属于Hudi管理数据时使用的相关字段.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; tripsSnapshotDF.printSchema()</span><br><span class="line">root</span><br><span class="line"> |-- _hoodie_commit_time: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_commit_seqno: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_record_key: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_partition_path: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_file_name: string (nullable &#x3D; true)</span><br><span class="line"> |-- begin_lat: double (nullable &#x3D; true)</span><br><span class="line"> |-- begin_lon: double (nullable &#x3D; true)</span><br><span class="line"> |-- driver: string (nullable &#x3D; true)</span><br><span class="line"> |-- end_lat: double (nullable &#x3D; true)</span><br><span class="line"> |-- end_lon: double (nullable &#x3D; true)</span><br><span class="line"> |-- fare: double (nullable &#x3D; true)</span><br><span class="line"> |-- rider: string (nullable &#x3D; true)</span><br><span class="line"> |-- ts: long (nullable &#x3D; true)</span><br><span class="line"> |-- uuid: string (nullable &#x3D; true)</span><br><span class="line"> |-- partitionpath: string (nullable &#x3D; true)</span><br></pre></td></tr></table></figure>

<p>将获取Hudi表数据DataFrame注册为临时视图,采用SQL方式依据业务查询分析数据.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tripsSnapshotDF.createOrReplaceTempView(<span class="string">&quot;hudi_trips_snapshot&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//乘车费用 大于 20 信息数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;select fare, begin_lon, begin_lat, ts from hudi_trips_snapshot where fare &gt; 20.0&quot;</span>).show()</span><br><span class="line"><span class="comment">//选取字段查询数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from hudi_trips_snapshot&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<img src="/images/fly1182.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>从 0.9.0 开始 hudi 已经支持 hudi 内置的 FileIndex:<code>HoodieFileIndex</code>来查询 hudi 表,支持分区剪枝和 metatable 查询.<br>这将有助于提高查询性能.<br>它还支持非全局查询路径,这意味着用户可以通过基本路径查询表,而无需在查询路径中指定&quot;*&quot;.<br>该功能默认为非全局查询路径启用.<br>对于全局查询路径,hudi 使用旧的查询路径.</p>
<h4 id="时间旅行查询-1"><a href="#时间旅行查询-1" class="headerlink" title="时间旅行查询"></a>时间旅行查询</h4><p>Hudi 从 0.9.0 开始支持时间旅行查询.<br>目前支持三种查询时间格式,如下所示.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark.read.</span><br><span class="line">  format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  option(<span class="string">&quot;as.of.instant&quot;</span>, <span class="string">&quot;20210728141108100&quot;</span>).</span><br><span class="line">  load(basePath)</span><br><span class="line"></span><br><span class="line">spark.read.</span><br><span class="line">  format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  option(<span class="string">&quot;as.of.instant&quot;</span>, <span class="string">&quot;2021-07-28 14:11:08.200&quot;</span>).</span><br><span class="line">  load(basePath)</span><br><span class="line"></span><br><span class="line"><span class="comment">// It is equal to &quot;as.of.instant = 2021-07-28 00:00:00&quot;</span></span><br><span class="line">spark.read.</span><br><span class="line">  format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  option(<span class="string">&quot;as.of.instant&quot;</span>, <span class="string">&quot;2021-07-28&quot;</span>).</span><br><span class="line">  load(basePath)</span><br></pre></td></tr></table></figure>

<h3 id="更新数据-Update-data-1"><a href="#更新数据-Update-data-1" class="headerlink" title="更新数据(Update data)"></a>更新数据(Update data)</h3><p>这类似于插入新数据.<br>使用数据生成器生成对现有行程的更新,加载到 DataFrame 并将 DataFrame 写入 hudi 表.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line"><span class="keyword">val</span> updates = convertToStringList(dataGen.generateUpdates(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(updates, <span class="number">2</span>))</span><br><span class="line">df.write.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  options(getQuickstartWriteConfigs).</span><br><span class="line">  option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">&quot;ts&quot;</span>).</span><br><span class="line">  option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">&quot;uuid&quot;</span>).</span><br><span class="line">  option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">&quot;partitionpath&quot;</span>).</span><br><span class="line">  option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">  mode(<span class="type">Append</span>).</span><br><span class="line">  save(basePath)</span><br></pre></td></tr></table></figure>

<img src="/images/fly1183.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>保存模式现在是Append.<br>通常,除非您是第一次尝试创建表,否则请始终使用append模式.<br>现在再次查询数据将显示更新的行程.<br>每个写入操作都会生成一个由时间戳表示的新提交.<br>在_hoodie_commit_time, rider,drivers字段中查找先前提交中相同_hoodie_record_key的更改.</p>
<h3 id="增量查询-Incremental-query"><a href="#增量查询-Incremental-query" class="headerlink" title="增量查询(Incremental query)"></a>增量查询(Incremental query)</h3><p>Hudi 还提供了获取自给定提交时间戳以来更改的记录流的功能.<br>这可以使用 Hudi 的增量查询并提供需要流式传输更改的开始时间来实现.<br>如果我们想要在给定的提交之后进行所有更改(通常情况下),我们不需要指定 endTime.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line"><span class="comment">// reload data</span></span><br><span class="line">spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(basePath).createOrReplaceTempView(<span class="string">&quot;hudi_trips_snapshot&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> commits = spark.sql(<span class="string">&quot;select distinct(_hoodie_commit_time) as commitTime from hudi_trips_snapshot order by commitTime&quot;</span>).map(k =&gt; k.getString(<span class="number">0</span>)).take(<span class="number">50</span>)</span><br><span class="line"><span class="keyword">val</span> beginTime = commits(commits.length - <span class="number">2</span>) <span class="comment">// commit time we are interested in</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// incrementally query data</span></span><br><span class="line"><span class="keyword">val</span> tripsIncrementalDF = spark.read.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  option(<span class="type">QUERY_TYPE_OPT_KEY</span>, <span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>).</span><br><span class="line">  option(<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>, beginTime).</span><br><span class="line">  load(basePath)</span><br><span class="line">tripsIncrementalDF.createOrReplaceTempView(<span class="string">&quot;hudi_trips_incremental&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_incremental where fare &gt; 20.0&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<img src="/images/fly1184.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>这将给出 beginTime 提交后发生的所有更改,过滤器为 fare &gt; 20.0.<br>此功能的独特之处在于它现在允许您在批处理数据上创作流式管道.</p>
<h3 id="时间点查询-Point-in-time-query"><a href="#时间点查询-Point-in-time-query" class="headerlink" title="时间点查询(Point in time query)"></a>时间点查询(Point in time query)</h3><p>查询特定时间的数据.<br>具体时间可以通过将 endTime 指向特定提交时间并将 beginTime 指向&quot;000&quot;(表示最早可能的提交时间)来表示.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line"><span class="keyword">val</span> beginTime = <span class="string">&quot;000&quot;</span> <span class="comment">// Represents all commits &gt; this time.</span></span><br><span class="line"><span class="keyword">val</span> endTime = commits(commits.length - <span class="number">2</span>) <span class="comment">// commit time we are interested in</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//incrementally query data</span></span><br><span class="line"><span class="keyword">val</span> tripsPointInTimeDF = spark.read.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  option(<span class="type">QUERY_TYPE_OPT_KEY</span>, <span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>).</span><br><span class="line">  option(<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>, beginTime).</span><br><span class="line">  option(<span class="type">END_INSTANTTIME_OPT_KEY</span>, endTime).</span><br><span class="line">  load(basePath)</span><br><span class="line">tripsPointInTimeDF.createOrReplaceTempView(<span class="string">&quot;hudi_trips_point_in_time&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_point_in_time where fare &gt; 20.0&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<img src="/images/fly1185.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="删除数据-Delete-data-1"><a href="#删除数据-Delete-data-1" class="headerlink" title="删除数据(Delete data)"></a>删除数据(Delete data)</h3><p>Apache Hudi 支持两种类型的删除:<br>1)软删除:保留记录键并清空所有其他字段的值(软删除中为空的记录始终保存在存储中,永远不会删除).<br>2)硬删除:从表中物理删除记录的任何痕迹.</p>
<h4 id="软删除-Soft-Deletes"><a href="#软删除-Soft-Deletes" class="headerlink" title="软删除(Soft Deletes)"></a>软删除(Soft Deletes)</h4><p>保存模式是Append.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line">spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(basePath).createOrReplaceTempView(<span class="string">&quot;hudi_trips_snapshot&quot;</span>)</span><br><span class="line"><span class="comment">// fetch total records count</span></span><br><span class="line">spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot&quot;</span>).count() <span class="comment">//10</span></span><br><span class="line">spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot where rider is not null&quot;</span>).count() <span class="comment">//10</span></span><br><span class="line"><span class="comment">// fetch two records for soft deletes</span></span><br><span class="line"><span class="keyword">val</span> softDeleteDs = spark.sql(<span class="string">&quot;select * from hudi_trips_snapshot&quot;</span>).limit(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// prepare the soft deletes by ensuring the appropriate fields are nullified</span></span><br><span class="line"><span class="keyword">val</span> nullifyColumns = softDeleteDs.schema.fields.</span><br><span class="line">  map(field =&gt; (field.name, field.dataType.typeName)).</span><br><span class="line">  filter(pair =&gt; (!<span class="type">HoodieRecord</span>.<span class="type">HOODIE_META_COLUMNS</span>.contains(pair._1)</span><br><span class="line">    &amp;&amp; !<span class="type">Array</span>(<span class="string">&quot;ts&quot;</span>, <span class="string">&quot;uuid&quot;</span>, <span class="string">&quot;partitionpath&quot;</span>).contains(pair._1)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> softDeleteDf = nullifyColumns.</span><br><span class="line">  foldLeft(softDeleteDs.drop(<span class="type">HoodieRecord</span>.<span class="type">HOODIE_META_COLUMNS</span>: _*))(</span><br><span class="line">    (ds, col) =&gt; ds.withColumn(col._1, lit(<span class="literal">null</span>).cast(col._2)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// simply upsert the table after setting these fields to null</span></span><br><span class="line">softDeleteDf.write.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  options(getQuickstartWriteConfigs).</span><br><span class="line">  option(<span class="type">OPERATION_OPT_KEY</span>, <span class="string">&quot;upsert&quot;</span>).</span><br><span class="line">  option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">&quot;ts&quot;</span>).</span><br><span class="line">  option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">&quot;uuid&quot;</span>).</span><br><span class="line">  option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">&quot;partitionpath&quot;</span>).</span><br><span class="line">  option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">  mode(<span class="type">Append</span>).</span><br><span class="line">  save(basePath)</span><br><span class="line"></span><br><span class="line"><span class="comment">// reload data</span></span><br><span class="line">spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(basePath).createOrReplaceTempView(<span class="string">&quot;hudi_trips_snapshot&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// This should return the same total count as before</span></span><br><span class="line">spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot&quot;</span>).count() <span class="comment">//10</span></span><br><span class="line"><span class="comment">// This should return (total - 2) count as two records are updated with nulls</span></span><br><span class="line">spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot where rider is not null&quot;</span>).count() <span class="comment">//8</span></span><br></pre></td></tr></table></figure>

<h4 id="硬删除-Hard-Deletes-1"><a href="#硬删除-Hard-Deletes-1" class="headerlink" title="硬删除(Hard Deletes)"></a>硬删除(Hard Deletes)</h4><p>删除传入的 HoodieKeys 的记录.<br>删除操作仅Append支持模式.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line"><span class="comment">// fetch total records count</span></span><br><span class="line">spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot&quot;</span>).count() <span class="comment">//10</span></span><br><span class="line"><span class="comment">// fetch two records to be deleted</span></span><br><span class="line"><span class="keyword">val</span> ds = spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot&quot;</span>).limit(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// issue deletes</span></span><br><span class="line"><span class="keyword">val</span> deletes = dataGen.generateDeletes(ds.collectAsList())</span><br><span class="line"><span class="keyword">val</span> hardDeleteDf = spark.read.json(spark.sparkContext.parallelize(deletes, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">hardDeleteDf.write.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  options(getQuickstartWriteConfigs).</span><br><span class="line">  option(<span class="type">OPERATION_OPT_KEY</span>, <span class="string">&quot;delete&quot;</span>).</span><br><span class="line">  option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">&quot;ts&quot;</span>).</span><br><span class="line">  option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">&quot;uuid&quot;</span>).</span><br><span class="line">  option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">&quot;partitionpath&quot;</span>).</span><br><span class="line">  option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">  mode(<span class="type">Append</span>).</span><br><span class="line">  save(basePath)</span><br><span class="line"></span><br><span class="line"><span class="comment">// run the same read query as above.</span></span><br><span class="line"><span class="keyword">val</span> roAfterDeleteViewDF = spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(basePath)</span><br><span class="line"></span><br><span class="line">roAfterDeleteViewDF.registerTempTable(<span class="string">&quot;hudi_trips_snapshot&quot;</span>)</span><br><span class="line"><span class="comment">// fetch should return (total - 2) records</span></span><br><span class="line">spark.sql(<span class="string">&quot;select uuid, partitionpath from hudi_trips_snapshot&quot;</span>).count() <span class="comment">//8</span></span><br></pre></td></tr></table></figure>

<h3 id="插入覆盖-Insert-Overwrite-1"><a href="#插入覆盖-Insert-Overwrite-1" class="headerlink" title="插入覆盖(Insert Overwrite)"></a>插入覆盖(Insert Overwrite)</h3><p>生成一些新的行程,覆盖输入中存在的所有分区.<br>此操作可能比upsert批处理 ETL 作业更快,后者一次重新计算整个目标分区(与增量更新目标表相反).<br>这是因为,我们能够完全绕过 upsert 写入路径中的索引、预组合和其他重新分区步骤.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark-shell</span></span><br><span class="line">spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(basePath).select(<span class="string">&quot;uuid&quot;</span>,<span class="string">&quot;partitionpath&quot;</span>).sort(<span class="string">&quot;partitionpath&quot;</span>,<span class="string">&quot;uuid&quot;</span>).show(<span class="number">100</span>, <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> inserts = convertToStringList(dataGen.generateInserts(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(inserts, <span class="number">2</span>)).</span><br><span class="line">  filter(<span class="string">&quot;partitionpath = &#x27;americas/united_states/san_francisco&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.write.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  options(getQuickstartWriteConfigs).</span><br><span class="line">  option(<span class="type">OPERATION</span>.key(),<span class="string">&quot;insert_overwrite&quot;</span>).</span><br><span class="line">  option(<span class="type">PRECOMBINE_FIELD</span>.key(), <span class="string">&quot;ts&quot;</span>).</span><br><span class="line">  option(<span class="type">RECORDKEY_FIELD</span>.key(), <span class="string">&quot;uuid&quot;</span>).</span><br><span class="line">  option(<span class="type">PARTITIONPATH_FIELD</span>.key(), <span class="string">&quot;partitionpath&quot;</span>).</span><br><span class="line">  option(<span class="type">TBL_NAME</span>.key(), tableName).</span><br><span class="line">  mode(<span class="type">Append</span>).</span><br><span class="line">  save(basePath)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Should have different keys now for San Francisco alone, from query before.</span></span><br><span class="line">spark.</span><br><span class="line">  read.format(<span class="string">&quot;hudi&quot;</span>).</span><br><span class="line">  load(basePath).</span><br><span class="line">  select(<span class="string">&quot;uuid&quot;</span>,<span class="string">&quot;partitionpath&quot;</span>).</span><br><span class="line">  sort(<span class="string">&quot;partitionpath&quot;</span>,<span class="string">&quot;uuid&quot;</span>).</span><br><span class="line">  show(<span class="number">100</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<h1 id="批处理-hive"><a href="#批处理-hive" class="headerlink" title="批处理/hive"></a>批处理/hive</h1><h2 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h2><p>滴滴出行数据为2017年5月1日-10月31日(半年)海口市每天的订单数据,包含订单的起终点经纬度以及订单类型/出行品类/乘车人数的订单属性数据.<br>具体字段含义说明如下所示:</p>
<img src="/images/fly1046.png" width="600" style="margin-left: 0px; padding-bottom: 10px;">

<p>依据海口滴滴出行数据,按照如下需求统计分析:</p>
<img src="/images/fly1047.png" width="400" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="工具类SparkUtils"><a href="#工具类SparkUtils" class="headerlink" title="工具类SparkUtils"></a>工具类SparkUtils</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkSQL操作数据(加载读取和保存写入)时工具类,比如获取SparkSession实例对象等</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 构建SparkSession实例对象,默认情况下本地模式运行</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createSparkSession</span></span>(clazz: <span class="type">Class</span>[_],</span><br><span class="line">                         master: <span class="type">String</span> = <span class="string">&quot;local[4]&quot;</span>, partitions: <span class="type">Int</span> = <span class="number">4</span>): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">    <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(clazz.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      .master(master)</span><br><span class="line">      .config(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, partitions)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="日期转换星期"><a href="#日期转换星期" class="headerlink" title="日期转换星期"></a>日期转换星期</h3><p>查询分析指标中,需要将日期时间字段值,转换为星期,方便统计工作日和休息日滴滴出行情况.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.<span class="type">FastDateFormat</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>, <span class="type">Date</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将日期转换星期,例如输入:2021-06-24  -&gt; 星期四</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DayWeekTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dateStr: <span class="type">String</span> = <span class="string">&quot;2021-06-24&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> format: <span class="type">FastDateFormat</span> = <span class="type">FastDateFormat</span>.getInstance(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> calendar: <span class="type">Calendar</span> = <span class="type">Calendar</span>.getInstance()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> date: <span class="type">Date</span> = format.parse(dateStr)</span><br><span class="line">    calendar.setTime(date)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dayWeek: <span class="type">String</span> = calendar.get(<span class="type">Calendar</span>.<span class="type">DAY_OF_WEEK</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;星期日&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">2</span> =&gt; <span class="string">&quot;星期一&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="string">&quot;星期二&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">4</span> =&gt; <span class="string">&quot;星期三&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">5</span> =&gt; <span class="string">&quot;星期四&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">6</span> =&gt; <span class="string">&quot;星期五&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">7</span> =&gt; <span class="string">&quot;星期六&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(dayWeek)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="数据ETL保存"><a href="#数据ETL保存" class="headerlink" title="数据ETL保存"></a>数据ETL保存</h2><p>从本地文件系统LocalFS加载海口市滴滴出行数据,进行相应ETL转换,最终存储Hudi表.</p>
<h3 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h3><p>编写SparkSQL程序,实现数据ETL转换保存,分为如下5步:</p>
<ol>
<li>构建SparkSession实例对象(集成Hudi和HDFS)</li>
<li>加载本地CSV文件格式滴滴出行数据</li>
<li>滴滴出行数据ETL处理</li>
<li>保存转换后数据至Hudi表</li>
<li>应用结束关闭资源</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 滴滴海口出行运营数据分析,使用SparkSQL操作数据,先读取CSV文件,保存至Hudi表.</span></span><br><span class="line"><span class="comment"> * 1.数据集说明</span></span><br><span class="line"><span class="comment"> * 2017年5月1日-10月31日海口市每天的订单数据,包含订单的起终点经纬度以及订单类型/出行品类/乘车人数的订单属性数据.</span></span><br><span class="line"><span class="comment"> * 数据存储为CSV格式,首行为列名称</span></span><br><span class="line"><span class="comment"> * 2.开发主要步骤</span></span><br><span class="line"><span class="comment"> * step1.构建SparkSession实例对象(集成Hudi和HDFS)</span></span><br><span class="line"><span class="comment"> * step2.加载本地CSV文件格式滴滴出行数据</span></span><br><span class="line"><span class="comment"> * step3.滴滴出行数据ETL处理</span></span><br><span class="line"><span class="comment"> * stpe4.保存转换后数据至Hudi表</span></span><br><span class="line"><span class="comment"> * step5.应用结束关闭资源</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DidiStorageSpark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 滴滴数据路径</span></span><br><span class="line">  <span class="keyword">val</span> datasPath: <span class="type">String</span> = <span class="string">&quot;/datas/dwv_order_make_haikou_2.txt&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Hudi中表的属性</span></span><br><span class="line">  <span class="keyword">val</span> hudiTableName: <span class="type">String</span> = <span class="string">&quot;tbl_didi_haikou&quot;</span></span><br><span class="line">  <span class="keyword">val</span> hudiTablePath: <span class="type">String</span> = <span class="string">&quot;/hudi-warehouse/tbl_didi_haikou&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// step1.构建SparkSession实例对象(集成Hudi和HDFS)</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkUtils</span>.createSparkSession(<span class="keyword">this</span>.getClass)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step2.加载本地CSV文件格式滴滴出行数据</span></span><br><span class="line">    <span class="keyword">val</span> didiDF: <span class="type">DataFrame</span> = readCsvFile(spark, datasPath)</span><br><span class="line">     didiDF.printSchema()</span><br><span class="line">     didiDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step3.滴滴出行数据ETL处理并保存至Hudi表</span></span><br><span class="line">    <span class="keyword">val</span> etlDF: <span class="type">DataFrame</span> = process(didiDF)</span><br><span class="line">    etlDF.printSchema()</span><br><span class="line">    etlDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// stpe4.保存转换后数据至Hudi表</span></span><br><span class="line">    saveToHudi(etlDF, hudiTableName, hudiTablePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// stpe5.应用结束,关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="加载CSV数据"><a href="#加载CSV数据" class="headerlink" title="加载CSV数据"></a>加载CSV数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 读取CSV格式文本文件数据,封装到DataFrame数据集</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readCsvFile</span></span>(spark: <span class="type">SparkSession</span>, path: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  spark.read</span><br><span class="line">    <span class="comment">// 设置分隔符为逗号</span></span><br><span class="line">    .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\\t&quot;</span>)</span><br><span class="line">    <span class="comment">// 文件首行为列名称</span></span><br><span class="line">    .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    <span class="comment">// 依据数值自动推断数据类型</span></span><br><span class="line">    .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    <span class="comment">// 指定文件路径</span></span><br><span class="line">    .csv(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="数据ETL转换"><a href="#数据ETL转换" class="headerlink" title="数据ETL转换"></a>数据ETL转换</h3><p>编写方法,对滴滴出行数据ETL转换,添加字段ts和partitionpath,方便保存数据至Hudi表时,指定字段名称.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, concat_ws, unix_timestamp&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 对滴滴出行海口数据进行ETL转换操作:指定ts和partitionpath 列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  dataframe</span><br><span class="line">    <span class="comment">// 添加分区列:三级分区 -&gt; yyyy/MM/dd</span></span><br><span class="line">    .withColumn(</span><br><span class="line">      <span class="string">&quot;partitionpath&quot;</span>, <span class="comment">// 列名称</span></span><br><span class="line">      concat_ws(<span class="string">&quot;/&quot;</span>, col(<span class="string">&quot;year&quot;</span>), col(<span class="string">&quot;month&quot;</span>), col(<span class="string">&quot;day&quot;</span>)) <span class="comment">//</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 删除列:year, month, day</span></span><br><span class="line">    .drop(<span class="string">&quot;year&quot;</span>, <span class="string">&quot;month&quot;</span>, <span class="string">&quot;day&quot;</span>)</span><br><span class="line">    <span class="comment">// 添加timestamp列,作为Hudi表记录数据与合并时字段,使用发车时间</span></span><br><span class="line">    .withColumn(</span><br><span class="line">      <span class="string">&quot;ts&quot;</span>,</span><br><span class="line">      unix_timestamp(col(<span class="string">&quot;departure_time&quot;</span>), <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="保存数据至Hudi"><a href="#保存数据至Hudi" class="headerlink" title="保存数据至Hudi"></a>保存数据至Hudi</h3><p>编写方法,将ETL转换后数据,保存到Hudi表中,采用COW模式.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将数据集DataFrame保存值Hudi表中,表的类型:COW</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveToHudi</span></span>(dataframe: <span class="type">DataFrame</span>, table: <span class="type">String</span>, path: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 导入包</span></span><br><span class="line">  <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceWriteOptions</span>._</span><br><span class="line">  <span class="keyword">import</span> org.apache.hudi.config.<span class="type">HoodieWriteConfig</span>._</span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 保存数据</span></span><br><span class="line">  dataframe.write</span><br><span class="line">    .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">    .format(<span class="string">&quot;hudi&quot;</span>) <span class="comment">// 指定数据源为Hudi</span></span><br><span class="line">    .option(<span class="string">&quot;hoodie.insert.shuffle.parallelism&quot;</span>, <span class="string">&quot;2&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;hoodie.upsert.shuffle.parallelism&quot;</span>, <span class="string">&quot;2&quot;</span>)</span><br><span class="line">    <span class="comment">// Hudi 表的属性设置</span></span><br><span class="line">    .option(<span class="type">RECORDKEY_FIELD</span>.key(), <span class="string">&quot;order_id&quot;</span>)</span><br><span class="line">    .option(<span class="type">PRECOMBINE_FIELD</span>.key(), <span class="string">&quot;ts&quot;</span>)</span><br><span class="line">    .option(<span class="type">PARTITIONPATH_FIELD</span>.key(), <span class="string">&quot;partitionpath&quot;</span>)</span><br><span class="line">    <span class="comment">// 表的名称和路径</span></span><br><span class="line">    .option(<span class="type">TBL_NAME</span>.key(), table)</span><br><span class="line">    .save(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Hudi-表存储结构"><a href="#Hudi-表存储结构" class="headerlink" title="Hudi 表存储结构"></a>Hudi 表存储结构</h3><p>运行Spark程序,读取CSV格式数据,ETL转换后,保存至Hudi表,查看HDFS目录结构.</p>
<img src="/images/fly1048.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1049.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1050.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1051.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1052.png" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="指标查询分析"><a href="#指标查询分析" class="headerlink" title="指标查询分析"></a>指标查询分析</h2><p>按照查询分析指标,从Hudi表加载数据,进行分组聚合统计,分析结果.</p>
<h3 id="开发步骤-1"><a href="#开发步骤-1" class="headerlink" title="开发步骤"></a>开发步骤</h3><p>其中将加载Hudi表数据和各个指标统计,分别封装到不同的方法中.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>, <span class="type">Date</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.<span class="type">FastDateFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 滴滴海口出行运营数据分析,使用SparkSQL操作数据,从加载Hudi表数据,按照业务需求统计.</span></span><br><span class="line"><span class="comment"> *    -1. 数据集说明</span></span><br><span class="line"><span class="comment"> *        海口市每天的订单数据,包含订单的起终点经纬度以及订单类型/出行品类/乘车人数的订单属性数据.</span></span><br><span class="line"><span class="comment"> *        数据存储为CSV格式,首行为列名称</span></span><br><span class="line"><span class="comment"> *    -2. 开发主要步骤</span></span><br><span class="line"><span class="comment"> *      step1. 构建SparkSession实例对象(集成Hudi和HDFS)</span></span><br><span class="line"><span class="comment"> *      step2. 依据指定字段从Hudi表中加载数据</span></span><br><span class="line"><span class="comment"> *      step3. 按照业务指标进行数据统计分析</span></span><br><span class="line"><span class="comment"> *      step4. 应用结束关闭资源</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DidiAnalysisSpark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Hudi中表的属性</span></span><br><span class="line">  <span class="keyword">val</span> hudiTablePath: <span class="type">String</span> = <span class="string">&quot;/hudi-warehouse/tbl_didi_haikou&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// step1. 构建SparkSession实例对象(集成Hudi和HDFS)</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkUtils</span>.createSparkSession(<span class="keyword">this</span>.getClass, partitions = <span class="number">8</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step2. 依据指定字段从Hudi表中加载数据</span></span><br><span class="line">    <span class="keyword">val</span> hudiDF: <span class="type">DataFrame</span> = readFromHudi(spark, hudiTablePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step3. 按照业务指标进行数据统计分析</span></span><br><span class="line">    <span class="comment">// 指标1:订单类型统计</span></span><br><span class="line">    <span class="comment">// reportProduct(hudiDF)</span></span><br><span class="line">    <span class="comment">// 指标2:订单时效统计</span></span><br><span class="line">    <span class="comment">// reportType(hudiDF)</span></span><br><span class="line">    <span class="comment">// 指标3:交通类型统计</span></span><br><span class="line">    <span class="comment">//reportTraffic(hudiDF)</span></span><br><span class="line">    <span class="comment">// 指标4:订单价格统计</span></span><br><span class="line">    <span class="comment">//reportPrice(hudiDF)</span></span><br><span class="line">    <span class="comment">// 指标5:订单距离统计</span></span><br><span class="line">    <span class="comment">//reportDistance(hudiDF)</span></span><br><span class="line">    <span class="comment">// 指标6:日期类型:星期,进行统计</span></span><br><span class="line">    <span class="comment">//reportWeek(hudiDF)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// step4. 应用结束关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="加载Hudi表数据"><a href="#加载Hudi表数据" class="headerlink" title="加载Hudi表数据"></a>加载Hudi表数据</h3><p>编写方法,封装SparkSQL从Hudi表加载数据,其中过滤获取指标统计时所需字段.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从Hudi表加载数据,指定数据存在路径</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readFromHudi</span></span>(spark: <span class="type">SparkSession</span>, path: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="comment">// a. 指定路径,加载数据,封装至DataFrame</span></span><br><span class="line">  <span class="keyword">val</span> didiDF: <span class="type">DataFrame</span> = spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(path)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// b. 选择字段</span></span><br><span class="line">  didiDF</span><br><span class="line">    <span class="comment">// 选择字段</span></span><br><span class="line">    .select(</span><br><span class="line">      <span class="string">&quot;order_id&quot;</span>, <span class="string">&quot;product_id&quot;</span>, <span class="string">&quot;type&quot;</span>, <span class="string">&quot;traffic_type&quot;</span>, <span class="comment">//</span></span><br><span class="line">      <span class="string">&quot;pre_total_fee&quot;</span>, <span class="string">&quot;start_dest_distance&quot;</span>, <span class="string">&quot;departure_time&quot;</span> <span class="comment">//</span></span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="指标一-订单类型统计"><a href="#指标一-订单类型统计" class="headerlink" title="指标一:订单类型统计"></a>指标一:订单类型统计</h3><p>对海口市滴滴出行数据,按照订单类型统计,使用字段:product_id,其中值:<br>1滴滴专车, 2滴滴企业专车, 3滴滴快车, 4滴滴企业快车</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*  订单类型统计,字段:product_id</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reportProduct</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, udf&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// a. 按照产品线ID分组统计</span></span><br><span class="line"><span class="keyword">val</span> reportDF: <span class="type">DataFrame</span> = dataframe.groupBy(<span class="string">&quot;product_id&quot;</span>).count()</span><br><span class="line"></span><br><span class="line"><span class="comment">// b. 自定义UDF函数,转换名称</span></span><br><span class="line"><span class="keyword">val</span> to_name = udf(</span><br><span class="line">  <span class="comment">// 1滴滴专车, 2滴滴企业专车, 3滴滴快车, 4滴滴企业快车</span></span><br><span class="line">  (productId: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">    productId <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span> =&gt;  <span class="string">&quot;滴滴专车&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">2</span> =&gt;  <span class="string">&quot;滴滴企业专车&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">3</span> =&gt;  <span class="string">&quot;滴滴快车&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">4</span> =&gt;  <span class="string">&quot;滴滴企业快车&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// c. 转换名称,应用函数</span></span><br><span class="line"><span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = reportDF.select(</span><br><span class="line">  to_name(col(<span class="string">&quot;product_id&quot;</span>)).as(<span class="string">&quot;order_type&quot;</span>), <span class="comment">//</span></span><br><span class="line">  col(<span class="string">&quot;count&quot;</span>).as(<span class="string">&quot;total&quot;</span>) <span class="comment">//</span></span><br><span class="line">)</span><br><span class="line">resultDF.printSchema()</span><br><span class="line">resultDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- order_type: string (nullable &#x3D; true)</span><br><span class="line"> |-- total: long (nullable &#x3D; false)</span><br><span class="line"></span><br><span class="line">+----------+-------+</span><br><span class="line">|order_type|total  |</span><br><span class="line">+----------+-------+</span><br><span class="line">|滴滴快车  |1298383|</span><br><span class="line">|滴滴专车  |15615  |</span><br><span class="line">+----------+-------+</span><br></pre></td></tr></table></figure>

<h3 id="指标二-订单时效性统计"><a href="#指标二-订单时效性统计" class="headerlink" title="指标二:订单时效性统计"></a>指标二:订单时效性统计</h3><p>依据用户下单的时效型:type,分组聚合统计.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 订单时效性统计,字段:type</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reportType</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// a. 按照产品线ID分组统计</span></span><br><span class="line"><span class="keyword">val</span> reportDF: <span class="type">DataFrame</span> = dataframe.groupBy(<span class="string">&quot;type&quot;</span>).count()</span><br><span class="line"></span><br><span class="line"><span class="comment">// b. 自定义UDF函数,转换名称</span></span><br><span class="line"><span class="keyword">val</span> to_name = udf(</span><br><span class="line">  <span class="comment">// 0实时,1预约</span></span><br><span class="line">  (realtimeType: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">    realtimeType <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="string">&quot;实时&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;预约&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// c. 转换名称,应用函数</span></span><br><span class="line"><span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = reportDF.select(</span><br><span class="line">  to_name(col(<span class="string">&quot;type&quot;</span>)).as(<span class="string">&quot;order_realtime&quot;</span>), <span class="comment">//</span></span><br><span class="line">  col(<span class="string">&quot;count&quot;</span>).as(<span class="string">&quot;total&quot;</span>) <span class="comment">//</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">resultDF.printSchema()</span><br><span class="line">resultDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- order_realtime: string (nullable &#x3D; true)</span><br><span class="line"> |-- total: long (nullable &#x3D; false)</span><br><span class="line"></span><br><span class="line">+--------------+-------+</span><br><span class="line">|order_realtime|total  |</span><br><span class="line">+--------------+-------+</span><br><span class="line">|预约          |28488  |</span><br><span class="line">|实时          |1285510|</span><br><span class="line">+--------------+-------+</span><br></pre></td></tr></table></figure>

<h3 id="指标三-订单交通类型统计"><a href="#指标三-订单交通类型统计" class="headerlink" title="指标三:订单交通类型统计"></a>指标三:订单交通类型统计</h3><p>对海口市滴滴出行数据,按照交通类型:traffic_type,分组聚合统计.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 交通类型统计,字段:traffic_type</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reportTraffic</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// a. 按照产品线ID分组统计</span></span><br><span class="line">  <span class="keyword">val</span> reportDF: <span class="type">DataFrame</span> = dataframe.groupBy(<span class="string">&quot;traffic_type&quot;</span>).count()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// b. 自定义UDF函数,转换名称</span></span><br><span class="line">  <span class="keyword">val</span> to_name = udf(</span><br><span class="line">    <span class="comment">// 1企业时租,2企业接机套餐,3企业送机套餐,4拼车,5接机,6送机,302跨城拼车</span></span><br><span class="line">    (trafficType: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">      trafficType <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="string">&quot;普通散客&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;企业时租&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span> =&gt; <span class="string">&quot;企业接机套餐&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="string">&quot;企业送机套餐&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">4</span> =&gt; <span class="string">&quot;拼车&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">5</span> =&gt; <span class="string">&quot;接机&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span> =&gt; <span class="string">&quot;送机&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">302</span> =&gt; <span class="string">&quot;跨城拼车&quot;</span></span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">&quot;未知&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// c. 转换名称,应用函数</span></span><br><span class="line">  <span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = reportDF.select(</span><br><span class="line">    to_name(col(<span class="string">&quot;traffic_type&quot;</span>)).as(<span class="string">&quot;traffic_type&quot;</span>), <span class="comment">//</span></span><br><span class="line">    col(<span class="string">&quot;count&quot;</span>).as(<span class="string">&quot;total&quot;</span>) <span class="comment">//</span></span><br><span class="line">  )</span><br><span class="line">  resultDF.printSchema()</span><br><span class="line">  resultDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- traffic_type: string (nullable &#x3D; true)</span><br><span class="line"> |-- total: long (nullable &#x3D; false)</span><br><span class="line"></span><br><span class="line">+------------+-------+</span><br><span class="line">|traffic_type|total  |</span><br><span class="line">+------------+-------+</span><br><span class="line">|送机        |37469  |</span><br><span class="line">|接机        |19694  |</span><br><span class="line">|普通散客    |1256835|</span><br><span class="line">+------------+-------+</span><br></pre></td></tr></table></figure>

<h3 id="指标四-订单价格统计"><a href="#指标四-订单价格统计" class="headerlink" title="指标四:订单价格统计"></a>指标四:订单价格统计</h3><p>对滴滴出行订单数据,依据价格划分不同级别,分组聚合统计.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 订单价格统计,将价格分阶段统计,字段:pre_total_fee</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reportPrice</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = dataframe</span><br><span class="line">    .agg(</span><br><span class="line">      <span class="comment">// 价格:0 ~ 15</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;pre_total_fee&quot;</span>).between(<span class="number">0</span>, <span class="number">15</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;0~15&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:16 ~ 30</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;pre_total_fee&quot;</span>).between(<span class="number">16</span>, <span class="number">30</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;16~30&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:31 ~ 50</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;pre_total_fee&quot;</span>).between(<span class="number">31</span>, <span class="number">50</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;31~50&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:50 ~ 100</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;pre_total_fee&quot;</span>).between(<span class="number">51</span>, <span class="number">100</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;51~100&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:100+</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;pre_total_fee&quot;</span>).gt(<span class="number">100</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;100+&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  resultDF.printSchema()</span><br><span class="line">  resultDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- 0~15: long (nullable &#x3D; true)</span><br><span class="line"> |-- 16~30: long (nullable &#x3D; true)</span><br><span class="line"> |-- 31~50: long (nullable &#x3D; true)</span><br><span class="line"> |-- 51~100: long (nullable &#x3D; true)</span><br><span class="line"> |-- 100+: long (nullable &#x3D; true)</span><br><span class="line"></span><br><span class="line">+------+------+-----+------+----+</span><br><span class="line">|0~15  |16~30 |31~50|51~100|100+|</span><br><span class="line">+------+------+-----+------+----+</span><br><span class="line">|605354|532553|96559|58172 |4746|</span><br><span class="line">+------+------+-----+------+----+</span><br></pre></td></tr></table></figure>

<h3 id="指标五-订单距离统计"><a href="#指标五-订单距离统计" class="headerlink" title="指标五:订单距离统计"></a>指标五:订单距离统计</h3><p>对滴滴出行数据,按照每次订单行程距离,划分不同分段范围,分组聚合统计.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 订单距离统计,将价格分阶段统计,字段:start_dest_distance</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reportDistance</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = dataframe</span><br><span class="line">    .agg(</span><br><span class="line">      <span class="comment">// 价格:0 ~ 15</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;start_dest_distance&quot;</span>).between(<span class="number">0</span>, <span class="number">10000</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;0~10km&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:16 ~ 30</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;start_dest_distance&quot;</span>).between(<span class="number">10001</span>, <span class="number">20000</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;10~20km&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:31 ~ 50</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;start_dest_distance&quot;</span>).between(<span class="number">200001</span>, <span class="number">30000</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;20~30km&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:50 ~ 100</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;start_dest_distance&quot;</span>).between(<span class="number">30001</span>, <span class="number">5000</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;30~50km&quot;</span>),</span><br><span class="line">      <span class="comment">// 价格:100+</span></span><br><span class="line">      sum(</span><br><span class="line">        when(</span><br><span class="line">          col(<span class="string">&quot;start_dest_distance&quot;</span>).gt(<span class="number">50000</span>), <span class="number">1</span></span><br><span class="line">        ).otherwise(<span class="number">0</span>)</span><br><span class="line">      ).as(<span class="string">&quot;50+km&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  resultDF.printSchema()</span><br><span class="line">  resultDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- 0~10km: long (nullable &#x3D; true)</span><br><span class="line"> |-- 10~20km: long (nullable &#x3D; true)</span><br><span class="line"> |-- 20~30km: long (nullable &#x3D; true)</span><br><span class="line"> |-- 30~50km: long (nullable &#x3D; true)</span><br><span class="line"> |-- 50+km: long (nullable &#x3D; true)</span><br><span class="line"></span><br><span class="line">+-------+-------+-------+-------+-----+</span><br><span class="line">|0~10km |10~20km|20~30km|30~50km|50+km|</span><br><span class="line">+-------+-------+-------+-------+-----+</span><br><span class="line">|1102204|167873 |0      |0      |636  |</span><br><span class="line">+-------+-------+-------+-------+-----+</span><br></pre></td></tr></table></figure>

<h3 id="指标六-订单星期统计"><a href="#指标六-订单星期统计" class="headerlink" title="指标六:订单星期统计"></a>指标六:订单星期统计</h3><p>转换日期为星期,分组聚合统计,查看工作日和休息,滴滴出情况.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 订单星期分组统计,字段:departure_time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reportWeek</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// a. 自定义UDF函数,转换日期为星期</span></span><br><span class="line">  <span class="keyword">val</span> to_week: <span class="type">UserDefinedFunction</span> = udf(</span><br><span class="line">    <span class="comment">// 0实时,1预约</span></span><br><span class="line">    (dateStr: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> format: <span class="type">FastDateFormat</span> = <span class="type">FastDateFormat</span>.getInstance(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> calendar: <span class="type">Calendar</span> = <span class="type">Calendar</span>.getInstance()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> date: <span class="type">Date</span> = format.parse(dateStr)</span><br><span class="line">      calendar.setTime(date)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> dayWeek: <span class="type">String</span> = calendar.get(<span class="type">Calendar</span>.<span class="type">DAY_OF_WEEK</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;星期日&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span> =&gt; <span class="string">&quot;星期一&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="string">&quot;星期二&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">4</span> =&gt; <span class="string">&quot;星期三&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">5</span> =&gt; <span class="string">&quot;星期四&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span> =&gt; <span class="string">&quot;星期五&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">7</span> =&gt; <span class="string">&quot;星期六&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回星期</span></span><br><span class="line">      dayWeek</span><br><span class="line">    &#125;</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// b. 转换日期为星期,并分组和统计</span></span><br><span class="line">  <span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = dataframe</span><br><span class="line">    .select(</span><br><span class="line">      to_week(col(<span class="string">&quot;departure_time&quot;</span>)).as(<span class="string">&quot;week&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    .groupBy(col(<span class="string">&quot;week&quot;</span>)).count()</span><br><span class="line">    .select(</span><br><span class="line">      col(<span class="string">&quot;week&quot;</span>), col(<span class="string">&quot;count&quot;</span>).as(<span class="string">&quot;total&quot;</span>) <span class="comment">//</span></span><br><span class="line">    )</span><br><span class="line">  resultDF.printSchema()</span><br><span class="line">  resultDF.show(<span class="number">10</span>, truncate = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- week: string (nullable &#x3D; true)</span><br><span class="line"> |-- total: long (nullable &#x3D; false)</span><br><span class="line"></span><br><span class="line">+------+------+</span><br><span class="line">|week  |total |</span><br><span class="line">+------+------+</span><br><span class="line">|星期日|137174|</span><br><span class="line">|星期四|197344|</span><br><span class="line">|星期一|185065|</span><br><span class="line">|星期三|175714|</span><br><span class="line">|星期二|185391|</span><br><span class="line">|星期六|228930|</span><br><span class="line">|星期五|204380|</span><br><span class="line">+------+------+</span><br></pre></td></tr></table></figure>

<h2 id="集成Hive查询"><a href="#集成Hive查询" class="headerlink" title="集成Hive查询"></a>集成Hive查询</h2><p>集成Hive表数据,从Hudi表读取数据.</p>
<img src="/images/fly1053.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="创建表及查询"><a href="#创建表及查询" class="headerlink" title="创建表及查询"></a>创建表及查询</h3><p>在Hive中创建表,关联至Hudi表,需要将集成JAR包:hudi-hadoop-mr-bundle-0.12.0.jar,放入至$HIVE_HOME/lib目录下.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hudi-hadoop-mr-bundle-0.12.0.jar /opt/hive/lib/</span><br></pre></td></tr></table></figure>

<p>拷贝依赖包到 Hive 路径是为了 Hive 能够正常读到 Hudi 的数据,至此服务器环境准备完毕.<br>前面Spark 将滴滴出行数据写到Hudi表,想要通过Hive访问到这块数据,就需要创建一个Hive外部表,因为 Hudi 配置了分区,所以为了能读到所有的数据,此时外部表也得分区,分区字段名可随意配置.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database db_hudi;</span><br><span class="line">use db_hudi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> tbl_hudi_didi(</span><br><span class="line">    order_id <span class="type">bigint</span>          ,</span><br><span class="line">    product_id <span class="type">int</span>           ,</span><br><span class="line">    city_id <span class="type">int</span>              ,</span><br><span class="line">    district <span class="type">int</span>             ,</span><br><span class="line">    county <span class="type">int</span>               ,</span><br><span class="line">    type <span class="type">int</span>                 ,</span><br><span class="line">    combo_type <span class="type">int</span>           ,</span><br><span class="line">    traffic_type <span class="type">int</span>         ,</span><br><span class="line">    passenger_count <span class="type">int</span>      ,</span><br><span class="line">    driver_product_id <span class="type">int</span>    ,</span><br><span class="line">    start_dest_distance <span class="type">int</span>  ,</span><br><span class="line">    arrive_time string       ,</span><br><span class="line">    departure_time <span class="type">bigint</span>    ,</span><br><span class="line">    pre_total_fee <span class="keyword">double</span>     ,</span><br><span class="line">    normal_time string       ,</span><br><span class="line">    bubble_trace_id string   ,</span><br><span class="line">    product_1level <span class="type">int</span>       ,</span><br><span class="line">    dest_lng <span class="keyword">double</span>          ,</span><br><span class="line">    dest_lat <span class="keyword">double</span>          ,</span><br><span class="line">    starting_lng <span class="keyword">double</span>      ,</span><br><span class="line">    starting_lat <span class="keyword">double</span>      ,</span><br><span class="line">    partitionpath string     ,</span><br><span class="line">    ts <span class="type">bigint</span>                </span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> ( </span><br><span class="line">  `yarn_str` string, `month_str` string, `day_str` string)</span><br><span class="line"><span class="type">ROW</span> FORMAT SERDE </span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&#x27;</span> </span><br><span class="line">STORED <span class="keyword">AS</span> INPUTFORMAT </span><br><span class="line">  <span class="string">&#x27;org.apache.hudi.hadoop.HoodieParquetInputFormat&#x27;</span> </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&#x27;</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou&#x27;</span> ;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;22&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/22&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;23&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/23&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;24&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/24&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;25&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/25&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;26&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/26&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;27&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/27&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;28&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/28&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;29&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/29&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;30&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/30&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;31&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/5/31&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;1&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/1&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;2&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/2&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;3&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/3&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;4&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/4&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;5&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/5&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/6&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;7&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/7&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;8&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/8&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;9&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/9&#x27;</span> ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> db_hudi.tbl_hudi_didi <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(yarn_str<span class="operator">=</span><span class="string">&#x27;2017&#x27;</span>, month_str<span class="operator">=</span><span class="string">&#x27;6&#x27;</span>, day_str<span class="operator">=</span><span class="string">&#x27;10&#x27;</span>) location <span class="string">&#x27;/hudi-warehouse/tbl_didi_haikou/2017/6/10&#x27;</span> ;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> partitions tbl_hudi_didi ;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> form tbl_hudi_didi limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<p>Hive表数据与Hudi表数据关联成功,可以在Hive中编写SQL语句分析Hudi数据,SELECT语句查询表的数据.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 设置非严格模式</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode <span class="operator">=</span> nonstrict ;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">SQL</span>查询前<span class="number">10</span>条数据</span><br><span class="line"><span class="keyword">select</span> order_id, product_id, type, traffic_type, pre_total_fee, start_dest_distance, departure_time </span><br><span class="line"><span class="keyword">from</span> db_hudi.tbl_hudi_didi limit <span class="number">10</span> ;</span><br></pre></td></tr></table></figure>

<img src="/images/fly1054.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="HiveQL-分析"><a href="#HiveQL-分析" class="headerlink" title="HiveQL 分析"></a>HiveQL 分析</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置Hive本地模式</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.tasks.max<span class="operator">=</span><span class="number">10</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max<span class="operator">=</span><span class="number">50000000</span>;</span><br></pre></td></tr></table></figure>

<h4 id="指标一-订单类型统计-1"><a href="#指标一-订单类型统计-1" class="headerlink" title="指标一:订单类型统计"></a>指标一:订单类型统计</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> tmp <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> product_id, <span class="built_in">COUNT</span>(<span class="number">1</span>) <span class="keyword">AS</span> total <span class="keyword">FROM</span> db_hudi.tbl_hudi_didi <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_id</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  <span class="keyword">CASE</span> product_id</span><br><span class="line">    <span class="keyword">WHEN</span> <span class="number">1</span> <span class="keyword">THEN</span> &quot;滴滴专车&quot;</span><br><span class="line">    <span class="keyword">WHEN</span> <span class="number">2</span> <span class="keyword">THEN</span> &quot;滴滴企业专车&quot;</span><br><span class="line">    <span class="keyword">WHEN</span> <span class="number">3</span> <span class="keyword">THEN</span> &quot;滴滴快车&quot;</span><br><span class="line">    <span class="keyword">WHEN</span> <span class="number">4</span> <span class="keyword">THEN</span> &quot;滴滴企业快车&quot;</span><br><span class="line">  <span class="keyword">END</span> <span class="keyword">AS</span> order_type,</span><br><span class="line">  total</span><br><span class="line"><span class="keyword">FROM</span> tmp ;</span><br></pre></td></tr></table></figure>

<p>滴滴专车  15615<br>滴滴快车  1298383</p>
<h4 id="指标二-订单时效性统计-1"><a href="#指标二-订单时效性统计-1" class="headerlink" title="指标二:订单时效性统计"></a>指标二:订单时效性统计</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> tmp <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> type <span class="keyword">AS</span> order_realtime, <span class="built_in">COUNT</span>(<span class="number">1</span>) <span class="keyword">AS</span> total <span class="keyword">FROM</span> db_hudi.tbl_hudi_didi <span class="keyword">GROUP</span> <span class="keyword">BY</span> type</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  <span class="keyword">CASE</span> order_realtime</span><br><span class="line">    <span class="keyword">WHEN</span> <span class="number">0</span> <span class="keyword">THEN</span> &quot;实时&quot;</span><br><span class="line">    <span class="keyword">WHEN</span> <span class="number">1</span> <span class="keyword">THEN</span> &quot;预约&quot;</span><br><span class="line">  <span class="keyword">END</span> <span class="keyword">AS</span> order_realtime,</span><br><span class="line">  total</span><br><span class="line"><span class="keyword">FROM</span> tmp ;</span><br></pre></td></tr></table></figure>

<p>实时  1285510<br>预约  28488</p>
<h4 id="指标三-订单交通类型统计-1"><a href="#指标三-订单交通类型统计-1" class="headerlink" title="指标三:订单交通类型统计"></a>指标三:订单交通类型统计</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> tmp <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> traffic_type, <span class="built_in">COUNT</span>(<span class="number">1</span>) <span class="keyword">AS</span> total <span class="keyword">FROM</span> db_hudi.tbl_hudi_didi <span class="keyword">GROUP</span> <span class="keyword">BY</span> traffic_type</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  <span class="keyword">CASE</span> traffic_type</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">0</span> <span class="keyword">THEN</span>  &quot;普通散客&quot; </span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">1</span> <span class="keyword">THEN</span>  &quot;企业时租&quot;</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">2</span> <span class="keyword">THEN</span>  &quot;企业接机套餐&quot;</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">3</span> <span class="keyword">THEN</span>  &quot;企业送机套餐&quot;</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">4</span> <span class="keyword">THEN</span>  &quot;拼车&quot;</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">5</span> <span class="keyword">THEN</span>  &quot;接机&quot;</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">6</span> <span class="keyword">THEN</span>  &quot;送机&quot;</span><br><span class="line">   <span class="keyword">WHEN</span> <span class="number">302</span> <span class="keyword">THEN</span>  &quot;跨城拼车&quot;</span><br><span class="line">   <span class="keyword">ELSE</span> &quot;未知&quot;</span><br><span class="line">  <span class="keyword">END</span> <span class="keyword">AS</span> traffic_type,</span><br><span class="line">  total</span><br><span class="line"><span class="keyword">FROM</span> tmp ;</span><br></pre></td></tr></table></figure>

<p>普通散客  1256835<br>接机  19694<br>送机  37469</p>
<h4 id="指标四-订单价格统计-1"><a href="#指标四-订单价格统计-1" class="headerlink" title="指标四:订单价格统计"></a>指标四:订单价格统计</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  <span class="built_in">SUM</span>(</span><br><span class="line">    <span class="keyword">CASE</span> <span class="keyword">WHEN</span> pre_total_fee <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">AND</span> <span class="number">15</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span></span><br><span class="line">  ) <span class="keyword">AS</span> <span class="number">0</span>_15,</span><br><span class="line">  <span class="built_in">SUM</span>(</span><br><span class="line">    <span class="keyword">CASE</span> <span class="keyword">WHEN</span> pre_total_fee <span class="keyword">BETWEEN</span> <span class="number">16</span> <span class="keyword">AND</span> <span class="number">30</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span></span><br><span class="line">  ) <span class="keyword">AS</span> <span class="number">16</span>_30,</span><br><span class="line">  <span class="built_in">SUM</span>(</span><br><span class="line">    <span class="keyword">CASE</span> <span class="keyword">WHEN</span> pre_total_fee <span class="keyword">BETWEEN</span> <span class="number">31</span> <span class="keyword">AND</span> <span class="number">50</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span></span><br><span class="line">  ) <span class="keyword">AS</span> <span class="number">31</span>_150,</span><br><span class="line">  <span class="built_in">SUM</span>(</span><br><span class="line">    <span class="keyword">CASE</span> <span class="keyword">WHEN</span> pre_total_fee <span class="keyword">BETWEEN</span> <span class="number">51</span> <span class="keyword">AND</span> <span class="number">100</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span></span><br><span class="line">  ) <span class="keyword">AS</span> <span class="number">51</span>_100,</span><br><span class="line">  <span class="built_in">SUM</span>(</span><br><span class="line">    <span class="keyword">CASE</span> <span class="keyword">WHEN</span> pre_total_fee <span class="operator">&gt;</span> <span class="number">100</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span></span><br><span class="line">  )  <span class="keyword">AS</span> <span class="number">100</span>_</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">  db_hudi.tbl_hudi_didi;</span><br></pre></td></tr></table></figure>

<p>605277  532553  96559 58172 4746</p>
<h1 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h1><p>整合Spark StructuredStreaming与Hudi,实时将流式数据写入Hudi表中,对每批次数据batch DataFrame,采用Spark DataSource方式写入数据.</p>
<p>属性参数说明:<a target="_blank" rel="noopener" href="https://hudi.apache.org/docs/writing_data#datasource-writer">https://hudi.apache.org/docs/writing_data#datasource-writer</a></p>
<h2 id="模拟交易订单"><a href="#模拟交易订单" class="headerlink" title="模拟交易订单"></a>模拟交易订单</h2><p>编程模拟生成交易订单数据,实时发送Kafka Topic,为了简单起见交易订单数据字段如下,封装到样例类OrderRecord中.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderRecord</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    orderId: <span class="type">String</span>, //订单<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">    userId: <span class="type">String</span>, //用户<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">    orderTime: <span class="type">String</span>, //订单日期时间</span></span></span><br><span class="line"><span class="class"><span class="params">    ip: <span class="type">String</span>, //下单<span class="type">IP</span>地址</span></span></span><br><span class="line"><span class="class"><span class="params">    orderMoney: <span class="type">Double</span>, //订单金额</span></span></span><br><span class="line"><span class="class"><span class="params">    orderStatus: <span class="type">Int</span> //订单状态</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure>

<p>实时产生交易订单数据,使用Json4J类库转换数据为JSON字符,发送Kafka Topic中.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.<span class="type">FastDateFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringSerializer</span></span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">Json</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 模拟生产订单数据,发送到Kafka Topic中</span></span><br><span class="line"><span class="comment"> * Topic中每条数据Message类型为String,以JSON格式数据发送</span></span><br><span class="line"><span class="comment"> * 数据转换:</span></span><br><span class="line"><span class="comment"> * 将Order类实例对象转换为JSON格式字符串数据(可以使用json4s类库)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MockOrderProducer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> producer: <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1. Kafka Client Producer 配置信息</span></span><br><span class="line">      <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">      props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;oceanbase004:9092&quot;</span>)</span><br><span class="line">      props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;1&quot;</span>)</span><br><span class="line">      props.put(<span class="string">&quot;retries&quot;</span>, <span class="string">&quot;3&quot;</span>)</span><br><span class="line">      props.put(<span class="string">&quot;key.serializer&quot;</span>, classOf[<span class="type">StringSerializer</span>].getName)</span><br><span class="line">      props.put(<span class="string">&quot;value.serializer&quot;</span>, classOf[<span class="type">StringSerializer</span>].getName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2. 创建KafkaProducer对象,传入配置信息</span></span><br><span class="line">      producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 随机数实例对象</span></span><br><span class="line">      <span class="keyword">val</span> random: <span class="type">Random</span> = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">      <span class="comment">// 订单状态:订单打开 0,订单取消 1,订单关闭 2,订单完成 3</span></span><br><span class="line">      <span class="keyword">val</span> allStatus = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// 每次循环 模拟产生的订单数目</span></span><br><span class="line">        <span class="keyword">val</span> batchNumber: <span class="type">Int</span> = random.nextInt(<span class="number">1</span>) + <span class="number">5</span></span><br><span class="line">        (<span class="number">1</span> to batchNumber).foreach &#123; number =&gt;</span><br><span class="line">          <span class="keyword">val</span> currentTime: <span class="type">Long</span> = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">          <span class="keyword">val</span> orderId: <span class="type">String</span> = <span class="string">s&quot;<span class="subst">$&#123;getDate(currentTime)&#125;</span>%06d&quot;</span>.format(number)</span><br><span class="line">          <span class="keyword">val</span> userId: <span class="type">String</span> = <span class="string">s&quot;<span class="subst">$&#123;1 + random.nextInt(5)&#125;</span>%08d&quot;</span>.format(random.nextInt(<span class="number">1000</span>))</span><br><span class="line">          <span class="keyword">val</span> orderTime: <span class="type">String</span> = getDate(currentTime, format = <span class="string">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>)</span><br><span class="line">          <span class="keyword">val</span> orderMoney: <span class="type">String</span> = <span class="string">s&quot;<span class="subst">$&#123;5 + random.nextInt(500)&#125;</span>.%02d&quot;</span>.format(random.nextInt(<span class="number">100</span>))</span><br><span class="line">          <span class="keyword">val</span> orderStatus: <span class="type">Int</span> = allStatus(random.nextInt(allStatus.length))</span><br><span class="line">          <span class="comment">// 3. 订单记录数据</span></span><br><span class="line">          <span class="keyword">val</span> orderRecord: <span class="type">OrderRecord</span> = <span class="type">OrderRecord</span>(</span><br><span class="line">            orderId, userId, orderTime, getRandomIp, orderMoney.toDouble, orderStatus</span><br><span class="line">          )</span><br><span class="line">          <span class="comment">// 转换为JSON格式数据</span></span><br><span class="line">          <span class="keyword">val</span> orderJson = <span class="keyword">new</span> <span class="type">Json</span>(org.json4s.<span class="type">DefaultFormats</span>).write(orderRecord)</span><br><span class="line">          println(orderJson)</span><br><span class="line">          <span class="comment">// 4. 构建ProducerRecord对象</span></span><br><span class="line">          <span class="keyword">val</span> record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">&quot;xx1&quot;</span>, orderId, orderJson)</span><br><span class="line">          <span class="comment">// 5. 发送数据:def send(messages: KeyedMessage[K,V]*), 将数据发送到Topic</span></span><br><span class="line">          producer.send(record)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">Thread</span>.sleep(random.nextInt(<span class="number">500</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> != producer) producer.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** =================获取当前时间================= */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getDate</span></span>(time: <span class="type">Long</span>, format: <span class="type">String</span> = <span class="string">&quot;yyyyMMddHHmmssSSS&quot;</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> fastFormat: <span class="type">FastDateFormat</span> = <span class="type">FastDateFormat</span>.getInstance(format)</span><br><span class="line">    <span class="keyword">val</span> formatDate: <span class="type">String</span> = fastFormat.format(time) <span class="comment">// 格式化日期</span></span><br><span class="line">    formatDate</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** ================= 获取随机IP地址 ================= */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getRandomIp</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="comment">// ip范围</span></span><br><span class="line">    <span class="keyword">val</span> range: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>(</span><br><span class="line">      (<span class="number">607649792</span>, <span class="number">608174079</span>), <span class="comment">//36.56.0.0-36.63.255.255</span></span><br><span class="line">      (<span class="number">1038614528</span>, <span class="number">1039007743</span>), <span class="comment">//61.232.0.0-61.237.255.255</span></span><br><span class="line">      (<span class="number">1783627776</span>, <span class="number">1784676351</span>), <span class="comment">//106.80.0.0-106.95.255.255</span></span><br><span class="line">      (<span class="number">2035023872</span>, <span class="number">2035154943</span>), <span class="comment">//121.76.0.0-121.77.255.255</span></span><br><span class="line">      (<span class="number">2078801920</span>, <span class="number">2079064063</span>), <span class="comment">//123.232.0.0-123.235.255.255</span></span><br><span class="line">      (<span class="number">-1950089216</span>, <span class="number">-1948778497</span>), <span class="comment">//139.196.0.0-139.215.255.255</span></span><br><span class="line">      (<span class="number">-1425539072</span>, <span class="number">-1425014785</span>), <span class="comment">//171.8.0.0-171.15.255.255</span></span><br><span class="line">      (<span class="number">-1236271104</span>, <span class="number">-1235419137</span>), <span class="comment">//182.80.0.0-182.92.255.255</span></span><br><span class="line">      (<span class="number">-770113536</span>, <span class="number">-768606209</span>), <span class="comment">//210.25.0.0-210.47.255.255</span></span><br><span class="line">      (<span class="number">-569376768</span>, <span class="number">-564133889</span>) <span class="comment">//222.16.0.0-222.95.255.255</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 随机数:IP地址范围下标</span></span><br><span class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    <span class="keyword">val</span> index = random.nextInt(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">val</span> ipNumber: <span class="type">Int</span> = range(index)._1 + random.nextInt(range(index)._2 - range(index)._1)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换Int类型IP地址为IPv4格式</span></span><br><span class="line">    number2IpString(ipNumber)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** =================将Int类型IPv4地址转换为字符串类型================= */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">number2IpString</span></span>(ip: <span class="type">Int</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> buffer: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">4</span>)</span><br><span class="line">    buffer(<span class="number">0</span>) = (ip &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span></span><br><span class="line">    buffer(<span class="number">1</span>) = (ip &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span></span><br><span class="line">    buffer(<span class="number">2</span>) = (ip &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span></span><br><span class="line">    buffer(<span class="number">3</span>) = ip &amp; <span class="number">0xff</span></span><br><span class="line">    <span class="comment">// 返回IPv4地址</span></span><br><span class="line">    buffer.mkString(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行应用程序,模拟生成交易订单数据.</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;orderId&quot;</span>: <span class="string">&quot;20221017135128144000002&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;userId&quot;</span>: <span class="string">&quot;300000632&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;orderTime&quot;</span>: <span class="string">&quot;2022-10-17 13:51:28.144&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;ip&quot;</span>: <span class="string">&quot;36.58.6.24&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;orderMoney&quot;</span>: <span class="number">70.19</span>,</span><br><span class="line">    <span class="attr">&quot;orderStatus&quot;</span>: <span class="number">0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="流式程序开发"><a href="#流式程序开发" class="headerlink" title="流式程序开发"></a>流式程序开发</h2><p>实时从Kafka的order-topic消费JSON格式数据,经过ETL转换后,存储到Hudi表中.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.internal.<span class="type">Logging</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">OutputMode</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于StructuredStreaming结构化流实时从Kafka消费数据,经过ETL转换后,存储至Hudi表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HudiStructuredDemo</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// step1/构建SparkSession实例对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = createSparkSession(<span class="keyword">this</span>.getClass)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step2/从Kafka实时消费数据</span></span><br><span class="line">    <span class="keyword">val</span> kafkaStreamDF: <span class="type">DataFrame</span> = readFromKafka(spark, <span class="string">&quot;hudi-demo&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step3/提取数据,转换数据类型</span></span><br><span class="line">    <span class="keyword">val</span> streamDF: <span class="type">DataFrame</span> = process(kafkaStreamDF)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step4/保存数据至Hudi表中:COW(写入时拷贝)和MOR(读取时保存)</span></span><br><span class="line">    saveToHudi(streamDF)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step5/流式应用启动以后,等待终止</span></span><br><span class="line">    spark.streams.active.foreach(query =&gt; println(<span class="string">s&quot;Query: <span class="subst">$&#123;query.name&#125;</span> is Running .............&quot;</span>))</span><br><span class="line">    spark.streams.awaitAnyTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建SparkSession会话实例对象,基本属性设置</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createSparkSession</span></span>(clazz: <span class="type">Class</span>[_]): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">    <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .appName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">        .master(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="comment">// 设置序列化方式:Kryo</span></span><br><span class="line">        .config(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">        <span class="comment">// 设置属性:Shuffle时分区数和并行度</span></span><br><span class="line">        .config(<span class="string">&quot;spark.default.parallelism&quot;</span>, <span class="number">2</span>)</span><br><span class="line">        .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">2</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 指定Kafka Topic名称,实时消费数据</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readFromKafka</span></span>(spark: <span class="type">SparkSession</span>, topicName: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    spark</span><br><span class="line">        .readStream</span><br><span class="line">        .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;oceanbase004:9092&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;subscribe&quot;</span>, topicName)</span><br><span class="line">        .option(<span class="string">&quot;startingOffsets&quot;</span>, <span class="string">&quot;latest&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;maxOffsetsPerTrigger&quot;</span>, <span class="number">100</span>)</span><br><span class="line">        .option(<span class="string">&quot;failOnDataLoss&quot;</span>, <span class="string">&quot;false&quot;</span>)</span><br><span class="line">        .load()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 对Kafka获取数据,进行转换操作,获取所有字段的值,转换为String,以便保存Hudi表</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(streamDF: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    <span class="comment">/* 从Kafka消费数据后,字段信息如</span></span><br><span class="line"><span class="comment">       key -&gt; binary,value -&gt; binary</span></span><br><span class="line"><span class="comment">       topic -&gt; string, partition -&gt; int, offset -&gt; long</span></span><br><span class="line"><span class="comment">       timestamp -&gt; long, timestampType -&gt; int</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    streamDF</span><br><span class="line">        <span class="comment">// 选择字段,转换类型为String</span></span><br><span class="line">        .selectExpr(</span><br><span class="line">          <span class="string">&quot;CAST(key AS STRING) order_id&quot;</span>, <span class="comment">//</span></span><br><span class="line">          <span class="string">&quot;CAST(value AS STRING) message&quot;</span>, <span class="comment">//</span></span><br><span class="line">          <span class="string">&quot;topic&quot;</span>, <span class="string">&quot;partition&quot;</span>, <span class="string">&quot;offset&quot;</span>, <span class="string">&quot;timestamp&quot;</span> <span class="comment">//</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 解析Message,提取字段内置</span></span><br><span class="line">        .withColumn(<span class="string">&quot;user_id&quot;</span>, get_json_object(col(<span class="string">&quot;message&quot;</span>), <span class="string">&quot;$.userId&quot;</span>))</span><br><span class="line">        .withColumn(<span class="string">&quot;order_time&quot;</span>, get_json_object(col(<span class="string">&quot;message&quot;</span>), <span class="string">&quot;$.orderTime&quot;</span>))</span><br><span class="line">        .withColumn(<span class="string">&quot;ip&quot;</span>, get_json_object(col(<span class="string">&quot;message&quot;</span>), <span class="string">&quot;$.ip&quot;</span>))</span><br><span class="line">        .withColumn(<span class="string">&quot;order_money&quot;</span>, get_json_object(col(<span class="string">&quot;message&quot;</span>), <span class="string">&quot;$.orderMoney&quot;</span>))</span><br><span class="line">        .withColumn(<span class="string">&quot;order_status&quot;</span>, get_json_object(col(<span class="string">&quot;message&quot;</span>), <span class="string">&quot;$.orderStatus&quot;</span>))</span><br><span class="line">        <span class="comment">// 删除Message列</span></span><br><span class="line">        .drop(col(<span class="string">&quot;message&quot;</span>))</span><br><span class="line">        <span class="comment">// 转换订单日期时间格式为Long类型,作为Hudi表中合并数据字段</span></span><br><span class="line">        .withColumn(<span class="string">&quot;ts&quot;</span>, to_timestamp(col(<span class="string">&quot;order_time&quot;</span>), <span class="string">&quot;yyyy-MM-dd HH:mm:ss.SSSS&quot;</span>))</span><br><span class="line">        <span class="comment">// 订单日期时间提取分区日期:yyyyMMdd</span></span><br><span class="line">        .withColumn(<span class="string">&quot;day&quot;</span>, substring(col(<span class="string">&quot;order_time&quot;</span>), <span class="number">0</span>, <span class="number">10</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将流式数据集DataFrame保存至Hudi表,分别表类型:COW和MOR</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveToHudi</span></span>(streamDF: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    streamDF.writeStream</span><br><span class="line">        .outputMode(<span class="type">OutputMode</span>.<span class="type">Append</span>())</span><br><span class="line">        .queryName(<span class="string">&quot;query-hudi-streaming&quot;</span>)</span><br><span class="line">        <span class="comment">// 针对每微批次数据保存</span></span><br><span class="line">        .foreachBatch((batchDF: <span class="type">Dataset</span>[<span class="type">Row</span>], batchId: <span class="type">Long</span>) =&gt; &#123;</span><br><span class="line">          println(<span class="string">s&quot;============== BatchId: <span class="subst">$&#123;batchId&#125;</span> start ==============&quot;</span>)</span><br><span class="line">          writeHudiMor(batchDF) <span class="comment">// <span class="doctag">TODO:</span>表的类型MOR</span></span><br><span class="line">        &#125;)</span><br><span class="line">        .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;/datas/hudi-spark/struct-ckpt-100&quot;</span>)</span><br><span class="line">        .start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将数据集DataFrame保存到Hudi表中,表的类型:MOR(读取时合并)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">writeHudiMor</span></span>(dataframe: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceWriteOptions</span>._</span><br><span class="line">    <span class="keyword">import</span> org.apache.hudi.config.<span class="type">HoodieWriteConfig</span>._</span><br><span class="line">    <span class="keyword">import</span> org.apache.hudi.keygen.constant.<span class="type">KeyGeneratorOptions</span>._</span><br><span class="line"></span><br><span class="line">    dataframe.write</span><br><span class="line">        .format(<span class="string">&quot;hudi&quot;</span>)</span><br><span class="line">        .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">        <span class="comment">// 表的名称</span></span><br><span class="line">        .option(<span class="type">TBL_NAME</span>.key, <span class="string">&quot;tbl_kafka_mor&quot;</span>)</span><br><span class="line">        <span class="comment">// 设置表的类型</span></span><br><span class="line">        .option(<span class="type">TABLE_TYPE</span>.key(), <span class="string">&quot;MERGE_ON_READ&quot;</span>)</span><br><span class="line">        <span class="comment">// 每条数据主键字段名称</span></span><br><span class="line">        .option(<span class="type">RECORDKEY_FIELD_NAME</span>.key(), <span class="string">&quot;order_id&quot;</span>)</span><br><span class="line">        <span class="comment">// 数据合并时,依据时间字段</span></span><br><span class="line">        .option(<span class="type">PRECOMBINE_FIELD_NAME</span>.key(), <span class="string">&quot;ts&quot;</span>)</span><br><span class="line">        <span class="comment">// 分区字段名称</span></span><br><span class="line">        .option(<span class="type">PARTITIONPATH_FIELD_NAME</span>.key(), <span class="string">&quot;day&quot;</span>)</span><br><span class="line">        <span class="comment">// 分区值对应目录格式,是否与Hive分区策略一致</span></span><br><span class="line">        .option(<span class="type">HIVE_STYLE_PARTITIONING_ENABLE</span>.key(), <span class="string">&quot;true&quot;</span>)</span><br><span class="line">        <span class="comment">// 插入数据,产生shuffle时,分区数目</span></span><br><span class="line">        .option(<span class="string">&quot;hoodie.insert.shuffle.parallelism&quot;</span>, <span class="string">&quot;2&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;hoodie.upsert.shuffle.parallelism&quot;</span>, <span class="string">&quot;2&quot;</span>)</span><br><span class="line">        <span class="comment">// 表数据存储路径</span></span><br><span class="line">        .save(<span class="string">&quot;/hudi-warehouse/tbl_order_mor&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码中有两个细节,对于流式应用来说很关键:</p>
<ol>
<li>从Kafka消费数据时,通过属性maxOffsetsPerTrigger,设置每批次最大数据量,实际生产项目需要结合流式数据波峰及应用运行资源综合考虑设置.</li>
<li>将ETL后数据保存至Hudi中,设置检查点位置Checkpoint Location,便于流式应用运行失败后,可以从Checkpoint恢复,继续上次消费数据,进行实时处理.</li>
</ol>
<p>运行上述程序,查看HDFS上Hudi表存储交易订单数据存储目录结构:<br><img src="/images/fly1055.png" style="margin-left: 0px; padding-bottom: 10px;"></p>
<h2 id="Spark-查询分析"><a href="#Spark-查询分析" class="headerlink" title="Spark 查询分析"></a>Spark 查询分析</h2><p>启动spark-shell命令行,查询Hudi表存储交易订单数据.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--conf &quot;spark.serializer=org.apache.spark.serializer.KryoSerializer&quot;  </span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定Hudi表数据存储目录,加载数据:</span></span><br><span class="line"><span class="keyword">val</span> orderDF = spark.read.format(<span class="string">&quot;hudi&quot;</span>).load(<span class="string">&quot;/hudi-warehouse/tbl_order_mor&quot;</span>)</span><br><span class="line"><span class="comment">//查看Schema信息</span></span><br><span class="line">orderDF.printSchema()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- _hoodie_commit_time: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_commit_seqno: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_record_key: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_partition_path: string (nullable &#x3D; true)</span><br><span class="line"> |-- _hoodie_file_name: string (nullable &#x3D; true)</span><br><span class="line"> |-- order_id: string (nullable &#x3D; true)</span><br><span class="line"> |-- topic: string (nullable &#x3D; true)</span><br><span class="line"> |-- partition: integer (nullable &#x3D; true)</span><br><span class="line"> |-- offset: long (nullable &#x3D; true)</span><br><span class="line"> |-- timestamp: timestamp (nullable &#x3D; true)</span><br><span class="line"> |-- user_id: string (nullable &#x3D; true)</span><br><span class="line"> |-- order_time: string (nullable &#x3D; true)</span><br><span class="line"> |-- ip: string (nullable &#x3D; true)</span><br><span class="line"> |-- order_money: string (nullable &#x3D; true)</span><br><span class="line"> |-- order_status: string (nullable &#x3D; true)</span><br><span class="line"> |-- ts: timestamp (nullable &#x3D; true)</span><br><span class="line"> |-- day: string (nullable &#x3D; true)</span><br></pre></td></tr></table></figure>

<p>查看订单表前10条数据,选择订单相关字段:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">orderDF.select(<span class="string">&quot;order_id&quot;</span>, <span class="string">&quot;user_id&quot;</span>, <span class="string">&quot;order_time&quot;</span>, <span class="string">&quot;ip&quot;</span>, <span class="string">&quot;order_money&quot;</span>, <span class="string">&quot;order_status&quot;</span>, <span class="string">&quot;day&quot;</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<img src="/images/fly1056.png" style="margin-left: 0px; padding-bottom: 10px;">

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看数据总条目数:</span></span><br><span class="line">orderDF.count()</span><br><span class="line"></span><br><span class="line"><span class="comment">//交易订单数据基本聚合统计:最大金额max/最小金额min/平均金额avg</span></span><br><span class="line">spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  with tmp AS (</span></span><br><span class="line"><span class="string">    SELECT CAST(order_money AS DOUBLE) FROM view_tmp_order WHERE order_status = &#x27;0&#x27;</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  select </span></span><br><span class="line"><span class="string">    max(order_money) as max_money, </span></span><br><span class="line"><span class="string">    min(order_money) as min_money, </span></span><br><span class="line"><span class="string">    round(avg(order_money), 2) as avg_money </span></span><br><span class="line"><span class="string">  from tmp </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h2 id="DeltaStreamer-工具类"><a href="#DeltaStreamer-工具类" class="headerlink" title="DeltaStreamer 工具类"></a>DeltaStreamer 工具类</h2><p>工具类:HoodieDeltaStreamer,本质上运行Spark 流式程序,实时从获取数据,存储在Hudi表中,执行如下命令,查看帮助文档:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \</span><br><span class="line">/opt/spark/jars/hudi-utilities-bundle_2.11-0.12.0.jar \</span><br><span class="line">--help</span><br></pre></td></tr></table></figure>

<p>工具类所在jar包hudi-utilities-bundle_2.11-0.12.0.jar,将其添加CLASSPATH.<br>官方提供案例:实时消费Kafka中数据,数据格式为Avro,将其存储到Hudi表.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[hoodie]$</span><span class="bash"> spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \</span></span><br><span class="line"><span class="bash">  --props file://<span class="variable">$&#123;PWD&#125;</span>/hudi-utilities/src/<span class="built_in">test</span>/resources/delta-streamer-config/kafka-source.properties \</span></span><br><span class="line"><span class="bash">  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \</span></span><br><span class="line"><span class="bash">  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \</span></span><br><span class="line"><span class="bash">  --source-ordering-field impresssiontime \</span></span><br><span class="line"><span class="bash">  --target-base-path file:\/\/\/tmp/hudi-deltastreamer-op \ </span></span><br><span class="line">  --target-table uber.impressions \</span><br><span class="line">  --op BULK_INSERT</span><br></pre></td></tr></table></figure>





    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/hudi/" rel="tag"># hudi</a>
              <a href="/tags/hive/" rel="tag"># hive</a>
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/10/13/hudi%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" rel="prev" title="hudi常见问题">
                  <i class="fa fa-chevron-left"></i> hudi常见问题
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/10/17/redis%20try/" rel="next" title="redis try">
                  redis try <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">maoeryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  





</body>
</html>
