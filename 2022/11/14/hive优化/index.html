<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maoeryu.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","width":200,"display":"post","padding":7,"offset":5},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="hive优化.">
<meta property="og:type" content="article">
<meta property="og:title" content="hive优化">
<meta property="og:url" content="https://maoeryu.github.io/2022/11/14/hive%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="FlyingPig">
<meta property="og:description" content="hive优化.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1304.png">
<meta property="article:published_time" content="2022-11-13T16:00:00.000Z">
<meta property="article:modified_time" content="2022-11-15T09:54:26.407Z">
<meta property="article:author" content="maoeryu">
<meta property="article:tag" content="hive">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maoeryu.github.io/images/fly1304.png">


<link rel="canonical" href="https://maoeryu.github.io/2022/11/14/hive%E4%BC%98%E5%8C%96/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>hive优化 | FlyingPig</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlyingPig</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="nav-number">1.</span> <span class="nav-text">常用参数调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#limit%E9%99%90%E5%88%B6%E8%B0%83%E6%95%B4"><span class="nav-number">1.1.</span> <span class="nav-text">limit限制调整</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#join%E4%BC%98%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">join优化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%86%E5%A4%A7%E8%A1%A8%E6%94%BE%E5%90%8E%E5%A4%B4"><span class="nav-number">1.2.1.</span> <span class="nav-text">将大表放后头</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%90%8C%E7%9A%84%E8%BF%9E%E6%8E%A5%E9%94%AE"><span class="nav-number">1.2.2.</span> <span class="nav-text">使用相同的连接键</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%BD%E9%87%8F%E5%B0%BD%E6%97%A9%E5%9C%B0%E8%BF%87%E6%BB%A4%E6%95%B0%E6%8D%AE"><span class="nav-number">1.2.3.</span> <span class="nav-text">尽量尽早地过滤数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%BD%E9%87%8F%E5%8E%9F%E5%AD%90%E5%8C%96%E6%93%8D%E4%BD%9C"><span class="nav-number">1.2.4.</span> <span class="nav-text">尽量原子化操作</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.3.</span> <span class="nav-text">本地模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="nav-number">1.4.</span> <span class="nav-text">并行执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#strict%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.5.</span> <span class="nav-text">strict模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E6%95%B4mapper%E5%92%8Creducer%E4%B8%AA%E6%95%B0"><span class="nav-number">1.6.</span> <span class="nav-text">调整mapper和reducer个数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Map%E9%98%B6%E6%AE%B5%E4%BC%98%E5%8C%96"><span class="nav-number">1.6.1.</span> <span class="nav-text">Map阶段优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6hive%E4%BB%BB%E5%8A%A1%E7%9A%84reduce%E6%95%B0"><span class="nav-number">1.6.2.</span> <span class="nav-text">控制hive任务的reduce数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reduce%E9%98%B6%E6%AE%B5%E4%BC%98%E5%8C%96"><span class="nav-number">1.6.3.</span> <span class="nav-text">Reduce阶段优化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jvm%E9%87%8D%E7%94%A8"><span class="nav-number">1.7.</span> <span class="nav-text">jvm重用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%B0%83%E6%95%B4"><span class="nav-number">1.8.</span> <span class="nav-text">动态分区调整</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C"><span class="nav-number">1.9.</span> <span class="nav-text">推测执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C"><span class="nav-number">1.10.</span> <span class="nav-text">数据倾斜</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fetch%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.11.</span> <span class="nav-text">fetch模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="nav-number">1.12.</span> <span class="nav-text">其他参数调优</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98"><span class="nav-number">1.13.</span> <span class="nav-text">小文件问题</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E6%98%AF%E5%A6%82%E4%BD%95%E4%BA%A7%E7%94%9F%E7%9A%84"><span class="nav-number">1.13.1.</span> <span class="nav-text">小文件是如何产生的</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">1.13.2.</span> <span class="nav-text">小文件问题的影响</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">1.13.3.</span> <span class="nav-text">小文件问题的解决方案</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B"><span class="nav-number">2.</span> <span class="nav-text">案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#join"><span class="nav-number">2.1.</span> <span class="nav-text">join</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">2.1.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%98%E5%8C%96"><span class="nav-number">2.1.2.</span> <span class="nav-text">优化</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BC%98%E5%8C%96"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">第一次优化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BC%98%E5%8C%96"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">第二次优化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AC%A1%E4%BC%98%E5%8C%96"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">第三次优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C-1"><span class="nav-number">2.2.</span> <span class="nav-text">数据倾斜</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A9%BA%E5%80%BC"><span class="nav-number">2.2.1.</span> <span class="nav-text">空值</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%A7%8D"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">第一种</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%A7%8D"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">第二种</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.2.2.</span> <span class="nav-text">不同数据类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8D%E5%8F%AF%E6%8B%86%E5%88%86%E5%A4%A7%E6%96%87%E4%BB%B6"><span class="nav-number">2.2.3.</span> <span class="nav-text">不可拆分大文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%86%A8%E8%83%80"><span class="nav-number">2.2.4.</span> <span class="nav-text">数据膨胀</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A1%A8%E8%BF%9E%E6%8E%A5%E6%97%B6"><span class="nav-number">2.2.5.</span> <span class="nav-text">表连接时</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A1%AE%E5%AE%9E%E6%97%A0%E6%B3%95%E5%87%8F%E5%B0%91%E6%95%B0%E6%8D%AE%E9%87%8F"><span class="nav-number">2.2.6.</span> <span class="nav-text">确实无法减少数据量</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">maoeryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">219</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://maoeryu.github.io/2022/11/14/hive%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="maoeryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlyingPig">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          hive优化
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-11-14 00:00:00" itemprop="dateCreated datePublished" datetime="2022-11-14T00:00:00+08:00">2022-11-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-11-15 17:54:26" itemprop="dateModified" datetime="2022-11-15T17:54:26+08:00">2022-11-15</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>hive优化.</p>
<span id="more"></span>
<h3 id="常用参数调优"><a href="#常用参数调优" class="headerlink" title="常用参数调优"></a>常用参数调优</h3><h4 id="limit限制调整"><a href="#limit限制调整" class="headerlink" title="limit限制调整"></a>limit限制调整</h4><p>一般情况下,Limit语句还是需要执行整个查询语句,然后再返回部分结果.<br>有一个配置属性可以开启,避免这种情况 --- 对数据源进行抽样.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive.limit.optimize.enable<span class="operator">=</span><span class="literal">true</span> <span class="comment">--开启对数据源进行采样的功能</span></span><br><span class="line">hive.limit.row.max.size <span class="comment">--设置最小的采样容量</span></span><br><span class="line">hive.limit.optimize.limit.file <span class="comment">--设置最大的采样样本数</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>缺点:</p>
<blockquote>
<p>有可能部分数据永远不会被处理到.</p>
</blockquote>
</blockquote>
<h4 id="join优化"><a href="#join优化" class="headerlink" title="join优化"></a>join优化</h4><h5 id="将大表放后头"><a href="#将大表放后头" class="headerlink" title="将大表放后头"></a>将大表放后头</h5><p>Hive假定查询中最后的一个表是大表.它会将其它表缓存起来,然后扫描最后那个表.因此通常需要将小表放前面,或者标记哪张表是大表:<br><code>/*streamtable(table_name) */</code></p>
<h5 id="使用相同的连接键"><a href="#使用相同的连接键" class="headerlink" title="使用相同的连接键"></a>使用相同的连接键</h5><p>当对3个或者更多个表进行join连接时,如果每个on子句都使用相同的连接键的话,那么只会产生一个MapReduce job.</p>
<h5 id="尽量尽早地过滤数据"><a href="#尽量尽早地过滤数据" class="headerlink" title="尽量尽早地过滤数据"></a>尽量尽早地过滤数据</h5><p>减少每个阶段的数据量,对于分区表要加分区,同时只选择需要使用到的字段.</p>
<h5 id="尽量原子化操作"><a href="#尽量原子化操作" class="headerlink" title="尽量原子化操作"></a>尽量原子化操作</h5><p>尽量避免一个SQL包含复杂逻辑,可以使用中间表来完成复杂的逻辑.</p>
<h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>有时hive的输入数据量是非常小的.在这种情况下,为查询出发执行任务的时间消耗可能会比实际job的执行时间要多的多.<br>对于大多数这种情况,hive可以通过本地模式在单台机器上处理所有的任务.对于小数据集,执行时间会明显被缩短.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>; <span class="comment">--开启本地mr</span></span><br></pre></td></tr></table></figure>

<p>当一个job满足如下条件才能真正使用本地模式:</p>
<ul>
<li>job的输入数据大小必须小于参数:<code>set hive.exec.mode.local.auto.inputbytes.max=51234560;</code>(默认128MB)</li>
<li>job的map数必须小于参数:<code>set hive.exec.mode.local.auto.input.files.max=10;</code>(默认4)</li>
<li>job的reduce数必须为0或者1</li>
</ul>
<p>可用参数<code>hive.mapred.local.mem</code>(默认0)控制child jvm使用的最大内存数.</p>
<h4 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h4><p>hive会将一个查询转化为一个或多个阶段,包括:MapReduce阶段/抽样阶段/合并阶段/limit阶段等.默认情况下,一次只执行一个阶段.不过,如果某些阶段不是互相依赖,是可以并行执行的.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>; <span class="comment">--可以开启并发执行.</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">16</span>; <span class="comment">--同一个sql允许最大并行度,默认为8.</span></span><br></pre></td></tr></table></figure>

<p>会比较耗系统资源.</p>
<h4 id="strict模式"><a href="#strict模式" class="headerlink" title="strict模式"></a>strict模式</h4><p>对分区表进行查询,在where子句中没有加分区过滤的话,将禁止提交任务(默认:nonstrict)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapred.mode<span class="operator">=</span>strict;</span><br></pre></td></tr></table></figure>

<p>注:使用严格模式可以禁止3种类型的查询:</p>
<ol>
<li>对于分区表,不加分区字段过滤条件,不能执行</li>
<li>对于order by语句,必须使用limit语句</li>
<li>限制笛卡尔积的查询(join的时候不使用on,而使用where的)</li>
</ol>
<h4 id="调整mapper和reducer个数"><a href="#调整mapper和reducer个数" class="headerlink" title="调整mapper和reducer个数"></a>调整mapper和reducer个数</h4><h5 id="Map阶段优化"><a href="#Map阶段优化" class="headerlink" title="Map阶段优化"></a>Map阶段优化</h5><p>map执行时间:map任务启动和初始化的时间+逻辑处理的时间.</p>
<p>通常情况下,作业会通过input的目录产生一个或者多个map任务.主要的决定因素有:</p>
<ol>
<li>input的文件总个数</li>
<li>input的文件大小</li>
<li>集群设置的文件块大小(目前为128M, 可在hive中通过s<code>et dfs.block.size;</code>命令查看到,该参数不能自定义修改);</li>
</ol>
<blockquote>
<p>举例<br>a)假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块(6个128m的块和1个12m的块),从而产生7个map数<br>b)假设input目录下有3个文件a,b,c,大小分别为10m,20m,130m,那么hadoop会分隔成4个块(10m,20m,128m,2m),从而产生4个map数<br>即,如果文件大于块大小(128m),那么会拆分,如果小于块大小,则把该文件当成一个块.</p>
</blockquote>
<blockquote>
<p>是不是map数越多越好?</p>
</blockquote>
<p>答案是否定的.<br>如果一个任务有很多小文件(远远小于块大小128m),则每个小文件也会被当做一个块,用一个map任务来完成,而一个map任务启动和初始化的时间远远大于逻辑处理的时间,就会造成很大的资源浪费.而且,同时可执行的map数是受限的.</p>
<blockquote>
<p>是不是保证每个map处理接近128m的文件块,就高枕无忧了?</p>
</blockquote>
<p>答案也是不一定.<br>比如有一个127m的文件,正常会用一个map去完成,但这个文件只有一个或者两个小字段,却有几千万的记录,如果map处理的逻辑比较复杂,用一个map任务去做,肯定也比较耗时.</p>
<p>针对上面的问题,我们需要采取两种方式来解决:即减少map数和增加map数;如何合并小文件,减少map数?</p>
<p>假设一个SQL任务:<br>Select count(1) from popt_tbaccountcopy_mes where pt = &#39;2012-07-04&#39;<br>该任务的inputdir <code>/group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04</code> 共有194个文件,其中很多是远远小于128m的小文件,总大小9G,正常执行会用194个map任务.<br>Map总共消耗的计算资源:SLOTS_MILLIS_MAPS= 623,020.<br>通过以下方法来在map执行前合并小文件,减少map数:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>再执行上面的语句,用了74个map任务,map消耗的计算资源:SLOTS_MILLIS_MAPS=333,500 对于这个简单SQL任务,执行时间上可能差不多,但节省了一半的计算资源.<br>大概解释一下,100000000表示100M</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>
<p>这个参数表示执行前进行小文件合并,前面三个参数确定合并文件块的大小,大于文件块大小128m的,按照128m来分隔,小于128m,大于100m的,按照100m来分隔,把那些小于100m的(包括小文件和分隔大文件剩下的),进行合并,最终生成了74个块.</p>
<p>如何适当的增加map数?<br>当input的文件都很大,任务逻辑复杂,map执行非常慢的时候,可以考虑增加Map数, 来使得每个map处理的数据量减少,从而提高任务的执行效率.假设有这样一个任务:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Select</span> data_desc,</span><br><span class="line"><span class="built_in">count</span>(<span class="number">1</span>),</span><br><span class="line"><span class="built_in">count</span>(<span class="keyword">distinct</span> id),</span><br><span class="line"><span class="built_in">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> …),</span><br><span class="line"><span class="built_in">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> ...),</span><br><span class="line"><span class="built_in">sum</span>(…)</span><br><span class="line"><span class="keyword">from</span> a <span class="keyword">group</span> <span class="keyword">by</span> data_desc</span><br></pre></td></tr></table></figure>

<p>如果表a只有一个文件,大小为120M,但包含几千万的记录,如果用1个map去完成这个任务,肯定是比较耗时的,这种情况下,我们要考虑将这一个文件合理的拆分成多个,这样就可以用多个map任务去完成.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">10</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> a_1 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a </span><br><span class="line">distribute <span class="keyword">by</span> rand(<span class="number">123</span>);</span><br></pre></td></tr></table></figure>

<p>这样会将a表的记录,随机的分散到包含10个文件的a_1表中,再用a_1代替上面sql中的a表,则会用10个map任务去完成.每个map任务处理大于12M(几百万记录)的数据,效率肯定会好很多.</p>
<p>看上去,貌似这两种有些矛盾,一个是要合并小文件,一个是要把大文件拆成小文件,这点正是重点需要关注的地方,根据实际情况,控制map数量需要遵循两个原则:</p>
<ol>
<li>使大数据量利用合适的map数.</li>
<li>使单个map任务处理合适的数据量.</li>
</ol>
<h5 id="控制hive任务的reduce数"><a href="#控制hive任务的reduce数" class="headerlink" title="控制hive任务的reduce数"></a>控制hive任务的reduce数</h5><blockquote>
<p>Hive自己如何确定reduce数</p>
</blockquote>
<p>reduce个数的设定极大影响任务执行效率,不指定reduce个数的情况下,Hive会猜测确定一个reduce个数,基于以下两个设定:<br>hive.exec.reducers.bytes.per.reducer(每个reduce任务处理的数据量,默认为1000^3=1G)<br>hive.exec.reducers.max(每个任务最大的reduce数,默认为999)</p>
<p>计算reducer数的公式很简单N=min(参数2,总输入数据量/参数1)</p>
<p>即,如果reduce的输入(map的输出)总大小不超过1G,那么只会有一个reduce任务,如:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> popt_tbaccountcopy_mes <span class="keyword">where</span> pt <span class="operator">=</span> <span class="string">&#x27;2012-07-04&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> pt;</span><br></pre></td></tr></table></figure>
<p>/group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04 总大小为9G多,</p>
<p>因此这句有10个reduce</p>
<blockquote>
<p>调整reduce个数方法一</p>
</blockquote>
<p>调整hive.exec.reducers.bytes.per.reducer参数的值.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="number">500000000</span>; <span class="comment">--(500M) </span></span><br><span class="line"><span class="keyword">select</span> pt,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> popt_tbaccountcopy_mes <span class="keyword">where</span> pt <span class="operator">=</span> <span class="string">&#x27;2012-07-04&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> pt; <span class="comment">--这次有20个reduce</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>调整reduce个数方法二</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks <span class="operator">=</span> <span class="number">15</span>;</span><br><span class="line"><span class="keyword">select</span> pt,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> popt_tbaccountcopy_mes <span class="keyword">where</span> pt <span class="operator">=</span> <span class="string">&#x27;2012-07-04&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> pt; <span class="comment">--这次有15个reduce</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>reduce个数并不是越多越好.</p>
</blockquote>
<p>同map一样,启动和初始化reduce也会消耗时间和资源.<br>另外,有多少个reduce,就会有多少个输出文件,如果生成了很多个小文件, 那么如果这些小文件作为下一个任务的输入,则也会出现小文件过多的问题.</p>
<blockquote>
<p>什么情况下只有一个reduce</p>
</blockquote>
<p>很多时候你会发现任务中不管数据量多大,不管你有没有设置调整reduce个数的参数,任务中一直都只有一个reduce任务.<br>其实只有一个reduce任务的情况,除了数据量小于hive.exec.reducers.bytes.per.reducer参数值的情况外,还有以下原因:</p>
<ol>
<li>没有group by的汇总,比如把<br>select pt,count(1) from popt_tbaccountcopy_mes where pt = &#39;2012-07-04&#39; group by pt;<br>写成<br>select count(1) from popt_tbaccountcopy_mes where pt = &#39;2012-07-04&#39;;<br>这点非常常见,希望大家尽量改写.</li>
<li>用了Order by</li>
<li>有笛卡尔积</li>
</ol>
<p>通常这些情况下,除了找办法来变通和避免,我们暂时没有什么好的办法,因为这些操作都是全局的,所以hadoop不得不用一个reduce去完成.<br>同样的,在设置reduce个数的时候也需要考虑这两个原则:</p>
<ol>
<li>使大数据量利用合适的reduce数</li>
<li>使单个reduce任务处理合适的数据量</li>
</ol>
<h5 id="Reduce阶段优化"><a href="#Reduce阶段优化" class="headerlink" title="Reduce阶段优化"></a>Reduce阶段优化</h5><p>调整方式:<br>set mapred.reduce.tasks = ?<br>set hive.exec.reducers.bytes.per.reducer = ?</p>
<p>一般根据输入文件的总大小,用它的estimation函数来自动计算reduce的个数:<br>reduce个数 = InputFileSize / bytes per reducer</p>
<h4 id="jvm重用"><a href="#jvm重用" class="headerlink" title="jvm重用"></a>jvm重用</h4><p>用于避免小文件的场景或者task特别多的场景,这类场景大多数执行时间都很短,因为hive调起mapreduce任务,jvm的启动过程会造成很大的开销,尤其是job有成千上万个task任务时,jvm重用可以使得jvm实例在同一个job中重新使用N次.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.reuse.jvm.num.tasks<span class="operator">=</span><span class="number">10</span>; <span class="comment">--10为重用个数,hive中设置</span></span><br></pre></td></tr></table></figure>

<p>或者在mapred-site.xml中设置.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>缺点<br>开启jvm重用将一直占用使用到的task插槽,以便进行重用,直到任务完成后才能释放.<br>如果某个&quot;不平衡的&quot;job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话,那么保留的插槽就会一直空闲着却无法被其他的job使用,直到所有的task都结束了才会释放.</p>
</blockquote>
<h4 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--true表示开启动态分区功能,默认为false</span></span><br><span class="line">hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--nonstrict,表示允许所有分区都是动态的,默认为strict</span></span><br><span class="line"><span class="comment">--strict,表示必须保证至少有一个分区是静态的</span></span><br><span class="line">hive.exec.dynamic.partition.mode<span class="operator">=</span>strict;</span><br><span class="line"></span><br><span class="line"><span class="comment">--每个mapper或reducer可以创建的最大动态分区个数</span></span><br><span class="line">hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line"><span class="comment">--一个动态分区创建语句可以创建的最大动态分区个数</span></span><br><span class="line">hive.exec.max.dynamic.partitions<span class="operator">=</span><span class="number">1000</span>;</span><br><span class="line"><span class="comment">--全局可以创建的最大文件个数</span></span><br><span class="line">hive.exec.max.created.files<span class="operator">=</span><span class="number">100000</span>;</span><br></pre></td></tr></table></figure>

<p>控制DataNode一次可以打开的文件个数 这个参数必须设置在DataNode的hdfs-site.xml文件中</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.max.xcievers<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h4><p>通过加快获取单个task的结果以及进行侦测将执行慢的TaskTracker加入到黑名单的方式来提高整体的任务执行效率.</p>
<p>修改mapred-site.xml文件.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>或者修改hive配置.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapred.reduce.tasks.speculative.execution<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<h4 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h4><p>任务进度长时间维持在99%(或100%),查看任务监控页面,发现只有少量(1个或几个)reduce子任务未完成.<br>因为其处理的数据量和其他reduce差异过大.<br>单一reduce的记录数与平均记录数差异过大,通常可能达到3倍甚至更多.<br>最长时长远大于平均时长.</p>
<blockquote>
<p>原因<br>1)key分布不均匀<br>2)业务数据本身的特性<br>3)建表时考虑不周<br>4)某些SQL语句本身就有数据倾斜</p>
</blockquote>
<img src="/images/fly1304.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>解决方案:参数调节</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启Map端聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--在Map端进行聚合操作的条目数目</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>;</span><br></pre></td></tr></table></figure>

<p>有数据倾斜的时候进行负载均衡(默认是false)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>当选项设定为 true,生成的查询计划会有两个MR Job.<br>第一个MR Job中,Map的输出结果会随机分布到Reduce中,每个Reduce做部分聚合操作,并输出结果,这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中,从而达到负载均衡的目的.<br>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中(这个过程可以保证相同的Group By Key被分布到同一个Reduce中),最后完成最终的聚合操作.</p>
<h4 id="fetch模式"><a href="#fetch模式" class="headerlink" title="fetch模式"></a>fetch模式</h4><p>默认是more,在全局查找/字段查找/limit查找等都不走mapreduce.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--对于简单的不需要聚合的类似SELECT col from table LIMIT n语句.</span></span><br><span class="line"><span class="comment">--不需要起MapReduce job,直接通过Fetch task获取数据</span></span><br><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion<span class="operator">=</span>more;</span><br></pre></td></tr></table></figure>

<h4 id="其他参数调优"><a href="#其他参数调优" class="headerlink" title="其他参数调优"></a>其他参数调优</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启CLI提示符前打印出当前所在的数据库名</span></span><br><span class="line"><span class="keyword">set</span> hive.cli.print.current.db<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--让CLI打印出字段名称</span></span><br><span class="line">hive.cli.print.header<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--设置任务名称,方便查找监控</span></span><br><span class="line"><span class="keyword">SET</span> mapred.job.name<span class="operator">=</span>P_DWA_D_IA_S_USER_PROD;</span><br></pre></td></tr></table></figure>

<h4 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h4><h5 id="小文件是如何产生的"><a href="#小文件是如何产生的" class="headerlink" title="小文件是如何产生的"></a>小文件是如何产生的</h5><ol>
<li>动态分区插入数据,产生大量的小文件,从而导致map数量剧增.</li>
<li>reduce数量越多,小文件也越多(reduce的个数和输出文件是对应的).</li>
<li>数据源本身就包含大量的小文件.</li>
</ol>
<h5 id="小文件问题的影响"><a href="#小文件问题的影响" class="headerlink" title="小文件问题的影响"></a>小文件问题的影响</h5><ol>
<li>从Hive的角度看,小文件会开很多map,一个map开一个jvm去执行,所以这些任务的初始化,启动,执行会浪费大量的资源,严重影响性能.</li>
<li>在HDFS中,每个小文件对象约占150byte,如果小文件过多会占用大量内存.这样NameNode内存容量严重制约了集群的扩展.</li>
</ol>
<h5 id="小文件问题的解决方案"><a href="#小文件问题的解决方案" class="headerlink" title="小文件问题的解决方案"></a>小文件问题的解决方案</h5><p>从小文件产生的途经就可以从源头上控制小文件数量,方法如下:</p>
<ol>
<li>使用Sequencefile作为表存储格式,不要用textfile,在一定程度上可以减少小文件</li>
<li>减少reduce的数量(可以使用参数进行控制)</li>
<li>少用动态分区,用时记得按distribute by分区</li>
</ol>
<p>对于已有的小文件,我们可以通过以下几种方案解决:</p>
<ol>
<li>使用hadoop archive命令把小文件进行归档</li>
<li>重建表,建表时减少reduce数量</li>
<li>通过参数进行调节,设置map/reduce端的相关参数.</li>
</ol>
<p>设置map输入合并小文件的相关参数:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--每个Map最大输入大小(这个值决定了合并后文件的数量)  </span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">256000000</span>;    </span><br><span class="line"><span class="comment">--一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)  </span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">100000000</span>;  </span><br><span class="line"><span class="comment">--一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)    </span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">100000000</span>;  </span><br><span class="line"><span class="comment">--执行Map前进行小文件合并  </span></span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>设置map输出和reduce输出进行合并的相关参数:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置map端输出进行合并,默认为true  </span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>  </span><br><span class="line"><span class="comment">--设置reduce端输出进行合并,默认为false  </span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>  </span><br><span class="line"><span class="comment">--设置合并文件的大小  </span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">256</span><span class="operator">*</span><span class="number">1000</span><span class="operator">*</span><span class="number">1000</span>  </span><br><span class="line"><span class="comment">--当输出文件的平均大小小于该值时,启动一个独立的MapReduce任务进行文件merge.</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize<span class="operator">=</span><span class="number">16000000</span></span><br></pre></td></tr></table></figure>

<p>设置如下参数取消一些限制(HIVE 0.7后没有此限制):</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--默认值:true</span></span><br><span class="line"><span class="comment">--描述:是否合并Map的输出文件,也就是把小文件合并成一个map</span></span><br><span class="line">hive.merge.mapfiles<span class="operator">=</span><span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--默认值:false</span></span><br><span class="line"><span class="comment">--描述:是否合并Reduce的输出文件,也就是在Map输出阶段做一次reduce操作,再输出.</span></span><br><span class="line">hive.merge.mapredfiles<span class="operator">=</span><span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--这个参数表示执行前进行小文件合并.</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><h5 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h5><p>用户轨迹工程的性能瓶颈一直是etract_track_info,其中耗时大户主要在于trackinfo与pm_info进行左关联的环节,trackinfo与pm_info两张表均为GB级别,左关联代码块如下:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trackinfo a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> pm_info b</span><br><span class="line"><span class="keyword">on</span> (a.ext_field7 <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure>

<h5 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h5><h6 id="第一次优化"><a href="#第一次优化" class="headerlink" title="第一次优化"></a>第一次优化</h6><p>考虑到pm_info表的id是bigint类型,trackinfo表的ext_field7是string类型,其关联时数据类型不一致.<br>默认的hash操作会按bigint型的id进行分配,这样会导致所有string类型的ext_field7集中到一个reduce里面,因此改为如下:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trackinfo a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> pm_info b</span><br><span class="line"><span class="keyword">on</span> (<span class="built_in">cast</span>(a.ext_field7 <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure>

<h6 id="第二次优化"><a href="#第二次优化" class="headerlink" title="第二次优化"></a>第二次优化</h6><p>考虑到trackinfo表的ext_field7字段缺失率很高(为空/字段长度为零/字段填充了非整数)情况,<br>做进行左关联时空字段的关联操作实际上没有意义,如果左表关联字段ext_field7为无效字段,则不需要关联,因此改为如下:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trackinfo a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> pm_info b</span><br><span class="line"><span class="keyword">on</span> (a.ext_field7 <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line"><span class="keyword">and</span> length(a.ext_field7) <span class="operator">&gt;</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">and</span> a.ext_field7 rlike <span class="string">&#x27;^[0-9]+$&#x27;</span></span><br><span class="line"><span class="keyword">and</span> a.ext_field7 <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure>

<p>如果左表关联字段ext_field7为无效字段时(为空/字段长度为零/字段填充了非整数),<br>不去关联右表,由于空字段左关联以后取到的右表字段仍然为null,所以不会影响结果.</p>
<h6 id="第三次优化"><a href="#第三次优化" class="headerlink" title="第三次优化"></a>第三次优化</h6><p>第二次优化效果效果不理想的原因,其实是在左关联中,虽然设置了左表关联字段为空不去关联右表,但是这样做,左表中未关联的记录(ext_field7为空)将会全部聚集在一个reduce中进行处理,体现为reduce进度长时间处在99%.</p>
<p>解决办法的突破点就在于如何把左表的未关联记录的key尽可能打散,因此可以这么做:<br>若左表关联字段无效(为空/字段长度为零/字段填充了非整数),则在关联前将左表关联字段设置为一个随机数,再去关联右表.<br>这么做的目的是即使是左表的未关联记录,它的key也分布得十分均匀.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trackinfo a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> pm_info b</span><br><span class="line"><span class="keyword">on</span> (</span><br><span class="line">  <span class="keyword">case</span> <span class="keyword">when</span> (a.ext_field7 <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line">    <span class="keyword">and</span> length(a.ext_field7) <span class="operator">&gt;</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">and</span> a.ext_field7 rlike <span class="string">&#x27;^[0-9]+$&#x27;</span>)</span><br><span class="line">  <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">cast</span>(a.ext_field7 <span class="keyword">as</span> <span class="type">bigint</span>)</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">cast</span>(<span class="built_in">ceiling</span>(rand() <span class="operator">*</span> <span class="number">-65535</span>) <span class="keyword">as</span> <span class="type">bigint</span>)</span><br><span class="line">  <span class="keyword">end</span> <span class="operator">=</span> b.id</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="数据倾斜-1"><a href="#数据倾斜-1" class="headerlink" title="数据倾斜"></a>数据倾斜</h4><h5 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h5><h6 id="第一种"><a href="#第一种" class="headerlink" title="第一种"></a>第一种</h6><p>可以直接不让null值参与join操作,即不让null值有shuffle阶段.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log a <span class="keyword">join</span> users b <span class="keyword">on</span> </span><br><span class="line">a.user_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">NULL</span> <span class="keyword">and</span> </span><br><span class="line">a.user_id <span class="operator">=</span> b.user_id</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log a <span class="keyword">where</span> a.user_id <span class="keyword">is</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>

<h6 id="第二种"><a href="#第二种" class="headerlink" title="第二种"></a>第二种</h6><p>因为null值参与shuffle时的hash结果是一样的,那么我们可以给null值随机赋值,这样它们的hash结果就不一样,就会进到不同的reduce中.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log a <span class="keyword">left</span> <span class="keyword">join</span> users b <span class="keyword">on</span></span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> a.user_id <span class="keyword">is</span> <span class="keyword">NULL</span> <span class="keyword">then</span> concat(<span class="string">&#x27;hive_&#x27;</span>, rand())</span><br><span class="line"><span class="keyword">else</span> a.user_id</span><br><span class="line"><span class="keyword">end</span> <span class="operator">=</span> b.user_id;</span><br></pre></td></tr></table></figure>

<h5 id="不同数据类型"><a href="#不同数据类型" class="headerlink" title="不同数据类型"></a>不同数据类型</h5><p>如果key字段既有string类型也有int类型,默认的hash就都会按int类型来分配,那我们直接把int类型都转为string就好了,这样key字段都为string,hash时就按照string类型分配了.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> users a <span class="keyword">left</span> <span class="keyword">join</span> logs b <span class="keyword">on</span></span><br><span class="line">a.user_id <span class="operator">=</span> <span class="built_in">cast</span>(b.user_id <span class="keyword">as</span> string);</span><br></pre></td></tr></table></figure>

<h5 id="不可拆分大文件"><a href="#不可拆分大文件" class="headerlink" title="不可拆分大文件"></a>不可拆分大文件</h5><p>将使用GZIP压缩等不支持文件分割的文件转为bzip和zip等支持文件分割的压缩方式.<br>所以,我们在对文件进行压缩时,为避免因不可拆分大文件而引发数据读取的倾斜,在数据压缩的时候可以采用bzip2和Zip等支持文件分割的压缩算法.</p>
<h5 id="数据膨胀"><a href="#数据膨胀" class="headerlink" title="数据膨胀"></a>数据膨胀</h5><p>在Hive中可以通过参数 <code>hive.new.job.grouping.set.cardinality</code> 配置的方式自动控制作业的拆解,该参数默认值是30.<br>表示针对grouping sets/rollups/cubes这类多维聚合的操作,如果最后拆解的键组合大于该值,会启用新的任务去处理大于该值之外的组合.<br>如果在处理数据时,某个分组聚合的列有较大的倾斜,可以适当调小该值.</p>
<h5 id="表连接时"><a href="#表连接时" class="headerlink" title="表连接时"></a>表连接时</h5><p>通常做法是将倾斜的数据存到分布式缓存中,分发到各个Map任务所在节点.<br>在Map阶段完成join操作,即MapJoin,这避免了Shuffle,从而避免了数据倾斜.</p>
<h5 id="确实无法减少数据量"><a href="#确实无法减少数据量" class="headerlink" title="确实无法减少数据量"></a>确实无法减少数据量</h5><p>这类问题最直接的方式就是调整reduce所执行的内存大小.<br>调整reduce的内存大小使用<code>mapreduce.reduce.memory.mb</code>这个配置.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/hive/" rel="tag"># hive</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/10/elasticsearch%20question/" rel="prev" title="elasticsearch question">
                  <i class="fa fa-chevron-left"></i> elasticsearch question
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/11/15/interview/" rel="next" title="interview">
                  interview <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">maoeryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  





</body>
</html>
