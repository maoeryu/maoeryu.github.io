<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maoeryu.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","width":200,"display":"post","padding":7,"offset":5},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="table th:first-of-type {   width: 40%; } table th:nth-of-type(2) {   width: 60%; }">
<meta property="og:type" content="article">
<meta property="og:title" content="spark结构化流编程指南">
<meta property="og:url" content="https://maoeryu.github.io/2022/11/23/spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/index.html">
<meta property="og:site_name" content="FlyingPig">
<meta property="og:description" content="table th:first-of-type {   width: 40%; } table th:nth-of-type(2) {   width: 60%; }">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1354.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1355.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1356.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1357.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1358.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1359.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1360.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1361.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1362.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1363.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1364.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1365.png">
<meta property="article:published_time" content="2022-11-22T16:00:00.000Z">
<meta property="article:modified_time" content="2022-11-24T02:03:26.078Z">
<meta property="article:author" content="maoeryu">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maoeryu.github.io/images/fly1354.png">


<link rel="canonical" href="https://maoeryu.github.io/2022/11/23/spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>spark结构化流编程指南 | FlyingPig</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlyingPig</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.</span> <span class="nav-text">快速示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">编程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">3.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E5%92%8C%E5%BB%B6%E8%BF%9F%E6%95%B0%E6%8D%AE"><span class="nav-number">3.2.</span> <span class="nav-text">处理事件时间和延迟数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E9%94%99%E8%AF%AD%E4%B9%89"><span class="nav-number">3.3.</span> <span class="nav-text">容错语义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Datasets%E5%92%8CDataFrames%E7%9A%84-API"><span class="nav-number">4.</span> <span class="nav-text">使用Datasets和DataFrames的 API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%B5%81DataFrames%E5%92%8C%E6%B5%81Datasets"><span class="nav-number">4.1.</span> <span class="nav-text">创建流DataFrames和流Datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E6%BA%90"><span class="nav-number">4.1.1.</span> <span class="nav-text">输入源</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81DataFrames-Datasets%E7%9A%84%E6%A8%A1%E5%BC%8F%E6%8E%A8%E6%96%AD%E5%92%8C%E5%88%86%E5%8C%BA"><span class="nav-number">4.1.2.</span> <span class="nav-text">流DataFrames&#x2F;Datasets的模式推断和分区</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E5%BC%8FDataFrames-Datasets%E4%B8%8A%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">4.2.</span> <span class="nav-text">流式DataFrames&#x2F;Datasets上的操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-%E9%80%89%E6%8B%A9-%E6%8A%95%E5%BD%B1-%E8%81%9A%E5%90%88"><span class="nav-number">4.2.1.</span> <span class="nav-text">基本操作-选择&#x2F;投影&#x2F;聚合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C"><span class="nav-number">4.2.2.</span> <span class="nav-text">事件时间窗口操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F%E6%95%B0%E6%8D%AE%E5%92%8C%E6%B0%B4%E5%8D%B0"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">处理延迟数据和水印</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A0%E6%B0%B4%E5%8D%B0%E6%B8%85%E9%99%A4%E8%81%9A%E5%90%88%E7%8A%B6%E6%80%81%E7%9A%84%E6%9D%A1%E4%BB%B6"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">加水印清除聚合状态的条件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B8%A6%E6%B0%B4%E5%8D%B0%E8%81%9A%E5%90%88%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%9D%E8%AF%81"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">带水印聚合的语义保证</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Join%E6%93%8D%E4%BD%9C"><span class="nav-number">4.2.3.</span> <span class="nav-text">Join操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B5%81%E9%9D%99%E6%80%81%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.2.3.1.</span> <span class="nav-text">流静态连接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B5%81-%E6%B5%81%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.2.3.2.</span> <span class="nav-text">流-流连接</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%B8%A6%E6%9C%89%E5%8F%AF%E9%80%89%E6%B0%B4%E5%8D%B0%E7%9A%84%E5%86%85%E9%83%A8%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.2.3.2.1.</span> <span class="nav-text">带有可选水印的内部连接</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%B8%A6%E6%B0%B4%E5%8D%B0%E7%9A%84%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.2.3.2.2.</span> <span class="nav-text">带水印的外连接</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%B5%81%E5%BC%8F%E6%9F%A5%E8%AF%A2%E4%B8%AD%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%94%AF%E6%8C%81%E7%9F%A9%E9%98%B5"><span class="nav-number">4.2.3.2.3.</span> <span class="nav-text">流式查询中连接的支持矩阵</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81%E5%BC%8F%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4"><span class="nav-number">4.2.4.</span> <span class="nav-text">流式重复数据删除</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E6%B0%B4%E5%8D%B0%E7%9A%84%E7%AD%96%E7%95%A5"><span class="nav-number">4.2.5.</span> <span class="nav-text">处理多个水印的策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%BB%E6%84%8F%E7%8A%B6%E6%80%81%E6%93%8D%E4%BD%9C"><span class="nav-number">4.2.6.</span> <span class="nav-text">任意状态操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E5%8F%97%E6%94%AF%E6%8C%81%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">4.2.7.</span> <span class="nav-text">不受支持的操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E5%A7%8B%E6%B5%81%E5%BC%8F%E6%9F%A5%E8%AF%A2"><span class="nav-number">4.3.</span> <span class="nav-text">开始流式查询</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Output-Modes"><span class="nav-number">4.3.1.</span> <span class="nav-text">Output Modes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Output-Sinks"><span class="nav-number">4.3.2.</span> <span class="nav-text">Output Sinks</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Foreach-ForeachBatch"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">使用 Foreach&#x2F;ForeachBatch</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#ForeachBatch"><span class="nav-number">4.3.2.1.1.</span> <span class="nav-text">ForeachBatch</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Foreach"><span class="nav-number">4.3.2.1.2.</span> <span class="nav-text">Foreach</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A6%E5%8F%91%E5%99%A8"><span class="nav-number">4.3.3.</span> <span class="nav-text">触发器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%A1%E7%90%86%E6%B5%81%E6%9F%A5%E8%AF%A2"><span class="nav-number">4.4.</span> <span class="nav-text">管理流查询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7%E6%B5%81%E6%9F%A5%E8%AF%A2"><span class="nav-number">4.5.</span> <span class="nav-text">监控流查询</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%AF%BB%E5%8F%96%E6%8C%87%E6%A0%87"><span class="nav-number">4.5.1.</span> <span class="nav-text">交互式读取指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%BC%82%E6%AD%A5-API-%E4%BB%A5%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E6%8A%A5%E5%91%8A%E6%8C%87%E6%A0%87"><span class="nav-number">4.5.2.</span> <span class="nav-text">使用异步 API 以编程方式报告指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Dropwizard-%E6%8A%A5%E5%91%8A%E6%8C%87%E6%A0%87"><span class="nav-number">4.5.3.</span> <span class="nav-text">使用 Dropwizard 报告指标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%A3%80%E6%9F%A5%E7%82%B9%E4%BB%8E%E6%95%85%E9%9A%9C%E4%B8%AD%E6%81%A2%E5%A4%8D"><span class="nav-number">4.6.</span> <span class="nav-text">使用检查点从故障中恢复</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E5%BC%8F%E6%9F%A5%E8%AF%A2%E6%9B%B4%E6%94%B9%E5%90%8E%E7%9A%84%E6%81%A2%E5%A4%8D%E8%AF%AD%E4%B9%89"><span class="nav-number">4.7.</span> <span class="nav-text">流式查询更改后的恢复语义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E5%8C%96%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.7.1.</span> <span class="nav-text">变化的类型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%8A%A0%E5%B7%A5"><span class="nav-number">5.</span> <span class="nav-text">连续加工</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84%E6%9F%A5%E8%AF%A2"><span class="nav-number">5.1.</span> <span class="nav-text">支持的查询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">5.2.</span> <span class="nav-text">注意事项</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">maoeryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">220</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://maoeryu.github.io/2022/11/23/spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="maoeryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlyingPig">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark结构化流编程指南
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-11-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-11-23T00:00:00+08:00">2022-11-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-11-24 10:03:26" itemprop="dateModified" datetime="2022-11-24T10:03:26+08:00">2022-11-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8D%8F%E5%90%8C%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">协同框架</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <style>
table th:first-of-type {
  width: 40%;
}
table th:nth-of-type(2) {
  width: 60%;
}
</style>


<span id="more"></span>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Structured Streaming 是一种基于 Spark SQL 引擎构建的可扩展且容错的流处理引擎.<br>您可以像表达对静态数据的批处理计算一样表达流式计算.<br>Spark SQL 引擎将负责以增量方式持续运行它,并随着流数据不断到达而更新最终结果.<br>您可以使用 Scala/Java/Python/R 中的Dataset/DataFrame API来表达流聚合/事件时间窗口/流到批连接等.<br>计算在同一个优化的 Spark SQL 引擎上执行.<br>最后,系统通过检查点和预写日志确保端到端的恰好一次容错保证.<br>简而言之,Structured Streaming 提供快速/可扩展/容错/端到端的 exactly-once 流处理,而无需用户对流进行推理.</p>
<p>在内部,默认情况下,结构化流查询是使用微批处理引擎处理的,该引擎将数据流处理为一系列小批作业,从而实现低至 100 毫秒的端到端延迟和恰好一次的容错保证.<br>然而,从 Spark 2.3 开始,我们引入了一种新的低延迟处理模式,称为Continuous Processing,它可以实现低至 1 毫秒的端到端延迟,并保证至少一次.<br>在不更改查询中的 Dataset/DataFrame 操作的情况下,您将能够根据您的应用程序要求选择模式.</p>
<p>在本指南中,我们将带您了解编程模型和 API.<br>我们将主要使用默认的微批处理模型来解释这些概念,然后再讨论连续处理模型.<br>首先,让我们从结构化流查询的一个简单示例开始-流式字数统计.</p>
<h2 id="快速示例"><a href="#快速示例" class="headerlink" title="快速示例"></a>快速示例</h2><p>假设您想要维护从侦听 TCP 套接字的数据服务器接收到的文本数据的运行字数.<br>让我们看看如何使用结构化流来表达这一点.<br>您可以在Scala/Java/Python/R中查看完整代码.<br><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/index.html#running-the-examples-and-shell">https://spark.apache.org/docs/2.4.8/index.html#running-the-examples-and-shell</a></p>
<p>如果你下载 Spark,你可以直接运行这个例子.<br>无论如何,让我们一步一步地浏览示例并了解它是如何工作的.<br>首先,我们必须导入必要的类并创建一个本地 SparkSession,这是与 Spark 相关的所有功能的起点.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">val spark &#x3D; SparkSession</span><br><span class="line">  .builder</span><br><span class="line">  .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">  .getOrCreate()</span><br><span class="line">  </span><br><span class="line">import spark.implicits._</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.sql.*;</span><br><span class="line">import org.apache.spark.sql.streaming.StreamingQuery;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">SparkSession spark &#x3D; SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;JavaStructuredNetworkWordCount&quot;)</span><br><span class="line">  .getOrCreate();</span><br></pre></td></tr></table></figure>

<p>接下来,让我们创建一个流式 DataFrame 来表示从侦听 localhost:9999 的服务器接收到的文本数据,并转换 DataFrame 以计算字数.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">val lines &#x3D; spark.readStream</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Split the lines into words</span><br><span class="line">val words &#x3D; lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Generate running word count</span><br><span class="line">val wordCounts &#x3D; words.groupBy(&quot;value&quot;).count()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">&#x2F;&#x2F; Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">Dataset&lt;Row&gt; lines &#x3D; spark</span><br><span class="line">  .readStream()</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Split the lines into words</span><br><span class="line">Dataset&lt;String&gt; words &#x3D; lines</span><br><span class="line">  .as(Encoders.STRING())</span><br><span class="line">  .flatMap((FlatMapFunction&lt;String, String&gt;) x -&gt; Arrays.asList(x.split(&quot; &quot;)).iterator(), Encoders.STRING());</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Generate running word count</span><br><span class="line">Dataset&lt;Row&gt; wordCounts &#x3D; words.groupBy(&quot;value&quot;).count();</span><br></pre></td></tr></table></figure>

<p>此linesDataFrame 表示包含流式文本数据的无限表.<br>该表包含一列名为&quot;value&quot;的字符串,流式文本数据中的每一行都成为表中的一行.<br>请注意,由于我们只是在设置转换,而且还没有启动它,因此它当前没有接收任何数据.<br>接下来,我们使用 将 DataFrame 转换为 String 的Datasets.<code>as[String]</code>,以便我们可以应用flatMap操作将每一行拆分为多个单词.<br>结果wordsDatasets包含所有单词.<br>最后,我们wordCounts通过按Datasets中的唯一值分组并计算它们来定义DataFrames.<br>请注意,这是一个流式DataFrames,表示流的运行字数.</p>
<p>我们现在已经设置了对流数据的查询.<br>剩下的就是实际开始接收数据并计算计数.<br>为此,我们将其设置为在每次更新时将完整的计数集(由outputMode(&quot;complete&quot;)指定)打印到控制台.<br>然后使用start()开始流式计算.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Start running the query that prints the running counts to the console</span><br><span class="line">val query &#x3D; wordCounts.writeStream</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">&#x2F;&#x2F; Start running the query that prints the running counts to the console</span><br><span class="line">StreamingQuery query &#x3D; wordCounts.writeStream()</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">query.awaitTermination();</span><br></pre></td></tr></table></figure>

<p>执行此代码后,流式计算将在后台启动.<br>该query对象是该活动流查询的句柄,我们决定等待查询终止,awaitTermination()以防止进程在查询处于活动状态时退出.</p>
<p>要实际执行此示例代码,您可以在自己的 Spark 应用程序中编译代码,或者在下载 Spark 后直接 运行示例.<br>我们展示的是后者.<br>您首先需要运行 Netcat(大多数类 Unix 系统中的一个小实用程序)作为数据服务器,使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure>

<p>然后,在不同的终端中,您可以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost 9999</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;java</span><br><span class="line">.&#x2F;bin&#x2F;run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999</span><br></pre></td></tr></table></figure>

<p>然后,在运行 netcat 服务器的终端中键入的任何行都将被计算并每秒打印在屏幕上.</p>
<h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>Structured Streaming 的关键思想是将实时数据流视为不断附加的表.<br>这导致了一种与批处理模型非常相似的新流处理模型.<br>您将在静态表上将流式计算表达为标准的类似批处理的查询,而 Spark 将其作为增量查询在无界输入表上运行.<br>让我们更详细地了解这个模型.</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>将输入数据流视为&quot;输入表&quot;.<br>到达流的每个数据项就像一个新行被附加到输入表.</p>
<img src="/images/fly1354.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>对输入的查询将生成&quot;结果表&quot;.<br>每个触发间隔(例如,每 1 秒),新行都会附加到输入表,最终更新结果表.<br>每当结果表更新时,我们都希望将更改后的结果行写入外部接收器.</p>
<img src="/images/fly1355.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>&quot;输出&quot;定义为写入外部存储的内容.<br>输出可以定义为不同的模式:</p>
<ol>
<li>Complete Mode - 整个更新的结果表将被写入外部存储.<br>由存储连接器决定如何处理整个表的写入.</li>
<li>Append Mode - 只有自上次触发以来结果表中追加的新行才会写入外部存储.<br>这仅适用于结果表中现有行预计不会更改的查询.</li>
<li>Update Mode - 只有自上次触发后结果表中更新的行才会写入外部存储(自 Spark 2.1.1 起可用).<br>请注意,这与完整模式不同,因为此模式仅输出自上次触发以来发生更改的行.<br>如果查询不包含聚合,则相当于 Append 模式.</li>
</ol>
<p>请注意,每种模式都适用于特定类型的查询.<br>这将在后面详细讨论.</p>
<p>为了说明该模型的用法,让我们在上面的快速示例的上下文中理解该模型.<br>第一个linesDataFrame 是输入表,最后一个wordCountsDataFrame 是结果表.<br>请注意,lines要生成wordCounts的流式 DataFrame 查询与静态 DataFrame 完全相同.<br>但是,当这个查询开始时,Spark 会不断地检查来自套接字连接的新数据.<br>如果有新数据,Spark 将运行一个&quot;增量&quot;查询,将以前运行的计数与新数据结合起来计算更新的计数,如下所示.</p>
<img src="/images/fly1356.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>请注意,结构化流式传输不会具体化整个表.<br>它从流式数据源中读取最新的可用数据,对其进行增量处理以更新结果,然后丢弃源数据.<br>它只保留更新结果所需的最小中间状态数据(例如,前面示例中的中间计数).</p>
<p>该模型与许多其他流处理引擎有很大不同.<br>许多流式系统要求用户自己维护运行的聚合,因此必须考虑容错和数据一致性(至少一次,最多一次,或恰好一次).<br>在这个模型中,当有新数据出现时,Spark 负责更新 Result Table,从而减轻用户的推理负担.<br>作为示例,让我们看看该模型如何处理基于事件时间的处理和延迟到达的数据.</p>
<h3 id="处理事件时间和延迟数据"><a href="#处理事件时间和延迟数据" class="headerlink" title="处理事件时间和延迟数据"></a>处理事件时间和延迟数据</h3><p>事件时间是嵌入数据本身的时间.<br>对于许多应用程序,您可能希望在此事件时间上进行操作.<br>例如,如果你想获取物联网设备每分钟产生的事件数量,那么你可能希望使用数据产生的时间(也就是数据中的event-time),而不是Spark接收到他们的时间.<br>这个事件时间在这个模型中非常自然地表达-来自设备的每个事件都是表中的一行,事件时间是行中的一个列值.<br>这允许基于窗口的聚合(例如每分钟的事件数)只是事件时间列上的一种特殊类型的分组和聚合-每个时间窗口都是一个组,每一行可以属于多个窗口/组.</p>
<p>此外,该模型自然会根据事件时间处理比预期晚到达的数据.<br>由于 Spark 正在更新结果表,它可以完全控制在有延迟数据时更新旧聚合,以及清理旧聚合以限制中间状态数据的大小.<br>从 Spark 2.1 开始,我们支持水印,允许用户指定延迟数据的阈值,并允许引擎相应地清理旧状态.<br>这些将在稍后的&quot;窗口操作&quot;部分中进行更详细的解释.</p>
<h3 id="容错语义"><a href="#容错语义" class="headerlink" title="容错语义"></a>容错语义</h3><p>提供端到端的 exactly-once 语义是结构化流设计背后的关键目标之一.<br>为实现这一目标,我们设计了 Structured Streaming sources/sinks/execution engine 以可靠地跟踪处理的确切进度,以便它可以通过重新启动和/或重新处理来处理任何类型的故障.<br>假定每个流源都有偏移量(类似于 Kafka 偏移量或 Kinesis 序列号)来跟踪流中的读取位置.<br>该引擎使用检查点和预写日志来记录每个触发器中正在处理的数据的偏移范围.<br>流水槽被设计为幂等处理再处理.<br>一起使用可重放的源和幂等的接收器,结构化流可以确保端到端的恰好一次语义在任何失败下.</p>
<h2 id="使用Datasets和DataFrames的-API"><a href="#使用Datasets和DataFrames的-API" class="headerlink" title="使用Datasets和DataFrames的 API"></a>使用Datasets和DataFrames的 API</h2><p>从 Spark 2.0 开始,DataFrames/Datasets 可以表示静态的/有界的数据,也可以表示流式的/无界的数据.<br>与静态Datasets/DataFrames类似,您可以使用公共入口点SparkSession (Scala/Java/Python/R文档)从流式源创建流式DataFrames/Datasets,并对其应用与静态DataFrames/Datasets相同的操作.<br>如果您不熟悉 Datasets/DataFrame,强烈建议您使用 DataFrame/Dataset Programming Guide熟悉它们.</p>
<h3 id="创建流DataFrames和流Datasets"><a href="#创建流DataFrames和流Datasets" class="headerlink" title="创建流DataFrames和流Datasets"></a>创建流DataFrames和流Datasets</h3><h4 id="输入源"><a href="#输入源" class="headerlink" title="输入源"></a>输入源</h4><p>有一些内置资源.</p>
<ol>
<li>File source - 读取目录中的文件作为数据流.<br>支持的文件格式有text/csv/json/orc/parquet.<br>请参阅 DataStreamReader 接口的文档以获取最新列表以及每种文件格式的支持选项.<br>请注意,文件必须以原子方式放置在给定目录中,这在大多数文件系统中可以通过文件移动操作来实现.</li>
<li>Kafka source - 从 Kafka 读取数据.<br>它与 Kafka 代理版本 0.10.0 或更高版本兼容.<br>有关详细信息,请参阅Kafka 集成指南.</li>
<li>套接字源(用于测试) - 从套接字连接读取 UTF8 文本数据.<br>侦听服务器套接字位于驱动程序处.<br>请注意,这应该仅用于测试,因为它不提供端到端的容错保证.</li>
<li>速率源(用于测试) - 以每秒指定的行数生成数据,每个输出行包含一个timestamp和value.<br>其中timestamp是Timestamp包含消息发送时间的类型,value是Long包含消息计数的类型,从 0 开始作为第一行.<br>此源用于测试和基准测试.</li>
</ol>
<p>一些源不是容错的,因为它们不保证在失败后可以使用检查点偏移量重放数据.<br>请参阅前面关于 容错语义的部分.<br>以下是 Spark 中所有源的详细信息.</p>
<img src="/images/fly1357.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1358.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>这里有些例子.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">val spark: SparkSession &#x3D; ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Read text from socket</span><br><span class="line">val socketDF &#x3D; spark</span><br><span class="line">  .readStream</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">socketDF.isStreaming    &#x2F;&#x2F; Returns True for DataFrames that have streaming sources</span><br><span class="line"></span><br><span class="line">socketDF.printSchema</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Read all the csv files written atomically in a directory</span><br><span class="line">val userSchema &#x3D; new StructType().add(&quot;name&quot;, &quot;string&quot;).add(&quot;age&quot;, &quot;integer&quot;)</span><br><span class="line">val csvDF &#x3D; spark</span><br><span class="line">  .readStream</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .schema(userSchema)      &#x2F;&#x2F; Specify schema of the csv files</span><br><span class="line">  .csv(&quot;&#x2F;path&#x2F;to&#x2F;directory&quot;)    &#x2F;&#x2F; Equivalent to format(&quot;csv&quot;).load(&quot;&#x2F;path&#x2F;to&#x2F;directory&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">SparkSession spark &#x3D; ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Read text from socket</span><br><span class="line">Dataset&lt;Row&gt; socketDF &#x3D; spark</span><br><span class="line">  .readStream()</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load();</span><br><span class="line"></span><br><span class="line">socketDF.isStreaming();    &#x2F;&#x2F; Returns True for DataFrames that have streaming sources</span><br><span class="line"></span><br><span class="line">socketDF.printSchema();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Read all the csv files written atomically in a directory</span><br><span class="line">StructType userSchema &#x3D; new StructType().add(&quot;name&quot;, &quot;string&quot;).add(&quot;age&quot;, &quot;integer&quot;);</span><br><span class="line">Dataset&lt;Row&gt; csvDF &#x3D; spark</span><br><span class="line">  .readStream()</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .schema(userSchema)      &#x2F;&#x2F; Specify schema of the csv files</span><br><span class="line">  .csv(&quot;&#x2F;path&#x2F;to&#x2F;directory&quot;);    &#x2F;&#x2F; Equivalent to format(&quot;csv&quot;).load(&quot;&#x2F;path&#x2F;to&#x2F;directory&quot;)</span><br></pre></td></tr></table></figure>

<p>这些示例生成未类型化的流式 DataFrame,这意味着 DataFrame 的模式不会在编译时检查,仅在提交查询时在运行时检查.<br>map,等一些操作flatMap需要在编译时知道类型.<br>为此,您可以使用与静态 DataFrame 相同的方法将这些未类型化的流式DataFrames转换为类型化的流式Datasets.<br>有关详细信息,请参阅SQL 编程指南.<br>此外,有关支持的流媒体源的更多详细信息将在文档后面讨论.</p>
<h4 id="流DataFrames-Datasets的模式推断和分区"><a href="#流DataFrames-Datasets的模式推断和分区" class="headerlink" title="流DataFrames/Datasets的模式推断和分区"></a>流DataFrames/Datasets的模式推断和分区</h4><p>默认情况下,来自基于文件的源的结构化流需要您指定模式,而不是依赖 Spark 自动推断它.<br>此限制可确保一致的架构将用于流式查询,即使在出现故障的情况下也是如此.<br>对于临时用例,您可以通过设置<code>spark.sql.streaming.schemaInference</code>为重新启用模式推断true.</p>
<p>/key=value/当命名的子目录存在并且列表将自动递归到这些目录中时,会发生分区发现.<br>如果这些列出现在用户提供的模式中,它们将由 Spark 根据正在读取的文件的路径进行填充.<br>构成分区方案的目录必须在查询开始时存在并且必须保持静态.<br>比如/data/year=2016/ when /data/year=2015/ was present添加是可以的,但是改变分区列(即创建目录/data/date=2016-04-17/)是无效的.</p>
<h3 id="流式DataFrames-Datasets上的操作"><a href="#流式DataFrames-Datasets上的操作" class="headerlink" title="流式DataFrames/Datasets上的操作"></a>流式DataFrames/Datasets上的操作</h3><p>您可以在流DataFrames/Datasets上应用各种操作-从无类型的/类 SQL 操作(例如select, where, groupBy)到类型化的 RDD 类操作(例如map, filter, flatMap).<br>有关详细信息,请参阅SQL 编程指南.<br>让我们看一下您可以使用的几个示例操作.</p>
<h4 id="基本操作-选择-投影-聚合"><a href="#基本操作-选择-投影-聚合" class="headerlink" title="基本操作-选择/投影/聚合"></a>基本操作-选择/投影/聚合</h4><p>DataFrame/Dataset 上的大多数常见操作都支持流式传输.<br>不支持的少数操作将在本节后面讨论.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)</span><br><span class="line"></span><br><span class="line">val df: DataFrame &#x3D; ... &#x2F;&#x2F; streaming DataFrame with IOT device data with schema &#123; device: string, deviceType: string, signal: double, time: string &#125;</span><br><span class="line">val ds: Dataset[DeviceData] &#x3D; df.as[DeviceData]    &#x2F;&#x2F; streaming Dataset with IOT device data</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Select the devices which have signal more than 10</span><br><span class="line">df.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;)      &#x2F;&#x2F; using untyped APIs   </span><br><span class="line">ds.filter(_.signal &gt; 10).map(_.device)         &#x2F;&#x2F; using typed APIs</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Running count of the number of updates for each device type</span><br><span class="line">df.groupBy(&quot;deviceType&quot;).count()                          &#x2F;&#x2F; using untyped API</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Running average signal for each device type</span><br><span class="line">import org.apache.spark.sql.expressions.scalalang.typed</span><br><span class="line">ds.groupByKey(_.deviceType).agg(typed.avg(_.signal))    &#x2F;&#x2F; using typed API</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">import org.apache.spark.api.java.function.*;</span><br><span class="line">import org.apache.spark.sql.*;</span><br><span class="line">import org.apache.spark.sql.expressions.javalang.typed;</span><br><span class="line">import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;</span><br><span class="line"></span><br><span class="line">public class DeviceData &#123;</span><br><span class="line">  private String device;</span><br><span class="line">  private String deviceType;</span><br><span class="line">  private Double signal;</span><br><span class="line">  private java.sql.Date time;</span><br><span class="line">  ...</span><br><span class="line">  &#x2F;&#x2F; Getter and setter methods for each field</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df &#x3D; ...;    &#x2F;&#x2F; streaming DataFrame with IOT device data with schema &#123; device: string, type: string, signal: double, time: DateType &#125;</span><br><span class="line">Dataset&lt;DeviceData&gt; ds &#x3D; df.as(ExpressionEncoder.javaBean(DeviceData.class)); &#x2F;&#x2F; streaming Dataset with IOT device data</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Select the devices which have signal more than 10</span><br><span class="line">df.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;); &#x2F;&#x2F; using untyped APIs</span><br><span class="line">ds.filter((FilterFunction&lt;DeviceData&gt;) value -&gt; value.getSignal() &gt; 10)</span><br><span class="line">  .map((MapFunction&lt;DeviceData, String&gt;) value -&gt; value.getDevice(), Encoders.STRING());</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Running count of the number of updates for each device type</span><br><span class="line">df.groupBy(&quot;deviceType&quot;).count(); &#x2F;&#x2F; using untyped API</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Running average signal for each device type</span><br><span class="line">ds.groupByKey((MapFunction&lt;DeviceData, String&gt;) value -&gt; value.getDeviceType(), Encoders.STRING())</span><br><span class="line">  .agg(typed.avg((MapFunction&lt;DeviceData, Double&gt;) value -&gt; value.getSignal()));</span><br></pre></td></tr></table></figure>

<p>您还可以将流式DataFrames/Datasets注册为临时视图,然后在其上应用 SQL 命令.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;updates&quot;)</span><br><span class="line">spark.sql(&quot;select count(*) from updates&quot;)  &#x2F;&#x2F; returns another streaming DF</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;java</span><br><span class="line">df.createOrReplaceTempView(&quot;updates&quot;);</span><br><span class="line">spark.sql(&quot;select count(*) from updates&quot;);  &#x2F;&#x2F; returns another streaming DF</span><br></pre></td></tr></table></figure>

<p>请注意,您可以通过使用df.isStreaming.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.isStreaming</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;java</span><br><span class="line">df.isStreaming()</span><br></pre></td></tr></table></figure>

<h4 id="事件时间窗口操作"><a href="#事件时间窗口操作" class="headerlink" title="事件时间窗口操作"></a>事件时间窗口操作</h4><p>滑动事件时间窗口上的聚合对于结构化流很简单,并且与分组聚合非常相似.<br>在分组聚合中,为用户指定的分组列中的每个唯一值维护聚合值(例如计数).<br>在基于窗口的聚合的情况下,为行的事件时间所在的每个窗口维护聚合值.<br>让我们用一个例子来理解这一点.</p>
<p>假设我们的快速示例已修改,流现在包含行以及生成行的时间.<br>我们不想计算字数,而是想在 10 分钟窗口内计算字数,每 5 分钟更新一次.<br>也就是说,在 10 分钟窗口 12:00 - 12:10/12:05 - 12:15/12:10 - 12:20 等之间收到的字数中的字数.<br>请注意,12:00 - 12:10 表示数据在 12:00 之后但在 12:10 之前到达.<br>现在,考虑在 12:07 收到的一个词.<br>这个词应该增加对应于两个窗口 12:00 - 12:10/12:05 - 12:15 的计数.<br>因此,计数将由分组键(即单词)和窗口(可以从事件时间计算)两者索引.</p>
<p>结果表如下所示.</p>
<img src="/images/fly1359.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>由于这种开窗类似于分组,在代码中,您可以使用groupBy()和window()操作来表达开窗聚合.<br>您可以在Scala/Java/Python中查看以下示例的完整代码 .<br><a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/v2.4.8/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala">https://github.com/apache/spark/blob/v2.4.8/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala</a><br><a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/v2.4.8/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java">https://github.com/apache/spark/blob/v2.4.8/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val words &#x3D; ... &#x2F;&#x2F; streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Group the data by window and word and compute the count of each group</span><br><span class="line">val windowedCounts &#x3D; words.groupBy(</span><br><span class="line">  window($&quot;timestamp&quot;, &quot;10 minutes&quot;, &quot;5 minutes&quot;),</span><br><span class="line">  $&quot;word&quot;</span><br><span class="line">).count()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">Dataset&lt;Row&gt; words &#x3D; ... &#x2F;&#x2F; streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Group the data by window and word and compute the count of each group</span><br><span class="line">Dataset&lt;Row&gt; windowedCounts &#x3D; words.groupBy(</span><br><span class="line">  functions.window(words.col(&quot;timestamp&quot;), &quot;10 minutes&quot;, &quot;5 minutes&quot;),</span><br><span class="line">  words.col(&quot;word&quot;)</span><br><span class="line">).count();</span><br></pre></td></tr></table></figure>

<h5 id="处理延迟数据和水印"><a href="#处理延迟数据和水印" class="headerlink" title="处理延迟数据和水印"></a>处理延迟数据和水印</h5><p>现在考虑如果其中一个事件迟到应用程序会发生什么.<br>例如,应用程序可以在 12:11 接收到在 12:04(即事件时间)生成的单词.<br>应用程序应该使用时间 12:04 而不是 12:11 来更新窗口的旧计数12:00 - 12:10.<br>这在我们基于窗口的分组中很自然地发生-Structured Streaming 可以长时间保持部分聚合的中间状态,以便延迟数据可以正确更新旧窗口的聚合,如下图所示.</p>
<img src="/images/fly1360.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>然而,要运行此查询数天,系统有必要限制它累积的中间内存状态的数量.<br>这意味着系统需要知道何时可以从内存状态中删除旧聚合,因为应用程序将不再接收该聚合的延迟数据.<br>为了实现这一点,在 Spark 2.1 中,我们引入了 水印,它让引擎自动跟踪数据中的当前事件时间,并尝试相应地清理旧状态.<br>您可以通过指定事件时间列和数据在事件时间方面预计延迟的阈值来定义查询的水印.<br>对于在 time 结束的特定窗口T,引擎将维护状态并允许迟到的数据更新状态,直到(max event time seen by the engine - late threshold &gt; T). 换句话说,阈值内的迟到数据将被聚合,但晚于阈值的数据将开始被丢弃(请参阅本节后面 的确切保证).<br>让我们用一个例子来理解这一点.<br>withWatermark()我们可以使用如下所示轻松地在前面的示例中定义水印.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val words &#x3D; ... &#x2F;&#x2F; streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Group the data by window and word and compute the count of each group</span><br><span class="line">val windowedCounts &#x3D; words</span><br><span class="line">    .withWatermark(&quot;timestamp&quot;, &quot;10 minutes&quot;)</span><br><span class="line">    .groupBy(</span><br><span class="line">        window($&quot;timestamp&quot;, &quot;10 minutes&quot;, &quot;5 minutes&quot;),</span><br><span class="line">        $&quot;word&quot;)</span><br><span class="line">    .count()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">Dataset&lt;Row&gt; words &#x3D; ... &#x2F;&#x2F; streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Group the data by window and word and compute the count of each group</span><br><span class="line">Dataset&lt;Row&gt; windowedCounts &#x3D; words</span><br><span class="line">    .withWatermark(&quot;timestamp&quot;, &quot;10 minutes&quot;)</span><br><span class="line">    .groupBy(</span><br><span class="line">        window(col(&quot;timestamp&quot;), &quot;10 minutes&quot;, &quot;5 minutes&quot;),</span><br><span class="line">        col(&quot;word&quot;))</span><br><span class="line">    .count();</span><br></pre></td></tr></table></figure>

<p>在这个例子中,我们在&quot;时间戳&quot;列的值上定义查询的水印,并将&quot;10 分钟&quot;定义为允许数据延迟的阈值.<br>如果此查询在更新输出模式下运行(稍后在输出模式部分讨论),引擎将继续更新结果表中窗口的计数,直到窗口比水印更旧,水印滞后于&quot;列&quot;中的当前事件时间时间戳&quot;10 分钟.<br>这是一个例子.</p>
<img src="/images/fly1361.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>如图所示,引擎跟踪的最大事件时间为 蓝色虚线,设置为(max event time - &#39;10 mins&#39;) 每次触发开始的水印为红线.<br>例如,当引擎观察到数据时 (12:14, dog),它将下一个触发器的水印设置为12:04.<br>此水印让引擎将中间状态保持额外 10 分钟,以允许对延迟数据进行计数.<br>比如数据(12:09, cat)乱序迟到,落在windows12:00 - 12:10和12:05 - 12:15. 因为它仍然12:04在触发器中的水印之前,所以引擎仍然将中间计数保持为状态并正确更新相关窗口的计数.<br>但是,当水印更新为12:11,窗口的中间状态(12:00 - 12:10)被清除,所有后续数据(例如(12:04, donkey))被认为&quot;太晚了&quot;,因此被忽略.<br>请注意,在每次触发后,更新的计数(即紫色行)将作为触发输出写入接收器,如更新模式所规定的那样.</p>
<p>某些接收器(例如文件)可能不支持更新模式所需的细粒度更新.<br>为了与它们一起工作,我们还支持追加模式,其中仅将最终计数写入接收器.<br>如下图所示.</p>
<p>请注意,withWatermark在非流Datasets上使用是空操作.<br>由于水印不应该以任何方式影响任何批量查询,我们将直接忽略它.</p>
<img src="/images/fly1362.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>与之前的更新模式类似,引擎为每个窗口维护中间计数.<br>但是,部分计数不会更新到结果表,也不会写入接收器.<br>引擎等待&quot;10 分钟&quot;以计算延迟日期,然后丢弃窗口的中间状态 &lt; 水印,并将最终计数附加到结果表/接收器.<br>例如,窗口的最终计数12:00 - 12:10仅在水印更新为 后才附加到结果表中12:11.</p>
<h5 id="加水印清除聚合状态的条件"><a href="#加水印清除聚合状态的条件" class="headerlink" title="加水印清除聚合状态的条件"></a>加水印清除聚合状态的条件</h5><p>重要的是要注意,水印必须满足以下条件才能清除聚合查询中的状态(从 Spark 2.1.1 开始,将来可能会发生变化).</p>
<ul>
<li>输出模式必须是追加或更新.完整模式要求保留所有聚合数据,因此不能使用水印来删除中间状态.有关每种输出模式的语义的详细说明,请参阅输出模式 部分.</li>
<li>聚合必须具有事件时间列或window事件时间列.</li>
<li>withWatermark必须在与聚合中使用的时间戳列相同的列上调用. 例如, df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time2&quot;).count()在 Append 输出模式下无效,因为水印是在与聚合列不同的列上定义的.</li>
<li>withWatermark必须在聚合之前调用以使用水印详细信息.例如,df.groupBy(&quot;time&quot;).count().withWatermark(&quot;time&quot;, &quot;1 min&quot;)在 Append 输出模式下无效.</li>
</ul>
<h5 id="带水印聚合的语义保证"><a href="#带水印聚合的语义保证" class="headerlink" title="带水印聚合的语义保证"></a>带水印聚合的语义保证</h5><p>&quot;2 小时&quot;的水印延迟(设置withWatermark)保证引擎永远不会丢弃任何延迟少于 2 小时的数据.<br>换句话说,任何比到那时处理的最新数据晚不到 2 小时(就事件时间而言)的数据都保证会被聚合.</p>
<p>然而,保证仅在一个方向上是严格的.<br>延迟超过 2 小时的数据不保证被丢弃；它可能会或可能不会被聚合.<br>数据延迟越多,引擎处理它的可能性就越小.</p>
<h4 id="Join操作"><a href="#Join操作" class="headerlink" title="Join操作"></a>Join操作</h4><p>结构化流支持将流Datasets/DataFrames与静态Datasets/DataFrames以及另一个流Datasets/DataFrames连接起来.<br>streaming join 的结果是增量生成的,类似于上一节的 streaming aggregation 的结果.<br>在本节中,我们将探讨在上述情况下支持哪些类型的连接(即内部/外部等).<br>请注意,在所有受支持的连接类型中,与流Datasets/DataFrames的连接结果将与与流中包含相同数据的静态Datasets/DataFrames的连接结果完全相同.</p>
<h5 id="流静态连接"><a href="#流静态连接" class="headerlink" title="流静态连接"></a>流静态连接</h5><p>自从在 Spark 2.0 中引入以来,Structured Streaming 就支持流和静态 DataFrame/Dataset 之间的连接(内部连接和某种类型的外部连接).<br>这是一个简单的例子.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val staticDf &#x3D; spark.read. ...</span><br><span class="line">val streamingDf &#x3D; spark.readStream. ...</span><br><span class="line"></span><br><span class="line">streamingDf.join(staticDf, &quot;type&quot;)          &#x2F;&#x2F; inner equi-join with a static DF</span><br><span class="line">streamingDf.join(staticDf, &quot;type&quot;, &quot;left_outer&quot;)  &#x2F;&#x2F; left outer join with a static DF</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">Dataset&lt;Row&gt; staticDf &#x3D; spark.read(). ...;</span><br><span class="line">Dataset&lt;Row&gt; streamingDf &#x3D; spark.readStream(). ...;</span><br><span class="line">streamingDf.join(staticDf, &quot;type&quot;);         &#x2F;&#x2F; inner equi-join with a static DF</span><br><span class="line">streamingDf.join(staticDf, &quot;type&quot;, &quot;left_outer&quot;);  &#x2F;&#x2F; left outer join with a static DF</span><br></pre></td></tr></table></figure>

<p>请注意,流静态连接不是有状态的,因此不需要状态管理.<br>但是,尚不支持几种类型的流静态外部联接.<br>这些列在加入部分的末尾.</p>
<h5 id="流-流连接"><a href="#流-流连接" class="headerlink" title="流-流连接"></a>流-流连接</h5><p>在 Spark 2.3 中,我们增加了对流-流连接的支持,即你可以连接两个流式 Datasets/DataFrames.<br>在两个数据流之间生成连接结果的挑战在于,在任何时间点,连接两侧的Datasets视图都是不完整的,这使得在输入之间找到匹配变得更加困难.<br>从一个输入流接收到的任何行都可以与来自另一个输入流的任何未来的/尚未接收到的行相匹配.<br>因此,对于两个输入流,我们将过去的输入缓冲为流状态,这样我们就可以将每个未来的输入与过去的输入进行匹配,并相应地生成连接结果.<br>此外,与流式聚合类似,我们会自动处理延迟的/乱序的数据,并可以使用水印来限制状态.<br>让我们讨论支持的流-流连接的不同类型以及如何使用它们.</p>
<h6 id="带有可选水印的内部连接"><a href="#带有可选水印的内部连接" class="headerlink" title="带有可选水印的内部连接"></a>带有可选水印的内部连接</h6><p>支持任何类型的列的内部连接以及任何类型的连接条件.<br>然而,随着流的运行,流状态的大小将无限增长,因为 必须保存所有过去的输入,因为任何新输入都可以与过去的任何输入相匹配.<br>为了避免无限状态,您必须定义额外的连接条件,以便无限期的旧输入无法与未来的输入匹配,因此可以从状态中清除.<br>换句话说,您将必须在联接中执行以下附加步骤.</p>
<ul>
<li>在两个输入上定义水印延迟,以便引擎知道输入可以延迟多长时间(类似于流式聚合)</li>
<li>定义两个输入的事件时间约束,以便引擎可以确定何时不需要一个输入的旧行(即不满足时间约束)来匹配另一个输入. 这个约束可以用两种方式之一来定义.<ul>
<li>时间范围加入条件(例如...JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR),</li>
<li>加入事件时间窗口(例如...JOIN ON leftTimeWindow = rightTimeWindow).</li>
</ul>
</li>
</ul>
<p>让我们用一个例子来理解这一点.</p>
<p>假设我们想要加入一个广告印象流(当显示广告时)与另一个用户点击广告流,以便在印象导致可获利点击时关联起来.<br>要允许在此流-流连接中进行状态清理,您必须指定水印延迟和时间限制,如下所示.</p>
<ol>
<li>水印延迟:比如说,印象和相应的点击在事件时间上最多分别延迟 2 小时和 3 小时.</li>
<li>Event-time range condition:比如说,点击可以在相应展示后的0秒到1小时的时间范围内发生.</li>
</ol>
<p>代码看起来像这样.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.functions.expr</span><br><span class="line"></span><br><span class="line">val impressions &#x3D; spark.readStream. ...</span><br><span class="line">val clicks &#x3D; spark.readStream. ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Apply watermarks on event-time columns</span><br><span class="line">val impressionsWithWatermark &#x3D; impressions.withWatermark(&quot;impressionTime&quot;, &quot;2 hours&quot;)</span><br><span class="line">val clicksWithWatermark &#x3D; clicks.withWatermark(&quot;clickTime&quot;, &quot;3 hours&quot;)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Join with event-time constraints</span><br><span class="line">impressionsWithWatermark.join(</span><br><span class="line">  clicksWithWatermark,</span><br><span class="line">  expr(&quot;&quot;&quot;</span><br><span class="line">    clickAdId &#x3D; impressionAdId AND</span><br><span class="line">    clickTime &gt;&#x3D; impressionTime AND</span><br><span class="line">    clickTime &lt;&#x3D; impressionTime + interval 1 hour</span><br><span class="line">    &quot;&quot;&quot;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">import static org.apache.spark.sql.functions.expr</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; impressions &#x3D; spark.readStream(). ...</span><br><span class="line">Dataset&lt;Row&gt; clicks &#x3D; spark.readStream(). ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Apply watermarks on event-time columns</span><br><span class="line">Dataset&lt;Row&gt; impressionsWithWatermark &#x3D; impressions.withWatermark(&quot;impressionTime&quot;, &quot;2 hours&quot;);</span><br><span class="line">Dataset&lt;Row&gt; clicksWithWatermark &#x3D; clicks.withWatermark(&quot;clickTime&quot;, &quot;3 hours&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Join with event-time constraints</span><br><span class="line">impressionsWithWatermark.join(</span><br><span class="line">  clicksWithWatermark,</span><br><span class="line">  expr(</span><br><span class="line">    &quot;clickAdId &#x3D; impressionAdId AND &quot; +</span><br><span class="line">    &quot;clickTime &gt;&#x3D; impressionTime AND &quot; +</span><br><span class="line">    &quot;clickTime &lt;&#x3D; impressionTime + interval 1 hour &quot;)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>带有水印的流-流内部连接的语义保证<br>这类似于聚合水印提供的保证.<br>&quot;2 小时&quot;的水印延迟保证引擎永远不会丢弃任何延迟少于 2 小时的数据.<br>但延迟超过 2 小时的数据可能会或可能不会得到处理.</p>
</blockquote>
<h6 id="带水印的外连接"><a href="#带水印的外连接" class="headerlink" title="带水印的外连接"></a>带水印的外连接</h6><p>虽然水印 + 事件时间约束对于内连接是可选的,但对于左外连接和右外连接必须指定它们.<br>这是因为要在外连接中生成 NULL 结果,引擎必须知道输入行将来何时不会与任何内容匹配.<br>因此,必须指定水印 + 事件时间约束才能生成正确的结果.<br>因此,使用外连接的查询看起来很像前面的广告货币化示例,只是会有一个额外的参数指定它是外连接.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">impressionsWithWatermark.join(</span><br><span class="line">  clicksWithWatermark,</span><br><span class="line">  expr(&quot;&quot;&quot;</span><br><span class="line">    clickAdId &#x3D; impressionAdId AND</span><br><span class="line">    clickTime &gt;&#x3D; impressionTime AND</span><br><span class="line">    clickTime &lt;&#x3D; impressionTime + interval 1 hour</span><br><span class="line">    &quot;&quot;&quot;),</span><br><span class="line">  joinType &#x3D; &quot;leftOuter&quot;      &#x2F;&#x2F; can be &quot;inner&quot;, &quot;leftOuter&quot;, &quot;rightOuter&quot;</span><br><span class="line"> )</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">impressionsWithWatermark.join(</span><br><span class="line">  clicksWithWatermark,</span><br><span class="line">  expr(</span><br><span class="line">    &quot;clickAdId &#x3D; impressionAdId AND &quot; +</span><br><span class="line">    &quot;clickTime &gt;&#x3D; impressionTime AND &quot; +</span><br><span class="line">    &quot;clickTime &lt;&#x3D; impressionTime + interval 1 hour &quot;),</span><br><span class="line">  &quot;leftOuter&quot;                 &#x2F;&#x2F; can be &quot;inner&quot;, &quot;leftOuter&quot;, &quot;rightOuter&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>带有水印的流-流外连接的语义保证<br>关于水印延迟以及数据是否会被丢弃,外连接与内连接具有相同的保证.</p>
</blockquote>
<blockquote>
<p>注意事项</p>
</blockquote>
<p>关于如何生成外部结果,有几个重要特征需要注意.</p>
<ol>
<li>将生成外部 NULL 结果,延迟取决于指定的水印延迟和时间范围条件.<br>这是因为引擎必须等待那么长时间才能确保没有匹配项,并且将来不会再有匹配项.</li>
<li>在微批引擎的当前实现中,水印在微批的末尾提前,下一个微批使用更新的水印来清理状态并输出外部结果.<br>由于我们仅在有新数据要处理时触发微批处理,因此如果流中没有接收到新数据,外部结果的生成可能会延迟.<br>简而言之,如果被连接的两个输入流中的任何一个在一段时间内没有接收到数据,则外部(左或右两种情况)输出可能会延迟.</li>
</ol>
<h6 id="流式查询中连接的支持矩阵"><a href="#流式查询中连接的支持矩阵" class="headerlink" title="流式查询中连接的支持矩阵"></a>流式查询中连接的支持矩阵</h6><img src="/images/fly1363.png" style="margin-left: 0px; padding-bottom: 10px;">

<ol>
<li>Joins 可以级联,也就是说,你可以做df1.join(df2, ...).join(df3, ...).join(df4, ....).</li>
<li>从 Spark 2.4 开始,您只能在查询处于追加输出模式时使用连接.<br>尚不支持其他输出模式.</li>
<li>从 Spark 2.4 开始,您不能在连接之前使用其他非类映射操作.<br>以下是一些不能使用的示例.<br>连接前不能使用流式聚合.<br>加入前不能在更新模式下使用 mapGroupsWithState/flatMapGroupsWithState.</li>
</ol>
<h4 id="流式重复数据删除"><a href="#流式重复数据删除" class="headerlink" title="流式重复数据删除"></a>流式重复数据删除</h4><p>您可以使用事件中的唯一标识符删除数据流中的重复记录.<br>这与使用唯一标识符列的静态重复数据删除完全相同.<br>查询将存储来自以前记录的必要数据量,以便它可以过滤重复记录.<br>与聚合类似,您可以使用带水印或不带水印的重复数据删除.</p>
<ol>
<li>带水印 - 如果重复记录到达的时间有上限,那么您可以在事件时间列上定义水印,并使用 guid 和事件时间列进行重复数据删除.<br>查询将使用水印从过去的记录中删除旧状态数据,这些数据预计不会再获得任何重复项.<br>这限制了查询必须维护的状态量.</li>
<li>没有水印 - 由于重复记录到达的时间没有限制,因此查询将所有过去记录的数据存储为状态.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val streamingDf &#x3D; spark.readStream. ...  &#x2F;&#x2F; columns: guid, eventTime, ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Without watermark using guid column</span><br><span class="line">streamingDf.dropDuplicates(&quot;guid&quot;)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; With watermark using guid and eventTime columns</span><br><span class="line">streamingDf</span><br><span class="line">  .withWatermark(&quot;eventTime&quot;, &quot;10 seconds&quot;)</span><br><span class="line">  .dropDuplicates(&quot;guid&quot;, &quot;eventTime&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">Dataset&lt;Row&gt; streamingDf &#x3D; spark.readStream(). ...;  &#x2F;&#x2F; columns: guid, eventTime, ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Without watermark using guid column</span><br><span class="line">streamingDf.dropDuplicates(&quot;guid&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; With watermark using guid and eventTime columns</span><br><span class="line">streamingDf</span><br><span class="line">  .withWatermark(&quot;eventTime&quot;, &quot;10 seconds&quot;)</span><br><span class="line">  .dropDuplicates(&quot;guid&quot;, &quot;eventTime&quot;);</span><br></pre></td></tr></table></figure>

<h4 id="处理多个水印的策略"><a href="#处理多个水印的策略" class="headerlink" title="处理多个水印的策略"></a>处理多个水印的策略</h4><p>流式查询可以有多个联合或连接在一起的输入流.<br>每个输入流都可以有不同的延迟数据阈值,需要为有状态操作容忍.<br>withWatermarks(&quot;eventTime&quot;, delay)您可以在每个输入流上使用指定这些阈值 .<br>例如,考虑一个在/之间进行流-流连接的inputStream1查询inputStream2.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputStream1.withWatermark(&quot;eventTime1&quot;, &quot;1 hour&quot;)</span><br><span class="line">  .join(</span><br><span class="line">    inputStream2.withWatermark(&quot;eventTime2&quot;, &quot;2 hours&quot;),</span><br><span class="line">    joinCondition)</span><br></pre></td></tr></table></figure>

<p>在执行查询时,Structured Streaming 单独跟踪每个输入流中看到的最大事件时间,根据相应的延迟计算水印,并选择一个全局水印用于有状态操作.<br>默认情况下,选择最小值作为全局水印,因为它确保如果其中一个流落后于其他流(例如,其中一个流由于上游故障而停止接收数据),则不会因太晚而意外丢弃任何数据.<br>换句话说,全局水印将安全地以最慢流的速度移动,查询输出将相应地延迟.</p>
<p>但是,在某些情况下,您可能希望获得更快的结果,即使这意味着从最慢的流中删除数据.<br>spark.sql.streaming.multipleWatermarkPolicy从 Spark 2.4 开始,您可以通过将 SQL 配置设置为max(默认为min)来设置多重水印策略以选择最大值作为全局水印 .<br>这让全局水印以最快的流速度移动.<br>然而,作为副作用,来自较慢流的数据将被积极丢弃.<br>因此,请谨慎使用此配置.</p>
<h4 id="任意状态操作"><a href="#任意状态操作" class="headerlink" title="任意状态操作"></a>任意状态操作</h4><p>许多用例需要比聚合更高级的有状态操作.<br>例如,在许多用例中,您必须从事件数据流中跟踪会话.<br>要进行此类会话化,您必须将任意类型的数据保存为状态,并在每个触发器中使用数据流事件对状态执行任意操作.<br>从 Spark 2.2 开始,这可以使用操作mapGroupsWithState和更强大的操作来完成flatMapGroupsWithState.<br>这两种操作都允许您在分组Datasets上应用用户定义的代码来更新用户定义的状态.<br>有关更具体的详细信息,请查看 API 文档 ( Scala/Java ) 和示例 ( Scala/Java ).</p>
<h4 id="不受支持的操作"><a href="#不受支持的操作" class="headerlink" title="不受支持的操作"></a>不受支持的操作</h4><p>流式DataFrames/Datasets不支持一些DataFrames/Datasets操作.<br>其中一些如下.</p>
<ol>
<li>流式Datasets尚不支持多个流式聚合(即流式 DF 上的聚合链).</li>
<li>流式Datasets不支持限制和获取前 N 行.</li>
<li>不支持对流Datasets进行不同的操作.</li>
<li>仅在聚合后和在完整输出模式下,流式Datasets才支持排序操作.</li>
<li>不支持流式Datasets上的少数类型的外部连接.<br>有关更多详细信息,请参阅 加入操作部分中的支持矩阵.</li>
</ol>
<p>此外,还有一些 Dataset 方法不适用于流Datasets.<br>它们是将立即运行查询并返回结果的操作,这在流式Datasets上没有意义.<br>相反,这些功能可以通过显式启动流式查询来完成(参见下一节).</p>
<ol>
<li>count() - 无法从流式Datasets中返回单个计数.<br>相反,使用ds.groupBy().count()它返回一个包含运行计数的流Datasets.</li>
<li>foreach() - 而是使用ds.writeStream.foreach(...)(见下一节).</li>
<li>show() - 而是使用控制台接收器(请参阅下一节).</li>
</ol>
<p>如果您尝试这些操作中的任何一个,您将看到AnalysisException类似&quot;流式DataFrames/Datasets不支持操作 XYZ&quot;的信息.<br>虽然其中一些可能会在未来的 Spark 版本中得到支持,但还有一些很难在流数据上有效地实现.<br>例如,不支持对输入流进行排序,因为它需要跟踪流中接收到的所有数据.<br>因此,这从根本上很难有效执行.</p>
<h3 id="开始流式查询"><a href="#开始流式查询" class="headerlink" title="开始流式查询"></a>开始流式查询</h3><p>一旦定义了最终结果 DataFrame/Dataset,剩下的就是开始流式计算.<br>为此,您必须使用通过Dataset.writeStream()返回的DataStreamWriter (Scala/Java/Python文档).<br>您必须在此界面中指定以下一项或多项.</p>
<ol>
<li>输出接收器的详细信息:数据格式/位置等.</li>
<li>输出模式:指定写入输出接收器的内容.</li>
<li>查询名称:可选,指定查询的唯一名称以供标识.</li>
<li>触发间隔:可选,指定触发间隔.如果未指定,则系统将在先前处理完成后立即检查新数据的可用性.如果因为前面的处理没有完成而错过了一个触发时间,那么系统会立即触发处理.</li>
<li>Checkpoint位置:对于一些可以保证端到端容错的output sink,指定系统将写入所有checkpoint信息的位置.<br>这应该是与 HDFS 兼容的容错文件系统中的一个目录.<br>下一节将更详细地讨论检查点的语义.</li>
</ol>
<h4 id="Output-Modes"><a href="#Output-Modes" class="headerlink" title="Output Modes"></a>Output Modes</h4><p>有几种类型的输出模式.</p>
<ol>
<li>Append mode (default) - 这是默认模式,只有自上次触发后添加到结果表的新行才会输出到接收器.<br>只有那些添加到结果表的行永远不会改变的查询才支持此功能.<br>因此,这种模式保证每行只输出一次(假设容错接收器).<br>例如,只有select, where, map, flatMap, filter,join等的查询将支持追加模式.</li>
<li>Complete mode - 整个 Result Table 将在每次触发后输出到 sink.<br>这支持聚合查询.</li>
<li>Update mode -(自 Spark 2.1.1 起可用)只有结果表中自上次触发以来更新的行将输出到接收器.<br>将在未来版本中添加更多信息.</li>
</ol>
<p>不同类型的流式查询支持不同的输出模式.<br>这是兼容性矩阵.</p>
<img src="/images/fly1364.png" style="margin-left: 0px; padding-bottom: 10px;">

<h4 id="Output-Sinks"><a href="#Output-Sinks" class="headerlink" title="Output Sinks"></a>Output Sinks</h4><p>有几种类型的内置输出接收器.</p>
<ul>
<li>File sink - 将输出存储到目录.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;parquet&quot;)        &#x2F;&#x2F; can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span><br><span class="line">    .option(&quot;path&quot;, &quot;path&#x2F;to&#x2F;destination&#x2F;dir&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li>
<li>Kafka sink - 将输出存储到 Kafka 中的一个或多个主题.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;kafka&quot;)</span><br><span class="line">    .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">    .option(&quot;topic&quot;, &quot;updates&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li>
<li>Foreach sink - 对输出中的记录运行任意计算.<br>有关更多详细信息,请参阅本节后面的内容.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li>
<li>Console sink(用于调试) - 每次触发时将输出打印到控制台/标准输出.<br>Append/Complete 输出模式均受支持.<br>这应该用于低数据量的调试目的,因为在每次触发后收集整个输出并存储在驱动程序的内存中.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;console&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li>
<li>Memory sink(用于调试) - 输出作为内存表存储在内存中.<br>Append/Complete 输出模式均受支持.<br>这应该用于低数据量的调试目的,因为整个输出被收集并存储在驱动程序的内存中.<br>因此,请谨慎使用.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;memory&quot;)</span><br><span class="line">    .queryName(&quot;tableName&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>一些接收器不是容错的,因为它们不保证输出的持久性并且仅用于调试目的.<br>请参阅前面关于 容错语义的部分.<br>以下是 Spark 中所有接收器的详细信息.</p>
<img src="/images/fly1365.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>请注意,您必须调用start()才能真正开始执行查询.<br>这将返回一个 StreamingQuery 对象,它是持续运行的执行的句柄.<br>您可以使用此对象来管理查询,我们将在下一小节中讨论.<br>现在,让我们通过几个例子来理解这一切.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; DF with no aggregations &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">val noAggDF &#x3D; deviceDataDf.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;)   </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Print new data to console</span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Write new data to Parquet files</span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;checkpointLocation&quot;, &quot;path&#x2F;to&#x2F;checkpoint&#x2F;dir&quot;)</span><br><span class="line">  .option(&quot;path&quot;, &quot;path&#x2F;to&#x2F;destination&#x2F;dir&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; DF with aggregation &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">val aggDF &#x3D; df.groupBy(&quot;device&quot;).count()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Print updated aggregations to console</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Have all the aggregates in an in-memory table</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .queryName(&quot;aggregates&quot;)    &#x2F;&#x2F; this query name will be the table name</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;memory&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from aggregates&quot;).show()   &#x2F;&#x2F; interactively query in-memory table</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; DF with no aggregations &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Dataset&lt;Row&gt; noAggDF &#x3D; deviceDataDf.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Print new data to console</span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Write new data to Parquet files</span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;checkpointLocation&quot;, &quot;path&#x2F;to&#x2F;checkpoint&#x2F;dir&quot;)</span><br><span class="line">  .option(&quot;path&quot;, &quot;path&#x2F;to&#x2F;destination&#x2F;dir&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; DF with aggregation &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Dataset&lt;Row&gt; aggDF &#x3D; df.groupBy(&quot;device&quot;).count();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Print updated aggregations to console</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Have all the aggregates in an in-memory table</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .queryName(&quot;aggregates&quot;)    &#x2F;&#x2F; this query name will be the table name</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;memory&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from aggregates&quot;).show();   &#x2F;&#x2F; interactively query in-memory table</span><br></pre></td></tr></table></figure>

<h5 id="使用-Foreach-ForeachBatch"><a href="#使用-Foreach-ForeachBatch" class="headerlink" title="使用 Foreach/ForeachBatch"></a>使用 Foreach/ForeachBatch</h5><p>foreach和foreachBatch操作允许您在流式查询的输出上应用任意操作和编写逻辑.<br>它们的用例略有不同-虽然foreach 允许在每一行上自定义写入逻辑,但foreachBatch允许对每个微批处理的输出进行任意操作和自定义逻辑.<br>让我们更详细地了解它们的用法.</p>
<h6 id="ForeachBatch"><a href="#ForeachBatch" class="headerlink" title="ForeachBatch"></a>ForeachBatch</h6><p>foreachBatch(...)允许您指定对流式查询的每个微批处理的输出数据执行的函数.<br>从 Spark 2.4 开始,Scala/Java/Python 都支持这一点.<br>它有两个参数:具有微批输出数据的 DataFrame/Dataset 和微批的唯一 ID.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: DataFrame, batchId: Long) &#x3D;&gt;</span><br><span class="line">  &#x2F;&#x2F; Transform and write batchDF </span><br><span class="line">&#125;.start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">streamingDatasetOfString.writeStream().foreachBatch(</span><br><span class="line">  new VoidFunction2&lt;Dataset&lt;String&gt;, Long&gt;() &#123;</span><br><span class="line">    public void call(Dataset&lt;String&gt; dataset, Long batchId) &#123;</span><br><span class="line">      &#x2F;&#x2F; Transform and write batchDF</span><br><span class="line">    &#125;    </span><br><span class="line">  &#125;</span><br><span class="line">).start();</span><br></pre></td></tr></table></figure>

<p>使用foreachBatch,您可以执行以下操作.</p>
<ol>
<li>重用现有的批处理数据源 - 对于许多存储系统,可能还没有可用的流式接收器,但可能已经存在用于批处理查询的数据编写器.<br>使用foreachBatch,您可以在每个微批次的输出上使用批处理数据编写器.</li>
<li>写入多个位置 - 如果你想将流式查询的输出写入多个位置,那么你可以简单地多次写入输出DataFrames/Datasets.<br>但是,每次尝试写入都可能导致重新计算输出数据(包括可能重新读取输入数据).<br>为避免重新计算,您应该缓存输出 DataFrame/Dataset,将其写入多个位置,然后取消缓存.<br>这是一个大纲.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: DataFrame, batchId: Long) &#x3D;&gt;</span><br><span class="line">  batchDF.persist()</span><br><span class="line">  batchDF.write.format(...).save(...)  &#x2F;&#x2F; location 1</span><br><span class="line">  batchDF.write.format(...).save(...)  &#x2F;&#x2F; location 2</span><br><span class="line">  batchDF.unpersist()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>应用额外的 DataFrame 操作 - 许多 DataFrame/Dataset 操作在流式 DataFrame 中不受支持,因为 Spark 在这些情况下不支持生成增量计划.<br>使用foreachBatch,您可以对每个微批输出应用其中一些操作.<br>但是,您必须自己推理执行该操作的端到端语义.</li>
</ol>
<blockquote>
<p>笔记:</p>
</blockquote>
<ol>
<li>默认情况下,foreachBatch仅提供至少一次写入保证.<br>但是,您可以使用提供给函数的 batchId 作为对输出进行重复数据删除并获得恰好一次保证的方法.</li>
<li>foreachBatch不适用于连续处理模式,因为它从根本上依赖于流式查询的微批处理执行.<br>如果您以连续模式写入数据,请foreach改用.</li>
</ol>
<h6 id="Foreach"><a href="#Foreach" class="headerlink" title="Foreach"></a>Foreach</h6><p>如果foreachBatch不是一个选项(例如,相应的批量数据编写器不存在,或者连续处理模式),那么您可以使用foreach. 具体来说,可以将数据写入逻辑分为open/process/close三种方法来表达.<br>从 Spark 2.4 开始,foreach可以在 Scala/Java/Python 中使用.</p>
<p>在 Scala 中,您必须扩展类ForeachWriter( docs ).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">streamingDatasetOfString.writeStream.foreach(</span><br><span class="line">  new ForeachWriter[String] &#123;</span><br><span class="line"></span><br><span class="line">    def open(partitionId: Long, version: Long): Boolean &#x3D; &#123;</span><br><span class="line">      &#x2F;&#x2F; Open connection</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def process(record: String): Unit &#x3D; &#123;</span><br><span class="line">      &#x2F;&#x2F; Write string to connection</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def close(errorOrNull: Throwable): Unit &#x3D; &#123;</span><br><span class="line">      &#x2F;&#x2F; Close the connection</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">).start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">streamingDatasetOfString.writeStream().foreach(</span><br><span class="line">  new ForeachWriter&lt;String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    @Override public boolean open(long partitionId, long version) &#123;</span><br><span class="line">      &#x2F;&#x2F; Open connection</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override public void process(String record) &#123;</span><br><span class="line">      &#x2F;&#x2F; Write string to connection</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override public void close(Throwable errorOrNull) &#123;</span><br><span class="line">      &#x2F;&#x2F; Close the connection</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">).start();</span><br></pre></td></tr></table></figure>

<p>执行语义,当流式查询开始时,Spark 以下列方式调用函数或对象的方法:</p>
<ul>
<li>该对象的单个副本负责查询中单个任务生成的所有数据.换句话说,一个实例负责处理分布式生成的数据的一个分区.</li>
<li>这个对象必须是可序列化的,因为每个任务都会得到所提供对象的一个新的序列化-反序列化副本.因此,强烈建议在调用 open() 方法之后完成写入数据的任何初始化(例如打开连接或启动事务),这表示任务已准备好生成数据.</li>
<li>方法的生命周期如下:<ul>
<li>对于具有 partition_id 的每个分区:<ul>
<li>对于具有 epoch_id 的每批/epoch 流数据:<ul>
<li>方法 open(partitionId, epochId) 被调用.</li>
<li>如果 open(…) 返回 true,则对于分区和批次/纪元中的每一行,方法 process(row) 被调用.</li>
<li>调用方法 close(error) 时出现错误(如果有),同时处理行.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>如果 open() 方法存在并成功返回(不考虑返回值),则调用 close() 方法(如果存在),除非 JVM/Python 进程在中间崩溃.</li>
</ul>
<p>注意: Spark 不保证 (partitionId, epochId) 的输出相同,因此 (partitionId, epochId) 无法实现去重.<br>例如,由于某些原因,源提供了不同数量的分区,Spark 优化更改了分区数量等.<br>有关更多详细信息,请参阅SPARK-28650.<br>foreachBatch如果您需要对输出进行重复数据删除,请改为尝试.</p>
<h4 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h4><p>流式查询的触发器设置定义了流式数据处理的时间,查询是作为具有固定批处理间隔的微批处理查询还是作为连续处理查询执行.<br>以下是受支持的不同类型的触发器.</p>
<table>
<thead>
<tr>
<th align="left">触发器类型</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">unspecified (default)</td>
<td align="left">如果没有明确指定触发器设置,那么默认情况下,查询将以微批处理模式执行,一旦前一个微批处理完成,就会生成微批处理.</td>
</tr>
<tr>
<td align="left">Fixed interval micro-batches</td>
<td align="left">查询将以微批模式执行,微批将以用户指定的时间间隔启动.</br>1.如果前一个微批在间隔内完成,那么引擎将等到间隔结束后再开始下一个微批.</br>2.如果前一个微批完成的时间比间隔长(即,如果错过了间隔边界),那么下一个微批将在前一个完成后立即开始(即,它不会等待下一个间隔边界)</br>.3.如果没有新数据可用,则不会启动微批处理.</td>
</tr>
<tr>
<td align="left">One-time micro-batch</td>
<td align="left">查询将执行<em>仅一个</em>微批处理以处理所有可用数据,然后自行停止.这在您希望定期启动集群/处理自上次以来可用的所有内容然后关闭集群的情况下很有用.在某些情况下,这可能会导致显着的成本节约.</td>
</tr>
<tr>
<td align="left">Continuous with fixed checkpoint interval(实验)</td>
<td align="left">查询将以新的低延迟/连续处理模式执行.在下面的连续处理部分中阅读更多相关信息.</td>
</tr>
</tbody></table>
<p>下面是一些代码示例.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.streaming.Trigger</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Default trigger (runs micro-batch as soon as it can)</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ProcessingTime trigger with two-seconds micro-batch interval</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.ProcessingTime(&quot;2 seconds&quot;))</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; One-time trigger</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.Once())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Continuous trigger with one-second checkpointing interval</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.Continuous(&quot;1 second&quot;))</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">import org.apache.spark.sql.streaming.Trigger</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Default trigger (runs micro-batch as soon as it can)</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ProcessingTime trigger with two-seconds micro-batch interval</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.ProcessingTime(&quot;2 seconds&quot;))</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; One-time trigger</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.Once())</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Continuous trigger with one-second checkpointing interval</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.Continuous(&quot;1 second&quot;))</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure>

<h3 id="管理流查询"><a href="#管理流查询" class="headerlink" title="管理流查询"></a>管理流查询</h3><p>启动查询时创建的StreamingQuery对象可用于监视和管理查询.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val query &#x3D; df.writeStream.format(&quot;console&quot;).start()   &#x2F;&#x2F; get the query object</span><br><span class="line"></span><br><span class="line">query.id          &#x2F;&#x2F; get the unique identifier of the running query that persists across restarts from checkpoint data</span><br><span class="line"></span><br><span class="line">query.runId       &#x2F;&#x2F; get the unique id of this run of the query, which will be generated at every start&#x2F;restart</span><br><span class="line"></span><br><span class="line">query.name        &#x2F;&#x2F; get the name of the auto-generated or user-specified name</span><br><span class="line"></span><br><span class="line">query.explain()   &#x2F;&#x2F; print detailed explanations of the query</span><br><span class="line"></span><br><span class="line">query.stop()      &#x2F;&#x2F; stop the query</span><br><span class="line"></span><br><span class="line">query.awaitTermination()   &#x2F;&#x2F; block until query is terminated, with stop() or with error</span><br><span class="line"></span><br><span class="line">query.exception       &#x2F;&#x2F; the exception if the query has been terminated with error</span><br><span class="line"></span><br><span class="line">query.recentProgress  &#x2F;&#x2F; an array of the most recent progress updates for this query</span><br><span class="line"></span><br><span class="line">query.lastProgress    &#x2F;&#x2F; the most recent progress update of this streaming query</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">StreamingQuery query &#x3D; df.writeStream().format(&quot;console&quot;).start();   &#x2F;&#x2F; get the query object</span><br><span class="line"></span><br><span class="line">query.id();          &#x2F;&#x2F; get the unique identifier of the running query that persists across restarts from checkpoint data</span><br><span class="line"></span><br><span class="line">query.runId();       &#x2F;&#x2F; get the unique id of this run of the query, which will be generated at every start&#x2F;restart</span><br><span class="line"></span><br><span class="line">query.name();        &#x2F;&#x2F; get the name of the auto-generated or user-specified name</span><br><span class="line"></span><br><span class="line">query.explain();   &#x2F;&#x2F; print detailed explanations of the query</span><br><span class="line"></span><br><span class="line">query.stop();      &#x2F;&#x2F; stop the query</span><br><span class="line"></span><br><span class="line">query.awaitTermination();   &#x2F;&#x2F; block until query is terminated, with stop() or with error</span><br><span class="line"></span><br><span class="line">query.exception();       &#x2F;&#x2F; the exception if the query has been terminated with error</span><br><span class="line"></span><br><span class="line">query.recentProgress();  &#x2F;&#x2F; an array of the most recent progress updates for this query</span><br><span class="line"></span><br><span class="line">query.lastProgress();    &#x2F;&#x2F; the most recent progress update of this streaming query</span><br></pre></td></tr></table></figure>

<p>您可以在单个 SparkSession 中启动任意数量的查询.<br>它们都将同时运行,共享集群资源.<br>您可以使用sparkSession.streams()来获取可用于管理当前活动查询的StreamingQueryManager (Scala/Java/Python文档).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val spark: SparkSession &#x3D; ...</span><br><span class="line"></span><br><span class="line">spark.streams.active    &#x2F;&#x2F; get the list of currently active streaming queries</span><br><span class="line"></span><br><span class="line">spark.streams.get(id)   &#x2F;&#x2F; get a query object by its unique id</span><br><span class="line"></span><br><span class="line">spark.streams.awaitAnyTermination()   &#x2F;&#x2F; block until any one of them terminates</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">SparkSession spark &#x3D; ...</span><br><span class="line"></span><br><span class="line">spark.streams().active();    &#x2F;&#x2F; get the list of currently active streaming queries</span><br><span class="line"></span><br><span class="line">spark.streams().get(id);   &#x2F;&#x2F; get a query object by its unique id</span><br><span class="line"></span><br><span class="line">spark.streams().awaitAnyTermination();   &#x2F;&#x2F; block until any one of them terminates</span><br></pre></td></tr></table></figure>

<h3 id="监控流查询"><a href="#监控流查询" class="headerlink" title="监控流查询"></a>监控流查询</h3><p>有多种方法可以监控活动流查询.<br>您可以使用 Spark 的 Dropwizard Metrics 支持将指标推送到外部系统,或者以编程方式访问它们.</p>
<h4 id="交互式读取指标"><a href="#交互式读取指标" class="headerlink" title="交互式读取指标"></a>交互式读取指标</h4><p>streamingQuery.lastProgress()您可以使用和直接获取活动查询的当前状态和指标 streamingQuery.status().<br>在Scala 和JavalastProgress()中返回一个StreamingQueryProgress对象, 在 Python 中返回一个具有相同字段的字典.<br>它包含有关流的最后一个触发器中取得的进度的所有信息 - 处理了哪些数据/处理速率是多少/延迟等.<br>还有 它返回最后几个进度的数组.<br>streamingQuery.recentProgress</p>
<p>此外,在Scala 和JavastreamingQuery.status()中返回一个StreamingQueryStatus对象, 在 Python 中返回一个具有相同字段的字典.<br>它提供有关查询立即执行的操作的信息 - 触发器是否处于活动状态,数据是否正在处理等.</p>
<p>这里有一些例子.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">val query: StreamingQuery &#x3D; ...</span><br><span class="line"></span><br><span class="line">println(query.lastProgress)</span><br><span class="line"></span><br><span class="line">&#x2F;* Will print something like the following.</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;id&quot; : &quot;ce011fdc-8762-4dcb-84eb-a77333e28109&quot;,</span><br><span class="line">  &quot;runId&quot; : &quot;88e2ff94-ede0-45a8-b687-6316fbef529a&quot;,</span><br><span class="line">  &quot;name&quot; : &quot;MyQuery&quot;,</span><br><span class="line">  &quot;timestamp&quot; : &quot;2016-12-14T18:45:24.873Z&quot;,</span><br><span class="line">  &quot;numInputRows&quot; : 10,</span><br><span class="line">  &quot;inputRowsPerSecond&quot; : 120.0,</span><br><span class="line">  &quot;processedRowsPerSecond&quot; : 200.0,</span><br><span class="line">  &quot;durationMs&quot; : &#123;</span><br><span class="line">    &quot;triggerExecution&quot; : 3,</span><br><span class="line">    &quot;getOffset&quot; : 2</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;eventTime&quot; : &#123;</span><br><span class="line">    &quot;watermark&quot; : &quot;2016-12-14T18:45:24.873Z&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;stateOperators&quot; : [ ],</span><br><span class="line">  &quot;sources&quot; : [ &#123;</span><br><span class="line">    &quot;description&quot; : &quot;KafkaSource[Subscribe[topic-0]]&quot;,</span><br><span class="line">    &quot;startOffset&quot; : &#123;</span><br><span class="line">      &quot;topic-0&quot; : &#123;</span><br><span class="line">        &quot;2&quot; : 0,</span><br><span class="line">        &quot;4&quot; : 1,</span><br><span class="line">        &quot;1&quot; : 1,</span><br><span class="line">        &quot;3&quot; : 1,</span><br><span class="line">        &quot;0&quot; : 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;endOffset&quot; : &#123;</span><br><span class="line">      &quot;topic-0&quot; : &#123;</span><br><span class="line">        &quot;2&quot; : 0,</span><br><span class="line">        &quot;4&quot; : 115,</span><br><span class="line">        &quot;1&quot; : 134,</span><br><span class="line">        &quot;3&quot; : 21,</span><br><span class="line">        &quot;0&quot; : 534</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;numInputRows&quot; : 10,</span><br><span class="line">    &quot;inputRowsPerSecond&quot; : 120.0,</span><br><span class="line">    &quot;processedRowsPerSecond&quot; : 200.0</span><br><span class="line">  &#125; ],</span><br><span class="line">  &quot;sink&quot; : &#123;</span><br><span class="line">    &quot;description&quot; : &quot;MemorySink&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">*&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">println(query.status)</span><br><span class="line"></span><br><span class="line">&#x2F;*  Will print something like the following.</span><br><span class="line">&#123;</span><br><span class="line">  &quot;message&quot; : &quot;Waiting for data to arrive&quot;,</span><br><span class="line">  &quot;isDataAvailable&quot; : false,</span><br><span class="line">  &quot;isTriggerActive&quot; : false</span><br><span class="line">&#125;</span><br><span class="line">*&#x2F;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">StreamingQuery query &#x3D; ...</span><br><span class="line"></span><br><span class="line">System.out.println(query.lastProgress());</span><br><span class="line">&#x2F;* Will print something like the following.</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;id&quot; : &quot;ce011fdc-8762-4dcb-84eb-a77333e28109&quot;,</span><br><span class="line">  &quot;runId&quot; : &quot;88e2ff94-ede0-45a8-b687-6316fbef529a&quot;,</span><br><span class="line">  &quot;name&quot; : &quot;MyQuery&quot;,</span><br><span class="line">  &quot;timestamp&quot; : &quot;2016-12-14T18:45:24.873Z&quot;,</span><br><span class="line">  &quot;numInputRows&quot; : 10,</span><br><span class="line">  &quot;inputRowsPerSecond&quot; : 120.0,</span><br><span class="line">  &quot;processedRowsPerSecond&quot; : 200.0,</span><br><span class="line">  &quot;durationMs&quot; : &#123;</span><br><span class="line">    &quot;triggerExecution&quot; : 3,</span><br><span class="line">    &quot;getOffset&quot; : 2</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;eventTime&quot; : &#123;</span><br><span class="line">    &quot;watermark&quot; : &quot;2016-12-14T18:45:24.873Z&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;stateOperators&quot; : [ ],</span><br><span class="line">  &quot;sources&quot; : [ &#123;</span><br><span class="line">    &quot;description&quot; : &quot;KafkaSource[Subscribe[topic-0]]&quot;,</span><br><span class="line">    &quot;startOffset&quot; : &#123;</span><br><span class="line">      &quot;topic-0&quot; : &#123;</span><br><span class="line">        &quot;2&quot; : 0,</span><br><span class="line">        &quot;4&quot; : 1,</span><br><span class="line">        &quot;1&quot; : 1,</span><br><span class="line">        &quot;3&quot; : 1,</span><br><span class="line">        &quot;0&quot; : 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;endOffset&quot; : &#123;</span><br><span class="line">      &quot;topic-0&quot; : &#123;</span><br><span class="line">        &quot;2&quot; : 0,</span><br><span class="line">        &quot;4&quot; : 115,</span><br><span class="line">        &quot;1&quot; : 134,</span><br><span class="line">        &quot;3&quot; : 21,</span><br><span class="line">        &quot;0&quot; : 534</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;numInputRows&quot; : 10,</span><br><span class="line">    &quot;inputRowsPerSecond&quot; : 120.0,</span><br><span class="line">    &quot;processedRowsPerSecond&quot; : 200.0</span><br><span class="line">  &#125; ],</span><br><span class="line">  &quot;sink&quot; : &#123;</span><br><span class="line">    &quot;description&quot; : &quot;MemorySink&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">*&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">System.out.println(query.status());</span><br><span class="line">&#x2F;*  Will print something like the following.</span><br><span class="line">&#123;</span><br><span class="line">  &quot;message&quot; : &quot;Waiting for data to arrive&quot;,</span><br><span class="line">  &quot;isDataAvailable&quot; : false,</span><br><span class="line">  &quot;isTriggerActive&quot; : false</span><br><span class="line">&#125;</span><br><span class="line">*&#x2F;</span><br></pre></td></tr></table></figure>

<h4 id="使用异步-API-以编程方式报告指标"><a href="#使用异步-API-以编程方式报告指标" class="headerlink" title="使用异步 API 以编程方式报告指标"></a>使用异步 API 以编程方式报告指标</h4><p>您还可以 SparkSession通过附加StreamingQueryListener ( Scala/Java文档) 异步监视与 a 关联的所有查询.<br>使用 附加自定义StreamingQueryListener对象后 sparkSession.streams.attachListener(),您将在查询开始和停止以及活动查询取得进展时获得回调.<br>这是一个例子,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val spark: SparkSession &#x3D; ...</span><br><span class="line"></span><br><span class="line">spark.streams.addListener(new StreamingQueryListener() &#123;</span><br><span class="line">    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit &#x3D; &#123;</span><br><span class="line">        println(&quot;Query started: &quot; + queryStarted.id)</span><br><span class="line">    &#125;</span><br><span class="line">    override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit &#x3D; &#123;</span><br><span class="line">        println(&quot;Query terminated: &quot; + queryTerminated.id)</span><br><span class="line">    &#125;</span><br><span class="line">    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit &#x3D; &#123;</span><br><span class="line">        println(&quot;Query made progress: &quot; + queryProgress.progress)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">SparkSession spark &#x3D; ...</span><br><span class="line"></span><br><span class="line">spark.streams().addListener(new StreamingQueryListener() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void onQueryStarted(QueryStartedEvent queryStarted) &#123;</span><br><span class="line">        System.out.println(&quot;Query started: &quot; + queryStarted.id());</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void onQueryTerminated(QueryTerminatedEvent queryTerminated) &#123;</span><br><span class="line">        System.out.println(&quot;Query terminated: &quot; + queryTerminated.id());</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void onQueryProgress(QueryProgressEvent queryProgress) &#123;</span><br><span class="line">        System.out.println(&quot;Query made progress: &quot; + queryProgress.progress());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h4 id="使用-Dropwizard-报告指标"><a href="#使用-Dropwizard-报告指标" class="headerlink" title="使用 Dropwizard 报告指标"></a>使用 Dropwizard 报告指标</h4><p>Spark 支持使用Dropwizard Library报告指标.<br>要同时报告结构化流查询的指标,您必须spark.sql.streaming.metricsEnabled在 SparkSession 中显式启用配置.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.conf.set(&quot;spark.sql.streaming.metricsEnabled&quot;, &quot;true&quot;)</span><br><span class="line">&#x2F;&#x2F; or</span><br><span class="line">spark.sql(&quot;SET spark.sql.streaming.metricsEnabled&#x3D;true&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">spark.conf().set(&quot;spark.sql.streaming.metricsEnabled&quot;, &quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F; or</span><br><span class="line">spark.sql(&quot;SET spark.sql.streaming.metricsEnabled&#x3D;true&quot;);</span><br></pre></td></tr></table></figure>

<p>启用此配置后,在 SparkSession 中启动的所有查询都将通过 Dropwizard 向已配置的任何接收器(例如 Ganglia/Graphite/JMX 等)报告指标.</p>
<h3 id="使用检查点从故障中恢复"><a href="#使用检查点从故障中恢复" class="headerlink" title="使用检查点从故障中恢复"></a>使用检查点从故障中恢复</h3><p>在发生故障或故意关闭的情况下,您可以恢复先前查询的先前进度和状态,并从中断处继续.<br>这是使用检查点和预写日志完成的.<br>您可以使用检查点位置配置查询,查询会将所有进度信息(即每个触发器中处理的偏移量范围)和运行聚合(例如快速示例中的字数)保存到检查点位置.<br>此检查点位置必须是 HDFS 兼容文件系统中的路径,并且可以在开始查询时设置为 DataStreamWriter 中的选项.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .option(&quot;checkpointLocation&quot;, &quot;path&#x2F;to&#x2F;HDFS&#x2F;dir&quot;)</span><br><span class="line">  .format(&quot;memory&quot;)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .option(&quot;checkpointLocation&quot;, &quot;path&#x2F;to&#x2F;HDFS&#x2F;dir&quot;)</span><br><span class="line">  .format(&quot;memory&quot;)</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure>

<h3 id="流式查询更改后的恢复语义"><a href="#流式查询更改后的恢复语义" class="headerlink" title="流式查询更改后的恢复语义"></a>流式查询更改后的恢复语义</h3><p>在从同一检查点位置重新启动之间允许流式查询中的哪些更改存在限制.<br>这里有几种更改是不允许的,或者更改的效果没有明确定义.<br>对于他们所有人:</p>
<ol>
<li>术语允许意味着您可以进行指定的更改,但其效果的语义是否明确定义取决于查询和更改.</li>
<li>术语&quot;不允许&quot;表示您不应执行指定的更改,因为重新启动的查询可能会因不可预测的错误而失败.<br>sdf表示使用 sparkSession.readStream 生成的流式DataFrames/Datasets.</li>
</ol>
<h4 id="变化的类型"><a href="#变化的类型" class="headerlink" title="变化的类型"></a>变化的类型</h4><p>输入源的数量或类型(即不同来源)的变化:这是不允许的.</p>
<p>输入源参数的更改:这是否被允许以及更改的语义是否明确定义取决于源和查询.<br>这里有一些例子.</p>
<p>允许添加/删除/修改速率限制:spark.readStream.format(&quot;kafka&quot;).option(&quot;subscribe&quot;, &quot;topic&quot;)到spark.readStream.format(&quot;kafka&quot;).option(&quot;subscribe&quot;, &quot;topic&quot;).option(&quot;maxOffsetsPerTrigger&quot;, ...)</p>
<p>spark.readStream.format(&quot;kafka&quot;).option(&quot;subscribe&quot;, &quot;topic&quot;)由于结果不可预测,通常不允许更改订阅的主题/文件:spark.readStream.format(&quot;kafka&quot;).option(&quot;subscribe&quot;, &quot;newTopic&quot;)</p>
<p>输出接收器类型的变化:允许在几个特定的接收器组合之间进行更改.<br>这需要根据具体情况进行验证.<br>这里有一些例子.</p>
<ol>
<li>允许文件接收器到 Kafka 接收器.<br>Kafka 只会看到新数据.</li>
<li>不允许 Kafka 接收器到文件接收器.</li>
<li>允许将 Kafka sink 更改为 foreach,反之亦然.</li>
</ol>
<p>输出接收器参数的更改:这是否被允许以及更改的语义是否明确定义取决于接收器和查询.<br>这里有一些例子.</p>
<p>不允许更改文件接收器的输出目录:sdf.writeStream.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/somePath&quot;)到sdf.writeStream.format(&quot;parquet&quot;).option(&quot;path&quot;, &quot;/anotherPath&quot;)</p>
<p>允许更改输出主题:sdf.writeStream.format(&quot;kafka&quot;).option(&quot;topic&quot;, &quot;someTopic&quot;)到sdf.writeStream.format(&quot;kafka&quot;).option(&quot;topic&quot;, &quot;anotherTopic&quot;)</p>
<p>允许更改用户定义的 foreach 接收器(即ForeachWriter代码),但更改的语义取决于代码.</p>
<p>投影/过滤器/类地图操作的变化:允许某些情况.<br>例如:<br>允许添加/删除过滤器:sdf.selectExpr(&quot;a&quot;)到sdf.where(...).selectExpr(&quot;a&quot;).filter(...).</p>
<p>允许更改具有相同输出模式的投影:sdf.selectExpr(&quot;stringColumn AS json&quot;).writeStream到sdf.selectExpr(&quot;anotherStringColumn AS json&quot;).writeStream</p>
<p>有条件地允许更改具有不同输出模式的投影:仅当输出接收器允许模式从 to 更改时才sdf.selectExpr(&quot;a&quot;).writeStream允许to .<br>sdf.selectExpr(&quot;b&quot;).writeStream&quot;a&quot;&quot;b&quot;</p>
<p>有状态操作的变化:流式查询中的一些操作需要维护状态数据,以便持续更新结果.<br>Structured Streaming 自动将状态数据检查点到容错存储(例如,HDFS/AWS S3/Azure Blob 存储)并在重启后恢复它.<br>但是,这假设状态数据的模式在重启后保持不变.<br>这意味着 在重新启动之间不允许对流式查询的有状态操作进行任何更改(即添加/删除或模式修改).<br>以下是有状态操作的列表,其架构不应在重新启动之间更改以确保状态恢复:</p>
<p>流式聚合:例如,sdf.groupBy(&quot;a&quot;).agg(...). 不允许对分组键或聚合的数量或类型进行任何更改.</p>
<p>流式重复数据删除:例如,sdf.dropDuplicates(&quot;a&quot;). 不允许对分组键或聚合的数量或类型进行任何更改.</p>
<p>Stream-stream join:例如,sdf1.join(sdf2, ...)(即两个输入都是用 生成的sparkSession.readStream).<br>不允许更改架构或等连接列.<br>不允许更改连接类型(外部或内部).<br>连接条件的其他变化是不明确的.</p>
<p>任意状态操作:例如,sdf.groupByKey(...).mapGroupsWithState(...)或sdf.groupByKey(...).flatMapGroupsWithState(...).<br>不允许对用户定义状态的模式和超时类型进行任何更改.<br>允许在用户定义的状态映射函数内进行任何更改,但更改的语义效果取决于用户定义的逻辑.<br>如果您真的想支持状态模式更改,那么您可以使用支持模式迁移的编码/解码方案将复杂的状态数据结构显式编码/解码为字节.<br>例如,如果您将状态保存为 Avro 编码字节,那么您可以在查询重新启动之间自由更改 Avro 状态模式,因为二进制状态将始终成功恢复.</p>
<h2 id="连续加工"><a href="#连续加工" class="headerlink" title="连续加工"></a>连续加工</h2><p><code>[实验]</code><br>连续处理是 Spark 2.3 中引入的一种新的实验性流式执行模式,可实现低(~1 毫秒)端到端延迟,并提供至少一次容错保证.<br>将此与默认的微批处理引擎进行比较,后者可以实现恰好一次保证,但最多可实现约 100 毫秒的延迟.<br>对于某些类型的查询(下面讨论),您可以选择以何种模式执行它们而无需修改应用程序逻辑(即无需更改 DataFrame/Dataset 操作).</p>
<p>要以连续处理模式运行支持的查询,您需要做的就是指定一个连续触发器,并将所需的检查点间隔作为参数.<br>例如,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.streaming.Trigger</span><br><span class="line"></span><br><span class="line">spark</span><br><span class="line">  .readStream</span><br><span class="line">  .format(&quot;kafka&quot;)</span><br><span class="line">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">  .option(&quot;subscribe&quot;, &quot;topic1&quot;)</span><br><span class="line">  .load()</span><br><span class="line">  .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(&quot;kafka&quot;)</span><br><span class="line">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">  .option(&quot;topic&quot;, &quot;topic1&quot;)</span><br><span class="line">  .trigger(Trigger.Continuous(&quot;1 second&quot;))  &#x2F;&#x2F; only change in query</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;java</span><br><span class="line">import org.apache.spark.sql.streaming.Trigger;</span><br><span class="line"></span><br><span class="line">spark</span><br><span class="line">  .readStream</span><br><span class="line">  .format(&quot;kafka&quot;)</span><br><span class="line">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">  .option(&quot;subscribe&quot;, &quot;topic1&quot;)</span><br><span class="line">  .load()</span><br><span class="line">  .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(&quot;kafka&quot;)</span><br><span class="line">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">  .option(&quot;topic&quot;, &quot;topic1&quot;)</span><br><span class="line">  .trigger(Trigger.Continuous(&quot;1 second&quot;))  &#x2F;&#x2F; only change in query</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure>

<p>1秒的检查点间隔意味着连续处理引擎将每秒记录查询的进度.<br>生成的检查点采用与微批处理引擎兼容的格式,因此可以使用任何触发器重新启动任何查询.<br>例如,以微批处理模式启动的受支持查询可以以连续模式重新启动,反之亦然.<br>请注意,无论何时切换到连续模式,您都将获得至少一次容错保证.</p>
<h3 id="支持的查询"><a href="#支持的查询" class="headerlink" title="支持的查询"></a>支持的查询</h3><p>从 Spark 2.4 开始,持续处理模式仅支持以下类型的查询.</p>
<p>操作:在连续模式下仅支持类似地图的 Dataset/DataFrame 操作,即仅投影(select, map, flatMap,mapPartitions等)和选择(where,filter等).</p>
<p>支持所有 SQL 函数,除了聚合函数(因为还不支持聚合)current_timestamp()和current_date()(使用时间的确定性计算具有挑战性).</p>
<p>资料来源:<br>Kafka 源:支持所有选项.<br>评分来源:适合测试.<br>只有连续模式支持的选项是numPartitions和rowsPerSecond.</p>
<p>水槽:<br>Kafka sink:支持所有选项.</p>
<p>Memory sink:有利于调试.</p>
<p>控制台接收器:有利于调试.<br>支持所有选项.<br>请注意,控制台将打印您在连续触发器中指定的每个检查点间隔.</p>
<p>有关它们的更多详细信息,请参阅输入源和输出接收器部分.<br>虽然控制台接收器非常适合测试,但使用 Kafka 作为源和接收器可以最好地观察到端到端的低延迟处理,因为这允许引擎处理数据并使结果在输出主题中可用输入主题中可用的输入数据的毫秒数.</p>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>持续处理引擎启动多个长时间运行的任务,这些任务持续从源读取数据/处理数据并持续写入接收器.<br>查询所需的任务数取决于查询可以从源中并行读取多少个分区.<br>因此,在开始连续处理查询之前,您必须确保集群中有足够的核心来并行处理所有任务.<br>例如,如果您正在读取具有 10 个分区的 Kafka 主题,则集群必须至少有 10 个核心才能使查询取得进展.</p>
<p>停止连续处理流可能会产生虚假的任务终止警告.<br>这些可以安全地忽略.</p>
<p>目前没有自动重试失败的任务.<br>任何失败都将导致查询停止,需要从检查点手动重新启动.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/22/spark%20rdd%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/" rel="prev" title="spark rdd编程指南">
                  <i class="fa fa-chevron-left"></i> spark rdd编程指南
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/11/23/spark-kafka%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97/" rel="next" title="spark-kafka集成指南">
                  spark-kafka集成指南 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">maoeryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  





</body>
</html>
