<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maoeryu.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","width":200,"display":"post","padding":7,"offset":5},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="table th:first-of-type {   width: 40%; } table th:nth-of-type(2) {   width: 60%; }    Spark SQL 是一个用于结构化数据处理的 Spark 模块.与基本的 Spark RDD API 不同,Spark SQL 提供的接口为 Spark 提供了有关数据结构和正在执行的计算的更多信息.在内部,Spark SQ">
<meta property="og:type" content="article">
<meta property="og:title" content="spark sql指南">
<meta property="og:url" content="https://maoeryu.github.io/2022/11/24/spark%20sql%E6%8C%87%E5%8D%97/index.html">
<meta property="og:site_name" content="FlyingPig">
<meta property="og:description" content="table th:first-of-type {   width: 40%; } table th:nth-of-type(2) {   width: 60%; }    Spark SQL 是一个用于结构化数据处理的 Spark 模块.与基本的 Spark RDD API 不同,Spark SQL 提供的接口为 Spark 提供了有关数据结构和正在执行的计算的更多信息.在内部,Spark SQ">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1366.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1367.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1368.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1369.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1370.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1371.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1372.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1373.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1374.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1375.png">
<meta property="og:image" content="https://maoeryu.github.io/images/fly1376.png">
<meta property="article:published_time" content="2022-11-23T16:00:00.000Z">
<meta property="article:modified_time" content="2022-12-02T06:15:03.827Z">
<meta property="article:author" content="maoeryu">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maoeryu.github.io/images/fly1366.png">


<link rel="canonical" href="https://maoeryu.github.io/2022/11/24/spark%20sql%E6%8C%87%E5%8D%97/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>spark sql指南 | FlyingPig</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlyingPig</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%A5%E9%97%A8"><span class="nav-number">1.</span> <span class="nav-text">入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSession"><span class="nav-number">1.1.</span> <span class="nav-text">SparkSession</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BADataFrames"><span class="nav-number">1.2.</span> <span class="nav-text">创建DataFrames</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A0%E7%B1%BB%E5%9E%8BDataset%E6%93%8D%E4%BD%9C-%E5%8F%88%E5%90%8D-DataFrame-%E6%93%8D%E4%BD%9C"><span class="nav-number">1.3.</span> <span class="nav-text">无类型Dataset操作(又名 DataFrame 操作)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A5%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E8%BF%90%E8%A1%8C-SQL-%E6%9F%A5%E8%AF%A2"><span class="nav-number">1.4.</span> <span class="nav-text">以编程方式运行 SQL 查询</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE"><span class="nav-number">1.5.</span> <span class="nav-text">全局临时视图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BADataset"><span class="nav-number">1.6.</span> <span class="nav-text">创建Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8E-RDD-%E4%BA%92%E6%93%8D%E4%BD%9C"><span class="nav-number">1.7.</span> <span class="nav-text">与 RDD 互操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%8F%8D%E5%B0%84%E6%8E%A8%E6%96%AD%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.7.1.</span> <span class="nav-text">使用反射推断模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A5%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E6%8C%87%E5%AE%9A%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.7.2.</span> <span class="nav-text">以编程方式指定模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%9A%E5%90%88"><span class="nav-number">1.8.</span> <span class="nav-text">聚合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%94%A8%E6%88%B7%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">1.8.1.</span> <span class="nav-text">无类型的用户定义聚合函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8%E7%9A%84%E7%94%A8%E6%88%B7%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">1.8.2.</span> <span class="nav-text">类型安全的用户定义聚合函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.</span> <span class="nav-text">数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E5%8A%A0%E8%BD%BD-%E4%BF%9D%E5%AD%98%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">通用加载&#x2F;保存函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E6%8C%87%E5%AE%9A%E9%80%89%E9%A1%B9"><span class="nav-number">2.1.1.</span> <span class="nav-text">手动指定选项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E5%AF%B9%E6%96%87%E4%BB%B6%E8%BF%90%E8%A1%8C-SQL"><span class="nav-number">2.1.2.</span> <span class="nav-text">直接对文件运行 SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%BC%8F"><span class="nav-number">2.1.3.</span> <span class="nav-text">保存模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E5%88%B0%E6%8C%81%E4%B9%85%E8%A1%A8"><span class="nav-number">2.1.4.</span> <span class="nav-text">保存到持久表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%A1%B6-%E6%8E%92%E5%BA%8F%E5%92%8C%E5%88%86%E5%8C%BA"><span class="nav-number">2.1.5.</span> <span class="nav-text">分桶&#x2F;排序和分区</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parquet-Files"><span class="nav-number">2.2.</span> <span class="nav-text">Parquet Files</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A5%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="nav-number">2.2.1.</span> <span class="nav-text">以编程方式加载数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E5%8F%91%E7%8E%B0"><span class="nav-number">2.2.2.</span> <span class="nav-text">分区发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E5%90%88%E5%B9%B6"><span class="nav-number">2.2.3.</span> <span class="nav-text">架构合并</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Metastore-Parquet-%E8%A1%A8%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.2.4.</span> <span class="nav-text">Hive Metastore Parquet 表转换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hive-Parquet-%E6%9E%B6%E6%9E%84%E5%8D%8F%E8%B0%83"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">Hive&#x2F;Parquet 架构协调</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E5%88%B7%E6%96%B0"><span class="nav-number">2.2.4.2.</span> <span class="nav-text">元数据刷新</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">2.2.5.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ORC-Files"><span class="nav-number">2.3.</span> <span class="nav-text">ORC Files</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JSON%E6%96%87%E4%BB%B6"><span class="nav-number">2.4.</span> <span class="nav-text">JSON文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive-Tables"><span class="nav-number">2.5.</span> <span class="nav-text">Hive Tables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E5%AE%9A-Hive-%E8%A1%A8%E7%9A%84%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.5.1.</span> <span class="nav-text">指定 Hive 表的存储格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84-Hive-Metastore-%E4%BA%A4%E4%BA%92"><span class="nav-number">2.5.2.</span> <span class="nav-text">与不同版本的 Hive Metastore 交互</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JDBC-%E5%88%B0%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">2.6.</span> <span class="nav-text">JDBC 到其他数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Avro-Files"><span class="nav-number">2.7.</span> <span class="nav-text">Avro Files</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E4%B8%AD"><span class="nav-number">2.7.1.</span> <span class="nav-text">部署中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E5%8A%9F%E8%83%BD"><span class="nav-number">2.7.2.</span> <span class="nav-text">加载和保存功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#to-avro-from-avro"><span class="nav-number">2.7.3.</span> <span class="nav-text">to_avro()&#x2F;from_avro()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BA%90%E9%80%89%E9%A1%B9"><span class="nav-number">2.7.4.</span> <span class="nav-text">数据源选项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-1"><span class="nav-number">2.7.5.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E-Databricks-spark-avro-%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">2.7.6.</span> <span class="nav-text">与 Databricks spark-avro 的兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Avro-%E6%94%AF%E6%8C%81%E7%9A%84%E7%B1%BB%E5%9E%8B-gt-Spark-SQL-%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.7.7.</span> <span class="nav-text">Avro 支持的类型 -&gt; Spark SQL 转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL-gt-Avro-%E8%BD%AC%E6%8D%A2%E6%94%AF%E6%8C%81%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.7.8.</span> <span class="nav-text">Spark SQL -&gt; Avro 转换支持的类型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4"><span class="nav-number">2.8.</span> <span class="nav-text">故障排除</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98"><span class="nav-number">3.</span> <span class="nav-text">性能调优</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE"><span class="nav-number">3.1.</span> <span class="nav-text">在内存中缓存数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9"><span class="nav-number">3.2.</span> <span class="nav-text">其他配置选项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SQL-%E6%9F%A5%E8%AF%A2%E7%9A%84%E5%B9%BF%E6%92%AD%E6%8F%90%E7%A4%BA"><span class="nav-number">3.3.</span> <span class="nav-text">SQL 查询的广播提示</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-SQL-%E5%BC%95%E6%93%8E"><span class="nav-number">4.</span> <span class="nav-text">分布式 SQL 引擎</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C-Thrift-JDBC-ODBC-%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">4.1.</span> <span class="nav-text">运行 Thrift JDBC&#x2F;ODBC 服务器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C-Spark-SQL-CLI"><span class="nav-number">4.2.</span> <span class="nav-text">运行 Spark SQL CLI</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">maoeryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">220</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://maoeryu.github.io/2022/11/24/spark%20sql%E6%8C%87%E5%8D%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="maoeryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlyingPig">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark sql指南
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-11-24 00:00:00" itemprop="dateCreated datePublished" datetime="2022-11-24T00:00:00+08:00">2022-11-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-12-02 14:15:03" itemprop="dateModified" datetime="2022-12-02T14:15:03+08:00">2022-12-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8D%8F%E5%90%8C%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">协同框架</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <style>
table th:first-of-type {
  width: 40%;
}
table th:nth-of-type(2) {
  width: 60%;
}
</style>


<p>Spark SQL 是一个用于结构化数据处理的 Spark 模块.<br>与基本的 Spark RDD API 不同,Spark SQL 提供的接口为 Spark 提供了有关数据结构和正在执行的计算的更多信息.<br>在内部,Spark SQL 使用这些额外的信息来执行额外的优化.<br>有多种方法可以与 Spark SQL 交互,包括 SQL/Dataset API.<br>在计算结果时,使用相同的执行引擎,与您用来表达计算的 API/语言无关.<br>这种统一意味着开发人员可以轻松地在不同的 API 之间来回切换,以提供最自然的方式来表达给定的转换.</p>
<span id="more"></span>
<h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><p>Spark 中所有功能的入口点是SparkSession类.<br>要创建一个基本的SparkSession,只需使用SparkSession.builder():</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark SQL basic example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">SparkSession spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Java Spark SQL basic example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">  .getOrCreate();</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<p>SparkSessionin Spark 2.0 为 Hive 功能提供内置支持,包括使用 HiveQL 编写查询的能力/访问 Hive UDF 以及从 Hive 表读取数据的能力.<br>要使用这些功能,您不需要现有的 Hive 设置.</p>
<h2 id="创建DataFrames"><a href="#创建DataFrames" class="headerlink" title="创建DataFrames"></a>创建DataFrames</h2><p>使用SparkSession,应用程序可以从现有的RDD/Hive 表或Spark 数据源创建 DataFrame.<br>例如,以下代码基于 JSON 文件的内容创建了一个 DataFrame:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="无类型Dataset操作-又名-DataFrame-操作"><a href="#无类型Dataset操作-又名-DataFrame-操作" class="headerlink" title="无类型Dataset操作(又名 DataFrame 操作)"></a>无类型Dataset操作(又名 DataFrame 操作)</h2><p>DataFrames 为Scala/Java/Python和R中的结构化数据操作提供了一种特定领域的语言.</p>
<p>上面说过,在Spark 2.0中,DataFrames只是Scala和Java API中的Dataset of Rows.<br>这些操作也称为&quot;无类型转换&quot;,与强类型 Scala/Java Dataset附带的&quot;类型转换&quot;形成对比.</p>
<p>这里我们包括一些使用Dataset进行结构化数据处理的基本示例:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This import is needed to use the $-notation</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the &quot;name&quot; column</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>).show()</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select($<span class="string">&quot;name&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">&quot;age&quot;</span>).count().show()</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="comment">// col(&quot;...&quot;) is preferable to df.col(&quot;...&quot;)</span></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema();</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the &quot;name&quot; column</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>).show();</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(col(<span class="string">&quot;name&quot;</span>), col(<span class="string">&quot;age&quot;</span>).plus(<span class="number">1</span>)).show();</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter(col(<span class="string">&quot;age&quot;</span>).gt(<span class="number">21</span>)).show();</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">&quot;age&quot;</span>).count().show();</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<p>有关可对Dataset执行的操作类型的完整列表,请参阅API 文档.</p>
<p>除了简单的列引用和表达式,Datasets 还拥有丰富的函数库,包括字符串操作/日期算术/常用数学运算等.<br>完整列表可在DataFrame 函数参考中找到.</p>
<h2 id="以编程方式运行-SQL-查询"><a href="#以编程方式运行-SQL-查询" class="headerlink" title="以编程方式运行 SQL 查询"></a>以编程方式运行 SQL 查询</h2><p>SparkSession上的sql函数使应用程序能够以编程方式运行 SQL 查询并将结果作为DataFrame/<code>Dataset&lt;Row&gt;</code>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">&quot;SELECT * FROM people&quot;</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">&quot;SELECT * FROM people&quot;</span>);</span><br><span class="line">sqlDF.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="全局临时视图"><a href="#全局临时视图" class="headerlink" title="全局临时视图"></a>全局临时视图</h2><p>Spark SQL 中的临时视图是会话范围的,如果创建它的会话终止,它将消失.<br>如果您希望拥有一个在所有会话之间共享的临时视图并保持活动状态直到 Spark 应用程序终止,您可以创建一个全局临时视图.<br>全局临时视图绑定到系统保留的数据库global_temp,我们必须使用限定名称来引用它,例如<code>SELECT * FROM global_temp.view1</code>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">&quot;people&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">GLOBAL</span> TEMPORARY <span class="keyword">VIEW</span> temp_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> a <span class="operator">+</span> <span class="number">1</span>, b <span class="operator">*</span> <span class="number">2</span> <span class="keyword">FROM</span> tbl</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> global_temp.temp_view</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="创建Dataset"><a href="#创建Dataset" class="headerlink" title="创建Dataset"></a>创建Dataset</h2><p>Dataset与 RDD 类似,但是,它们不使用 Java 序列化或 Kryo,而是使用专门的编码器来序列化对象以进行处理或通过网络传输.<br>虽然编码器和标准序列化都负责将对象转换为字节,但编码器是动态生成的代码,并使用一种格式,允许 Spark 执行许多操作,如过滤/排序和散列,而无需将字节反序列化回对象.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// Encoders are created for case classes</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |name|age|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |Andy| 32|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">&quot;examples/src/main/resources/people.json&quot;</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> age;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an instance of a Bean class</span></span><br><span class="line">Person person = <span class="keyword">new</span> Person();</span><br><span class="line">person.setName(<span class="string">&quot;Andy&quot;</span>);</span><br><span class="line">person.setAge(<span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders are created for Java beans</span></span><br><span class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</span><br><span class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</span><br><span class="line">  Collections.singletonList(person),</span><br><span class="line">  personEncoder</span><br><span class="line">);</span><br><span class="line">javaBeanDS.show();</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 32|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></span><br><span class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</span><br><span class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</span><br><span class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</span><br><span class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</span><br><span class="line">    integerEncoder);</span><br><span class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></span><br><span class="line">String path = <span class="string">&quot;examples/src/main/resources/people.json&quot;</span>;</span><br><span class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</span><br><span class="line">peopleDS.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="与-RDD-互操作"><a href="#与-RDD-互操作" class="headerlink" title="与 RDD 互操作"></a>与 RDD 互操作</h2><p>Spark SQL 支持两种不同的方法将现有的 RDD 转换为Dataset.<br>第一种方法使用反射来推断包含特定类型对象的 RDD 的模式.<br>这种基于反射的方法可以生成更简洁的代码,并且当您在编写 Spark 应用程序时已经知道架构时效果很好.</p>
<p>创建Dataset的第二种方法是通过一个编程接口,它允许您构建一个模式,然后将其应用于现有的 RDD.<br>虽然此方法更冗长,但它允许您在列及其类型直到运行时才知道的情况下构造Dataset.</p>
<h3 id="使用反射推断模式"><a href="#使用反射推断模式" class="headerlink" title="使用反射推断模式"></a>使用反射推断模式</h3><p>Spark SQL 的 Scala 接口支持自动将包含案例类的 RDD 转换为 DataFrame.<br>案例类定义表的架构.<br>使用反射读取案例类的参数名称,并成为列的名称.<br>案例类也可以嵌套或包含复杂类型,例如Seqs 或Arrays.<br>这个 RDD 可以隐式转换为 DataFrame,然后注册为表.<br>表可以在后续的 SQL 语句中使用.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// For implicit conversions from RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.sparkContext</span><br><span class="line">  .textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">  .map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">  .map(attributes =&gt; <span class="type">Person</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">  .toDF()</span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by Spark</span></span><br><span class="line"><span class="keyword">val</span> teenagersDF = spark.sql(<span class="string">&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">&quot;Name: &quot;</span> + teenager(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">&quot;Name: &quot;</span> + teenager.getAs[<span class="type">String</span>](<span class="string">&quot;name&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="type">Encoders</span>.kryo[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>]]</span><br><span class="line"><span class="comment">// Primitive types and case classes can be also defined as</span></span><br><span class="line"><span class="comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>))).collect()</span><br><span class="line"><span class="comment">// Array(Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19))</span></span><br></pre></td></tr></table></figure>

<p>Spark SQL 支持自动将 JavaBeans的 RDD 转换为 DataFrame.使用BeanInfo反射获得的 定义了表的模式.目前,Spark SQL 不支持包含Map字段的 JavaBeans.尽管支持嵌套的 JavaBeans 和List/或Array 字段.您可以创建一个 JavaBean,方法是创建一个实现 Serializable 并为其所有字段提供 getter/setter 的类.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></span><br><span class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</span><br><span class="line">  .textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">  .javaRDD()</span><br><span class="line">  .map(line -&gt; &#123;</span><br><span class="line">    String[] parts = line.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">    Person person = <span class="keyword">new</span> Person();</span><br><span class="line">    person.setName(parts[<span class="number">0</span>]);</span><br><span class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</span><br><span class="line">    <span class="keyword">return</span> person;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></span><br><span class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</span><br><span class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">&quot;Name: &quot;</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    stringEncoder);</span><br><span class="line">teenagerNamesByIndexDF.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">&quot;Name: &quot;</span> + row.&lt;String&gt;getAs(<span class="string">&quot;name&quot;</span>),</span><br><span class="line">    stringEncoder);</span><br><span class="line">teenagerNamesByFieldDF.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="以编程方式指定模式"><a href="#以编程方式指定模式" class="headerlink" title="以编程方式指定模式"></a>以编程方式指定模式</h3><p>当无法提前定义案例类时(例如,记录的结构被编码为字符串,或者文本Dataset将被解析并且字段将针对不同的用户进行不同的投影),DataFrame可以通过三个步骤以编程方式创建.</p>
<ol>
<li>从原始 RDD创建Rows 的 RDD.</li>
<li>在第 1 步创建的 RDD 中创建由匹配 Rows 结构StructType的模式表示.</li>
<li>通过SparkSession提供的createDataFrame方法将模式应用于Rows的 RDD.</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = spark.sparkContext.textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string</span></span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">&quot;name age&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line"><span class="keyword">val</span> fields = schemaString.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">  .map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = peopleRDD</span><br><span class="line">  .map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">  .map(attributes =&gt; <span class="type">Row</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line"><span class="keyword">val</span> results = spark.sql(<span class="string">&quot;SELECT name FROM people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">results.map(attributes =&gt; <span class="string">&quot;Name: &quot;</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure>

<p>当无法提前定义 JavaBean 类时(例如,记录的结构被编码为字符串,或者文本数据集将被解析并且字段将针对不同的用户进行不同的投影),可以通过三个步骤以编程方式创建<code>Dataset&lt;Row&gt;</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</span><br><span class="line">  .textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>, <span class="number">1</span>)</span><br><span class="line">  .toJavaRDD();</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string</span></span><br><span class="line">String schemaString = <span class="string">&quot;name age&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">&quot; &quot;</span>)) &#123;</span><br><span class="line">  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</span><br><span class="line">  fields.add(field);</span><br><span class="line">&#125;</span><br><span class="line">StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</span><br><span class="line">  String[] attributes = record.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">&quot;SELECT name FROM people&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">Dataset&lt;String&gt; namesDS = results.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">&quot;Name: &quot;</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    Encoders.STRING());</span><br><span class="line">namesDS.show();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h2><p>内置的 DataFrames 函数提供常见的聚合,例如count(), countDistinct(), avg(), max(),min()等.<br>虽然这些函数是为 DataFrames 设计的,但 Spark SQL 在 Scala和 Java中也为其中一些函数提供了类型安全版本,以处理强类型Dataset.<br>此外,用户不限于预定义的聚合函数,可以创建自己的聚合函数.</p>
<h3 id="无类型的用户定义聚合函数"><a href="#无类型的用户定义聚合函数" class="headerlink" title="无类型的用户定义聚合函数"></a>无类型的用户定义聚合函数</h3><p>用户必须扩展UserDefinedAggregateFunction 抽象类来实现自定义无类型聚合函数.<br>例如,用户定义的平均值可能如下所示:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">MutableAggregationBuffer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedAggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Data types of input arguments of this aggregate function</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;inputColumn&quot;</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  <span class="comment">// Data types of values in the aggregation buffer</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// The data type of the returned value</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line">  <span class="comment">// Whether this function always returns the same output on the identical input</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span></span><br><span class="line">  <span class="comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span></span><br><span class="line">  <span class="comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span></span><br><span class="line">  <span class="comment">// immutable.</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Calculates the final result</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the function to access it</span></span><br><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>, <span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/employees.json&quot;</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;employees&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;</span>)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.MutableAggregationBuffer;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.UserDefinedAggregateFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataType;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> StructType inputSchema;</span><br><span class="line">  <span class="keyword">private</span> StructType bufferSchema;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyAverage</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;StructField&gt; inputFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    inputFields.add(DataTypes.createStructField(<span class="string">&quot;inputColumn&quot;</span>, DataTypes.LongType, <span class="keyword">true</span>));</span><br><span class="line">    inputSchema = DataTypes.createStructType(inputFields);</span><br><span class="line"></span><br><span class="line">    List&lt;StructField&gt; bufferFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">&quot;sum&quot;</span>, DataTypes.LongType, <span class="keyword">true</span>));</span><br><span class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">&quot;count&quot;</span>, DataTypes.LongType, <span class="keyword">true</span>));</span><br><span class="line">    bufferSchema = DataTypes.createStructType(bufferFields);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Data types of input arguments of this aggregate function</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">inputSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> inputSchema;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Data types of values in the aggregation buffer</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">bufferSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> bufferSchema;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// The data type of the returned value</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> DataType <span class="title">dataType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> DataTypes.DoubleType;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Whether this function always returns the same output on the identical input</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deterministic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span></span><br><span class="line">  <span class="comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span></span><br><span class="line">  <span class="comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span></span><br><span class="line">  <span class="comment">// immutable.</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MutableAggregationBuffer buffer)</span> </span>&#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>, <span class="number">0L</span>);</span><br><span class="line">    buffer.update(<span class="number">1</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(MutableAggregationBuffer buffer, Row input)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      <span class="keyword">long</span> updatedSum = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>);</span><br><span class="line">      <span class="keyword">long</span> updatedCount = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">      buffer.update(<span class="number">0</span>, updatedSum);</span><br><span class="line">      buffer.update(<span class="number">1</span>, updatedCount);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MutableAggregationBuffer buffer1, Row buffer2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> mergedSum = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> mergedCount = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>);</span><br><span class="line">    buffer1.update(<span class="number">0</span>, mergedSum);</span><br><span class="line">    buffer1.update(<span class="number">1</span>, mergedCount);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Calculates the final result</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">evaluate</span><span class="params">(Row buffer)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) buffer.getLong(<span class="number">0</span>)) / buffer.getLong(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the function to access it</span></span><br><span class="line">spark.udf().register(<span class="string">&quot;myAverage&quot;</span>, <span class="keyword">new</span> MyAverage());</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">&quot;examples/src/main/resources/employees.json&quot;</span>);</span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;employees&quot;</span>);</span><br><span class="line">df.show();</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; result = spark.sql(<span class="string">&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;</span>);</span><br><span class="line">result.show();</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala&quot;中找到完整的示例代码.</p>
<h3 id="类型安全的用户定义聚合函数"><a href="#类型安全的用户定义聚合函数" class="headerlink" title="类型安全的用户定义聚合函数"></a>类型安全的用户定义聚合函数</h3><p>强类型Dataset的用户定义聚合围绕Aggregator抽象类.<br>例如,类型安全的用户定义平均值可能如下所示:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line">  <span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buffer: <span class="type">Average</span>, employee: <span class="type">Employee</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    buffer.sum += employee.salary</span><br><span class="line">    buffer.count += <span class="number">1</span></span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Merge two intermediate values</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Transform the output of the reduction</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.toDouble / reduction.count</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">&quot;examples/src/main/resources/employees.json&quot;</span>).as[<span class="type">Employee</span>]</span><br><span class="line">ds.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></span><br><span class="line"><span class="keyword">val</span> averageSalary = <span class="type">MyAverage</span>.toColumn.name(<span class="string">&quot;average_salary&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.TypedColumn;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> salary;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Constructors, getters, setters...</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Average</span> <span class="keyword">implements</span> <span class="title">Serializable</span>  </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sum;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Constructors, getters, setters...</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>&lt;<span class="title">Employee</span>, <span class="title">Average</span>, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">zero</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Average(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line">  <span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">reduce</span><span class="params">(Average buffer, Employee employee)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> newSum = buffer.getSum() + employee.getSalary();</span><br><span class="line">    <span class="keyword">long</span> newCount = buffer.getCount() + <span class="number">1</span>;</span><br><span class="line">    buffer.setSum(newSum);</span><br><span class="line">    buffer.setCount(newCount);</span><br><span class="line">    <span class="keyword">return</span> buffer;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Merge two intermediate values</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">merge</span><span class="params">(Average b1, Average b2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> mergedSum = b1.getSum() + b2.getSum();</span><br><span class="line">    <span class="keyword">long</span> mergedCount = b1.getCount() + b2.getCount();</span><br><span class="line">    b1.setSum(mergedSum);</span><br><span class="line">    b1.setCount(mergedCount);</span><br><span class="line">    <span class="keyword">return</span> b1;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Transform the output of the reduction</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">finish</span><span class="params">(Average reduction)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) reduction.getSum()) / reduction.getCount();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Average&gt; <span class="title">bufferEncoder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Encoders.bean(Average.class);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Double&gt; <span class="title">outputEncoder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Encoders.DOUBLE();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Encoder&lt;Employee&gt; employeeEncoder = Encoders.bean(Employee.class);</span><br><span class="line">String path = <span class="string">&quot;examples/src/main/resources/employees.json&quot;</span>;</span><br><span class="line">Dataset&lt;Employee&gt; ds = spark.read().json(path).as(employeeEncoder);</span><br><span class="line">ds.show();</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line">MyAverage myAverage = <span class="keyword">new</span> MyAverage();</span><br><span class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></span><br><span class="line">TypedColumn&lt;Employee, Double&gt; averageSalary = myAverage.toColumn().name(<span class="string">&quot;average_salary&quot;</span>);</span><br><span class="line">Dataset&lt;Double&gt; result = ds.select(averageSalary);</span><br><span class="line">result.show();</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala&quot;中找到完整的示例代码.</p>
<h1 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h1><p>Spark SQL 支持通过DataFrame 接口对多种数据源进行操作.DataFrame 可以使用关系转换进行操作,也可以用于创建临时视图.将 DataFrame 注册为临时视图允许您对其数据运行 SQL 查询.</p>
<h2 id="通用加载-保存函数"><a href="#通用加载-保存函数" class="headerlink" title="通用加载/保存函数"></a>通用加载/保存函数</h2><p>在最简单的形式中,默认数据源(parquet除非另有配置 <code>spark.sql.sources.default</code>)将用于所有操作.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> usersDF = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">Dataset&lt;Row&gt; usersDF = spark.read().load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>);</span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write().save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>您还可以手动指定将与要传递给数据源的任何额外选项一起使用的数据源.<br>数据源由它们的完全限定名称(即<code>org.apache.spark.sql.parquet</code>)指定,但对于内置源,您也可以使用它们的短名称(json/parquet/jdbc/orc/libsvm/csv/text).<br>从任何数据源类型加载的 DataFrame 都可以使用此语法转换为其他类型.</p>
<p>要加载 JSON 文件,您可以使用:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">peopleDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesAndAges.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDF =</span><br><span class="line">  spark.read().format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>);</span><br><span class="line">peopleDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write().format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesAndAges.parquet&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<p>要加载 CSV 文件,您可以使用:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDFCsv = spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDFCsv = spark.read().format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<p>额外的选项也在写操作期间使用.<br>例如,您可以控制 ORC 数据源的布隆过滤器和字典编码.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">usersDF.write.format(<span class="string">&quot;orc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;orc.bloom.filter.columns&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;orc.dictionary.key.threshold&quot;</span>, <span class="string">&quot;1.0&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;users_with_options.orc&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">usersDF.write().format(<span class="string">&quot;orc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;orc.bloom.filter.columns&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;orc.dictionary.key.threshold&quot;</span>, <span class="string">&quot;1.0&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;users_with_options.orc&quot;</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> users_with_options (</span><br><span class="line">  name STRING,</span><br><span class="line">  favorite_color STRING,</span><br><span class="line">  favorite_numbers <span class="keyword">array</span><span class="operator">&lt;</span><span class="type">integer</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">USING</span> ORC</span><br><span class="line">OPTIONS (</span><br><span class="line">  orc.bloom.filter.columns <span class="string">&#x27;favorite_color&#x27;</span>,</span><br><span class="line">  orc.dictionary.key.threshold <span class="string">&#x27;1.0&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="直接对文件运行-SQL"><a href="#直接对文件运行-SQL" class="headerlink" title="直接对文件运行 SQL"></a>直接对文件运行 SQL</h3><p>除了使用读取 API 将文件加载到 DataFrame 并进行查询之外,您还可以直接使用 SQL 查询该文件.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">Dataset&lt;Row&gt; sqlDF =</span><br><span class="line">  spark.sql(<span class="string">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="保存模式"><a href="#保存模式" class="headerlink" title="保存模式"></a>保存模式</h3><p>保存操作可以选择采用SaveMode, 指定如何处理现有数据(如果存在).<br>重要的是要认识到这些保存模式不使用任何锁定并且不是原子的.<br>此外,执行Overwrite时,数据将在写出新数据之前被删除.</p>
<img src="/images/fly1366.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="保存到持久表"><a href="#保存到持久表" class="headerlink" title="保存到持久表"></a>保存到持久表</h3><p>DataFrames也可以使用saveAsTable 命令将其作为持久表保存到 Hive Metastore 中.<br>请注意,使用此功能不需要现有的 Hive 部署.<br>Spark 将为您创建默认的本地 Hive 元存储(使用 Derby).<br>与createOrReplaceTempView命令不同, saveAsTable将具体化 DataFrame 的内容并创建指向 Hive 元存储中数据的指针.<br>即使在您的 Spark 程序重新启动后,只要您保持与同一元存储的连接,持久表仍将存在.<br>table可以通过使用表的名称调用 上的方法来创建持久表的 DataFrame SparkSession.</p>
<p>对于基于文件的数据源,例如 text/parquet/json 等,您可以通过 path选项指定自定义表路径,例如<code>df.write.option(&quot;path&quot;, &quot;/some/path&quot;).saveAsTable(&quot;t&quot;)</code>.删除表时,自定义的表路径不会被删除,表数据还在.<br>如果不指定自定义表路径,Spark会将数据写入仓库目录下的默认表路径.<br>当表被删除时,默认的表路径也将被删除.</p>
<p>从 Spark 2.1 开始,持久数据源表将每个分区的元数据存储在 Hive 元存储中.<br>这带来了几个好处:</p>
<ol>
<li>由于 Metastore 可以仅为查询返回必要的分区,因此不再需要在对表的第一个查询中发现所有分区.</li>
<li>Hive DDL <code>ALTER TABLE PARTITION ... SET LOCATION</code>现在可用于使用数据源 API 创建的表.</li>
</ol>
<p>请注意,在创建外部数据源表(具有path选项的表)时,默认情况下不会收集分区信息.<br>要同步 Metastore 中的分区信息,您可以调用<code>MSCK REPAIR TABLE</code>.</p>
<h3 id="分桶-排序和分区"><a href="#分桶-排序和分区" class="headerlink" title="分桶/排序和分区"></a>分桶/排序和分区</h3><p>对于基于文件的数据源,还可以对输出进行分桶和排序或分区.<br>分桶和排序仅适用于持久表:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala</span></span><br><span class="line">peopleDF.write.bucketBy(<span class="number">42</span>, <span class="string">&quot;name&quot;</span>).sortBy(<span class="string">&quot;age&quot;</span>).saveAsTable(<span class="string">&quot;people_bucketed&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">peopleDF.write().bucketBy(<span class="number">42</span>, <span class="string">&quot;name&quot;</span>).sortBy(<span class="string">&quot;age&quot;</span>).saveAsTable(<span class="string">&quot;people_bucketed&quot;</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> users_bucketed_by_name(</span><br><span class="line">  name STRING,</span><br><span class="line">  favorite_color STRING,</span><br><span class="line">  favorite_numbers <span class="keyword">array</span><span class="operator">&lt;</span><span class="type">integer</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">USING</span> parquet</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(name) <span class="keyword">INTO</span> <span class="number">42</span> BUCKETS;</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<p>而分区可以与两者一起使用,也可以save在saveAsTable使用数据集 API 时使用.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala</span></span><br><span class="line">usersDF.write.partitionBy(<span class="string">&quot;favorite_color&quot;</span>).format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesPartByColor.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">usersDF</span><br><span class="line">  .write()</span><br><span class="line">  .partitionBy(<span class="string">&quot;favorite_color&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;parquet&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;namesPartByColor.parquet&quot;</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> users_by_favorite_color(</span><br><span class="line">  name STRING,</span><br><span class="line">  favorite_color STRING,</span><br><span class="line">  favorite_numbers <span class="keyword">array</span><span class="operator">&lt;</span><span class="type">integer</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">USING</span> csv PARTITIONED <span class="keyword">BY</span>(favorite_color);</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<p>可以对单个表同时使用分区和分桶:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">usersDF</span><br><span class="line">  .write</span><br><span class="line">  .partitionBy(<span class="string">&quot;favorite_color&quot;</span>)</span><br><span class="line">  .bucketBy(<span class="number">42</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line">  .saveAsTable(<span class="string">&quot;users_partitioned_bucketed&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">peopleDF</span><br><span class="line">  .write()</span><br><span class="line">  .partitionBy(<span class="string">&quot;favorite_color&quot;</span>)</span><br><span class="line">  .bucketBy(<span class="number">42</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line">  .saveAsTable(<span class="string">&quot;people_partitioned_bucketed&quot;</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> users_bucketed_and_partitioned(</span><br><span class="line">  name STRING,</span><br><span class="line">  favorite_color STRING,</span><br><span class="line">  favorite_numbers <span class="keyword">array</span><span class="operator">&lt;</span><span class="type">integer</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">USING</span> parquet</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (favorite_color)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(name) SORTED <span class="keyword">BY</span> (favorite_numbers) <span class="keyword">INTO</span> <span class="number">42</span> BUCKETS;</span><br></pre></td></tr></table></figure>
<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<p>partitionBy创建分区发现部分中描述的目录结构.<br>因此,它对具有高基数的列的适用性有限.<br>相反 bucketBy,将数据分布在固定数量的桶中,并且可以在多个唯一值不受限制时使用.</p>
<h2 id="Parquet-Files"><a href="#Parquet-Files" class="headerlink" title="Parquet Files"></a>Parquet Files</h2><p>Parquet是一种columnar 格式,受到许多其他数据处理系统的支持.<br>Spark SQL 支持读取和写入自动保留原始数据模式的 Parquet 文件.<br>编写 Parquet 文件时,出于兼容性原因,所有列都会自动转换为可为空.</p>
<h3 id="以编程方式加载数据"><a href="#以编程方式加载数据" class="headerlink" title="以编程方式加载数据"></a>以编程方式加载数据</h3><p>使用上述示例中的数据:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></span><br><span class="line">peopleDF.write.parquet(<span class="string">&quot;people.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the parquet file created above</span></span><br><span class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></span><br><span class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame</span></span><br><span class="line"><span class="keyword">val</span> parquetFileDF = spark.read.parquet(<span class="string">&quot;people.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></span><br><span class="line">parquetFileDF.createOrReplaceTempView(<span class="string">&quot;parquetFile&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> namesDF = spark.sql(<span class="string">&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;</span>)</span><br><span class="line">namesDF.map(attributes =&gt; <span class="string">&quot;Name: &quot;</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; peopleDF = spark.read().json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></span><br><span class="line">peopleDF.write().parquet(<span class="string">&quot;people.parquet&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the Parquet file created above.</span></span><br><span class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></span><br><span class="line"><span class="comment">// The result of loading a parquet file is also a DataFrame</span></span><br><span class="line">Dataset&lt;Row&gt; parquetFileDF = spark.read().parquet(<span class="string">&quot;people.parquet&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></span><br><span class="line">parquetFileDF.createOrReplaceTempView(<span class="string">&quot;parquetFile&quot;</span>);</span><br><span class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;</span>);</span><br><span class="line">Dataset&lt;String&gt; namesDS = namesDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">&quot;Name: &quot;</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    Encoders.STRING());</span><br><span class="line">namesDS.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> parquetTable</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.parquet</span><br><span class="line">OPTIONS (</span><br><span class="line">  path &quot;examples/src/main/resources/people.parquet&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> parquetTable</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="分区发现"><a href="#分区发现" class="headerlink" title="分区发现"></a>分区发现</h3><p>表分区是 Hive 等系统中常用的优化方法.<br>在分区表中,数据通常存储在不同的目录中,分区列值编码在每个分区目录的路径中.<br>所有内置文件源(包括 Text/CSV/JSON/ORC/Parquet)都能够自动发现和推断分区信息.<br>例如,我们可以使用以下目录结构将我们之前使用的所有人口数据存储到分区表中,具有两个额外的列,gender并country作为分区列:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender&#x3D;male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country&#x3D;US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country&#x3D;CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender&#x3D;female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country&#x3D;US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country&#x3D;CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure>

<p>通过传递path/to/table给SparkSession.read.parquet/SparkSession.read.load,Spark SQL 将自动从路径中提取分区信息.<br>现在返回的 DataFrame 的架构变为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable &#x3D; true)</span><br><span class="line">|-- age: long (nullable &#x3D; true)</span><br><span class="line">|-- gender: string (nullable &#x3D; true)</span><br><span class="line">|-- country: string (nullable &#x3D; true)</span><br></pre></td></tr></table></figure>

<p>请注意,分区列的数据类型是自动推断的.<br>目前支持numeric data types/date/timestamp/string type类型.<br>有时用户可能不想自动推断分区列的数据类型.<br>对于这些用例,自动类型推断可以通过配置 <code>spark.sql.sources.partitionColumnTypeInference.enabled</code>,默认为true. 禁用类型推断时,字符串类型将用于分区列.</p>
<p>从 Spark 1.6.0 开始,分区发现默认只查找给定路径下的分区.<br>对于上面的示例,如果用户传递path/to/table/gender=male给 SparkSession.read.parquet/SparkSession.read.load,gender则不会被视为分区列.<br>如果用户需要指定分区发现的起始路径,可以在数据源选项中设置basePath.<br>例如,当path/to/table/gender=male,basePath路径设置为path/to/table/时,gender将是一个分区列.</p>
<h3 id="架构合并"><a href="#架构合并" class="headerlink" title="架构合并"></a>架构合并</h3><p>与 Protocol Buffer/Avro/Thrift 一样,Parquet 也支持模式演化.<br>用户可以从一个简单的模式开始,然后根据需要逐渐向模式添加更多列.<br>这样,用户最终可能会得到多个具有不同但相互兼容的架构的 Parquet 文件.<br>Parquet 数据源现在能够自动检测这种情况并合并所有这些文件的模式.</p>
<p>由于模式合并是一个相对昂贵的操作,并且在大多数情况下不是必需的,我们从 1.5.0 开始默认关闭它.<br>您可以通过以下方式启用它.</p>
<ol>
<li>mergeSchema在读取 Parquet 文件时设置数据源选项true(如下例所示),或</li>
<li>将全局 SQL 选项设置<code>spark.sql.parquet.mergeSchema</code>为true.</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala</span></span><br><span class="line"><span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a simple DataFrame, store into a partition directory</span></span><br><span class="line"><span class="keyword">val</span> squaresDF = spark.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * i)).toDF(<span class="string">&quot;value&quot;</span>, <span class="string">&quot;square&quot;</span>)</span><br><span class="line">squaresDF.write.parquet(<span class="string">&quot;data/test_table/key=1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment">// adding a new column and dropping an existing column</span></span><br><span class="line"><span class="keyword">val</span> cubesDF = spark.sparkContext.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * i * i)).toDF(<span class="string">&quot;value&quot;</span>, <span class="string">&quot;cube&quot;</span>)</span><br><span class="line">cubesDF.write.parquet(<span class="string">&quot;data/test_table/key=2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read the partitioned table</span></span><br><span class="line"><span class="keyword">val</span> mergedDF = spark.read.option(<span class="string">&quot;mergeSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).parquet(<span class="string">&quot;data/test_table&quot;</span>)</span><br><span class="line">mergedDF.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- value: int (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- square: int (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- cube: int (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- key: int (nullable = true)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> square;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Getters and setters...</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Cube</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> cube;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Getters and setters...</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Square&gt; squares = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">1</span>; value &lt;= <span class="number">5</span>; value++) &#123;</span><br><span class="line">  Square square = <span class="keyword">new</span> Square();</span><br><span class="line">  square.setValue(value);</span><br><span class="line">  square.setSquare(value * value);</span><br><span class="line">  squares.add(square);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a simple DataFrame, store into a partition directory</span></span><br><span class="line">Dataset&lt;Row&gt; squaresDF = spark.createDataFrame(squares, Square.class);</span><br><span class="line">squaresDF.write().parquet(<span class="string">&quot;data/test_table/key=1&quot;</span>);</span><br><span class="line"></span><br><span class="line">List&lt;Cube&gt; cubes = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">6</span>; value &lt;= <span class="number">10</span>; value++) &#123;</span><br><span class="line">  Cube cube = <span class="keyword">new</span> Cube();</span><br><span class="line">  cube.setValue(value);</span><br><span class="line">  cube.setCube(value * value * value);</span><br><span class="line">  cubes.add(cube);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment">// adding a new column and dropping an existing column</span></span><br><span class="line">Dataset&lt;Row&gt; cubesDF = spark.createDataFrame(cubes, Cube.class);</span><br><span class="line">cubesDF.write().parquet(<span class="string">&quot;data/test_table/key=2&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read the partitioned table</span></span><br><span class="line">Dataset&lt;Row&gt; mergedDF = spark.read().option(<span class="string">&quot;mergeSchema&quot;</span>, <span class="keyword">true</span>).parquet(<span class="string">&quot;data/test_table&quot;</span>);</span><br><span class="line">mergedDF.printSchema();</span><br><span class="line"></span><br><span class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- value: int (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- square: int (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- cube: int (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- key: int (nullable = true)</span></span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="Hive-Metastore-Parquet-表转换"><a href="#Hive-Metastore-Parquet-表转换" class="headerlink" title="Hive Metastore Parquet 表转换"></a>Hive Metastore Parquet 表转换</h3><p>当读取和写入 Hive metastore Parquet 表时,Spark SQL 将尝试使用其自己的 Parquet 支持而不是 Hive SerDe 以获得更好的性能.<br>此行为由 <code>spark.sql.hive.convertMetastoreParquet</code>配置控制,默认情况下处于启用状态.</p>
<h4 id="Hive-Parquet-架构协调"><a href="#Hive-Parquet-架构协调" class="headerlink" title="Hive/Parquet 架构协调"></a>Hive/Parquet 架构协调</h4><p>从表模式处理的角度来看,Hive/Parquet 之间有两个关键的区别.</p>
<ol>
<li>Hive 是不区分大小写的,而 Parquet 不是</li>
<li>Hive 认为所有列都可以为空,而 Parquet 中的可空性很重要</li>
</ol>
<p>由于这个原因,在将 Hive 元存储 Parquet 表转换为 Spark SQL Parquet 表时,我们必须协调 Hive 元存储模式与 Parquet 模式.<br>调和规则是:</p>
<ol>
<li>无论是否为空,在两个模式中具有相同名称的字段必须具有相同的数据类型.<br>协调后的字段应具有 Parquet 端的数据类型,以便为空性得到尊重.</li>
<li>协调后的模式恰好包含 Hive 元存储模式中定义的那些字段.<br>任何仅出现在 Parquet 架构中的字段都将在协调后的架构中删除.<br>任何仅出现在 Hive 元存储架构中的字段都将作为可空字段添加到协调架构中.</li>
</ol>
<h4 id="元数据刷新"><a href="#元数据刷新" class="headerlink" title="元数据刷新"></a>元数据刷新</h4><p>Spark SQL 缓存 Parquet 元数据以获得更好的性能.<br>当启用 Hive metastore Parquet 表转换时,这些转换后的表的元数据也会被缓存.<br>如果这些表是由 Hive 或其他外部工具更新的,您需要手动刷新它们以确保元数据一致.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala</span></span><br><span class="line"><span class="comment">// spark is an existing SparkSession</span></span><br><span class="line">spark.catalog.refreshTable(<span class="string">&quot;my_table&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="comment">// spark is an existing SparkSession</span></span><br><span class="line">spark.catalog().refreshTable(<span class="string">&quot;my_table&quot;</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line">REFRESH <span class="keyword">TABLE</span> my_table;</span><br></pre></td></tr></table></figure>

<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>可以使用SparkSession的setConf方法或使用 SQL运行SET key=value命令来完成 Parquet 的配置.</p>
<p>spark.sql.parquet.binaryAsString<br>false<br>其他一些 Parquet 生成系统,特别是 Impala/Hive 和旧版本的 Spark SQL,在写出 Parquet 模式时不区分二进制数据和字符串.此标志告诉 Spark SQL 将二进制数据解释为字符串以提供与这些系统的兼容性.</p>
<p>spark.sql.parquet.int96AsTimestamp<br>true<br>一些 Parquet 生成系统,特别是 Impala/Hive,将时间戳存储到 INT96 中.此标志告诉 Spark SQL 将 INT96 数据解释为时间戳,以提供与这些系统的兼容性.</p>
<p>spark.sql.parquet.compression.codec<br>snappy<br>设置写入 Parquet 文件时使用的压缩编解码器.如果在特定于表的选项/属性中指定了&quot;compression&quot;或&quot;parquet.compression&quot;,则优先级为&quot;compression&quot;/&quot;parquet.compression&quot;/&quot;spark.sql.parquet.compression.codec&quot;.<br>可接受的值包括:none/uncompressed/snappy/gzip/lzo/brotli/lz4/zstd.<br>注意<code>zstd</code>需要在Hadoop 2.9.0之前安装<code>ZStandardCodec</code>,<code>brotli</code>需要安装<code>BrotliCodec</code>.</p>
<p>spark.sql.parquet.filterPushdown<br>true<br>设置为 true 时启用 Parquet 过滤器下推优化.</p>
<p>spark.sql.hive.convertMetastoreParquet<br>true<br>当设置为 false 时,Spark SQL 将为parquet 表使用 Hive SerDe 而不是内置支持.</p>
<p>spark.sql.parquet.mergeSchema<br>false<br>当为 true 时,Parquet 数据源合并从所有数据文件收集的模式,否则从摘要文件或随机数据文件中选择模式(如果没有可用的摘要文件).</p>
<p>spark.sql.parquet.writeLegacyFormat<br>false<br>如果为 true,数据将以 Spark 1.4 及更早版本的方式写入.<br>例如,十进制值将以 Apache Parquet 的固定长度字节数组格式写入,Apache Hive/Apache Impala 等其他系统也使用这种格式.<br>如果为 false,将使用 Parquet 中较新的格式.<br>例如,小数将以基于 int 的格式编写.<br>如果 Parquet 输出旨在与不支持这种较新格式的系统一起使用,请设置为 true.</p>
<h2 id="ORC-Files"><a href="#ORC-Files" class="headerlink" title="ORC Files"></a>ORC Files</h2><p>从 Spark 2.3 开始,Spark 支持矢量化的 ORC 阅读器,并为 ORC 文件提供了一种新的 ORC 文件格式.<br>为此,新添加了以下配置.<br>向量化阅读器用于本地 ORC 表(例如,使用子句<code>USING ORC</code>创建的表)当<code>spark.sql.orc.impl</code> 设置为<code>native</code>和<code>spark.sql.orc.enableVectorizedReader</code>设置为true时.<br>对于 Hive ORC serde 表(例如,使用子句<code>USING HIVE OPTIONS (fileFormat &#39;ORC&#39;)</code>创建的表),当<code>spark.sql.hive.convertMetastoreOrc</code>也设置为true时使用矢量化读取器.</p>
<p>spark.sql.orc.impl<br>native<br>ORC 实现的名称.它可以是native/hive.<br>native表示基于 Apache ORC 1.4 构建的本机 ORC 支持.<br><code>hive</code> 表示 Hive 1.2.1 中的 ORC 库.</p>
<p>spark.sql.orc.enableVectorizedReader<br>true<br>在实现中启用矢量化 orc 解码native.如果false,则在实现中使用新的非矢量化 ORC 阅读器native.对于hive实现,这将被忽略.</p>
<h2 id="JSON文件"><a href="#JSON文件" class="headerlink" title="JSON文件"></a>JSON文件</h2><p>Spark SQL 可以自动推断 JSON 数据集的模式并将其加载为Dataset[Row]. 可以使用 SparkSession.read.json()/或<code>Dataset[String]</code> JSON 文件完成此转换.</p>
<p>请注意,作为json 文件提供的文件不是典型的 JSON 文件.<br>每行必须包含一个单独的/自包含的有效 JSON 对象.<br>有关详细信息,请参阅 JSON 行文本格式,也称为换行分隔的 JSON.</p>
<p>对于常规的多行 JSON 文件,将multiLine选项设置为true.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span></span><br><span class="line"><span class="comment">// supported by importing this when creating a Dataset.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">&quot;examples/src/main/resources/people.json&quot;</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |  name|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |Justin|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment">// a Dataset[String] storing one JSON object per string</span></span><br><span class="line"><span class="keyword">val</span> otherPeopleDataset = spark.createDataset(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot;</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="keyword">val</span> otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |        address|name|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></span><br><span class="line">Dataset&lt;Row&gt; people = spark.read().json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></span><br><span class="line">people.printSchema();</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">people.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span>);</span><br><span class="line">namesDF.show();</span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |  name|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |Justin|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment">// a Dataset&lt;String&gt; storing one JSON object per string.</span></span><br><span class="line">List&lt;String&gt; jsonData = Arrays.asList(</span><br><span class="line">        <span class="string">&quot;&#123;\&quot;name\&quot;:\&quot;Yin\&quot;,\&quot;address\&quot;:&#123;\&quot;city\&quot;:\&quot;Columbus\&quot;,\&quot;state\&quot;:\&quot;Ohio\&quot;&#125;&#125;&quot;</span>);</span><br><span class="line">Dataset&lt;String&gt; anotherPeopleDataset = spark.createDataset(jsonData, Encoders.STRING());</span><br><span class="line">Dataset&lt;Row&gt; anotherPeople = spark.read().json(anotherPeopleDataset);</span><br><span class="line">anotherPeople.show();</span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |        address|name|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> jsonTable</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.json</span><br><span class="line">OPTIONS (</span><br><span class="line">  path &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> jsonTable</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="Hive-Tables"><a href="#Hive-Tables" class="headerlink" title="Hive Tables"></a>Hive Tables</h2><p>Spark SQL 还支持读取和写入存储在Apache Hive中的数据.<br>但是,由于 Hive 有大量的依赖项,这些依赖项不包含在默认的 Spark 分发中.<br>如果可以在类路径中找到 Hive 依赖项,Spark 将自动加载它们.<br>请注意,这些 Hive 依赖项还必须存在于所有工作节点上,因为它们需要访问 Hive 序列化和反序列化库 (SerDes) 才能访问存储在 Hive 中的数据.</p>
<p>Hive 的配置是通过将hive-site.xml/core-site.xml(用于安全配置)/hdfs-site.xml(用于 HDFS 配置)文件放在conf/.</p>
<p>使用 Hive 时,必须SparkSession使用 Hive 支持进行实例化,包括与持久 Hive 元存储的连接/对 Hive serdes 的支持以及 Hive 用户定义的函数.<br>没有现有 Hive 部署的用户仍然可以启用 Hive 支持.<br>未配置时hive-site.xml,context自动在当前目录创建metastore_db,并创建一个配置的目录spark.sql.warehouse.dir,默认为Spark应用启动的当前目录下的目录spark-warehouse.<br>请注意,自 Spark 2.0.0 以来,不推荐使用中的hive.metastore.warehouse.dir属性.<br>hive-site.xml相反,用于spark.sql.warehouse.dir指定数据库在仓库中的默认位置.<br>您可能需要向启动 Spark 应用程序的用户授予写权限.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;spark-warehouse&quot;</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark Hive Example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>)</span><br><span class="line">sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM src&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">sql(<span class="string">&quot;SELECT COUNT(*) FROM src&quot;</span>).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s&quot;Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line"><span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s&quot;val_<span class="subst">$i</span>&quot;</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">&quot;records&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span></span><br><span class="line"><span class="comment">// `USING hive`</span></span><br><span class="line">sql(<span class="string">&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;</span>)</span><br><span class="line"><span class="comment">// Save DataFrame to the Hive managed table</span></span><br><span class="line"><span class="keyword">val</span> df = spark.table(<span class="string">&quot;src&quot;</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">&quot;hive_records&quot;</span>)</span><br><span class="line"><span class="comment">// After insertion, the Hive managed table has data now</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM hive_records&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Prepare a Parquet data directory</span></span><br><span class="line"><span class="keyword">val</span> dataDir = <span class="string">&quot;/tmp/parquet_data&quot;</span></span><br><span class="line">spark.range(<span class="number">10</span>).write.parquet(dataDir)</span><br><span class="line"><span class="comment">// Create a Hive external Parquet table</span></span><br><span class="line">sql(<span class="string">s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;<span class="subst">$dataDir</span>&#x27;&quot;</span>)</span><br><span class="line"><span class="comment">// The Hive external table should already have data</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM hive_bigints&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+</span></span><br><span class="line"><span class="comment">// | id|</span></span><br><span class="line"><span class="comment">// +---+</span></span><br><span class="line"><span class="comment">// |  0|</span></span><br><span class="line"><span class="comment">// |  1|</span></span><br><span class="line"><span class="comment">// |  2|</span></span><br><span class="line"><span class="comment">// ... Order may vary, as spark processes the partitions in parallel.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Turn on flag for Hive Dynamic Partitioning</span></span><br><span class="line">spark.sqlContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">spark.sqlContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>)</span><br><span class="line"><span class="comment">// Create a Hive partitioned table using DataFrame API</span></span><br><span class="line">df.write.partitionBy(<span class="string">&quot;key&quot;</span>).format(<span class="string">&quot;hive&quot;</span>).saveAsTable(<span class="string">&quot;hive_part_tbl&quot;</span>)</span><br><span class="line"><span class="comment">// Partitioned column `key` will be moved to the end of the schema.</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM hive_part_tbl&quot;</span>).show()</span><br><span class="line"><span class="comment">// +-------+---+</span></span><br><span class="line"><span class="comment">// |  value|key|</span></span><br><span class="line"><span class="comment">// +-------+---+</span></span><br><span class="line"><span class="comment">// |val_238|238|</span></span><br><span class="line"><span class="comment">// | val_86| 86|</span></span><br><span class="line"><span class="comment">// |val_311|311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> key;</span><br><span class="line">  <span class="keyword">private</span> String value;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKey</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> key;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setKey</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.key = key;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValue</span><span class="params">(String value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.value = value;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></span><br><span class="line">String warehouseLocation = <span class="keyword">new</span> File(<span class="string">&quot;spark-warehouse&quot;</span>).getAbsolutePath();</span><br><span class="line">SparkSession spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Java Spark Hive Example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate();</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>);</span><br><span class="line">spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM src&quot;</span>).show();</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT COUNT(*) FROM src&quot;</span>).show();</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span></span><br><span class="line">Dataset&lt;String&gt; stringsDS = sqlDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">&quot;Key: &quot;</span> + row.get(<span class="number">0</span>) + <span class="string">&quot;, Value: &quot;</span> + row.get(<span class="number">1</span>),</span><br><span class="line">    Encoders.STRING());</span><br><span class="line">stringsDS.show();</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line">List&lt;Record&gt; records = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> key = <span class="number">1</span>; key &lt; <span class="number">100</span>; key++) &#123;</span><br><span class="line">  Record record = <span class="keyword">new</span> Record();</span><br><span class="line">  record.setKey(key);</span><br><span class="line">  record.setValue(<span class="string">&quot;val_&quot;</span> + key);</span><br><span class="line">  records.add(record);</span><br><span class="line">&#125;</span><br><span class="line">Dataset&lt;Row&gt; recordsDF = spark.createDataFrame(records, Record.class);</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">&quot;records&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrames data with data stored in Hive.</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span>).show();</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala&quot;中找到完整的示例代码.</p>
<h3 id="指定-Hive-表的存储格式"><a href="#指定-Hive-表的存储格式" class="headerlink" title="指定 Hive 表的存储格式"></a>指定 Hive 表的存储格式</h3><p>当你创建一个 Hive 表时,你需要定义这个表应该如何从文件系统读取/写入数据,即&quot;输入格式&quot;和&quot;输出格式&quot;.<br>您还需要定义该表应如何将数据反序列化为行,或将行序列化为数据,即&quot;serde&quot;.<br>以下选项可用于指定存储格式(&quot;serde&quot;/&quot;输入格式&quot;/&quot;输出格式&quot;),例如CREATE TABLE src(id int) USING hive OPTIONS(fileFormat &#39;parquet&#39;). 默认情况下,我们将以纯文本形式读取表文件.<br>注意,建表时暂不支持Hive storage handler,可以在Hive端使用storage handler建表,使用Spark SQL读取.</p>
<img src="/images/fly1367.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>用OPTIONS 定义的所有其他属性将被视为 Hive serde 属性.</p>
<h3 id="与不同版本的-Hive-Metastore-交互"><a href="#与不同版本的-Hive-Metastore-交互" class="headerlink" title="与不同版本的 Hive Metastore 交互"></a>与不同版本的 Hive Metastore 交互</h3><p>Spark SQL 的 Hive 支持最重要的部分之一是与 Hive metastore 的交互,它使 Spark SQL 能够访问 Hive 表的元数据.<br>从 Spark 1.4.0 开始,Spark SQL 的单个二进制构建可用于查询不同版本的 Hive 元存储,使用下面描述的配置.<br>请注意,独立于用于与 Metastore 对话的 Hive 版本,内部 Spark SQL 将针对 Hive 1.2.1 进行编译,并将这些类用于内部执行(serdes/UDF/UDAF 等).</p>
<p>以下选项可用于配置用于检索元数据的 Hive 版本:</p>
<p>spark.sql.hive.metastore.version<br>1.2.1<br>Hive 元存储的版本.可用的选项是0.12.0~2.3.3.</p>
<p>spark.sql.hive.metastore.jars<br>builtin<br>应该用于实例化 HiveMetastoreClient 的 jar 的位置.此属性可以是以下三个选项之一:</p>
<ul>
<li>builtin: 使用 Hive 1.2.1,它在-Phive启用时与 Spark 程序集捆绑在一起. 选择此选项时,spark.sql.hive.metastore.version必须1.2.1定义或不定义.</li>
<li>maven: 使用从 Maven 存储库下载的指定版本的 Hive jar.通常不建议将此配置用于生产部署.</li>
<li>JVM 标准格式的类路径. 类路径必须包含所有 Hive 及其依赖项,包括正确版本的 Hadoop.这些 jar 只需要存在于驱动程序中,但如果您在 yarn 集群模式下运行,则必须确保它们与您的应用程序打包在一起.</li>
</ul>
<p>spark.sql.hive.metastore.sharedPrefixes<br>com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc<br>应使用在 Spark SQL 和特定版本的 Hive 之间共享的类加载器加载的类前缀的逗号分隔列表.应该共享的类的一个示例是与 Metastore 对话所需的 JDBC 驱动程序.其他需要共享的类是那些与已经共享的类交互的类.例如,log4j 使用的自定义附加程序.</p>
<p>spark.sql.hive.metastore.barrierPrefixes<br>(empty)<br>一个逗号分隔的类前缀列表,应该为 Spark SQL 与之通信的每个 Hive 版本显式重新加载.例如,在通常会共享的前缀中声明的 Hive UDF(即<code>org.apache.spark.*</code>).</p>
<h2 id="JDBC-到其他数据库"><a href="#JDBC-到其他数据库" class="headerlink" title="JDBC 到其他数据库"></a>JDBC 到其他数据库</h2><p>Spark SQL 还包含一个数据源,可以使用 JDBC 从其他数据库读取数据.<br>应该优先使用此功能而不是使用JdbcRDD.<br>这是因为结果以 DataFrame 的形式返回,它们可以很容易地在 Spark SQL 中进行处理或与其他数据源连接.<br>JDBC 数据源也更易于从 Java 或 Python 使用,因为它不需要用户提供 ClassTag.<br>(请注意,这与 Spark SQL JDBC 服务器不同,后者允许其他应用程序使用 Spark SQL 运行查询).</p>
<p>要开始,您需要在 spark 类路径中包含特定数据库的 JDBC 驱动程序.<br>例如,要从 Spark Shell 连接到 postgres,您可以运行以下命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</span><br></pre></td></tr></table></figure>

<p>远程数据库中的表可以使用 Data Sources API 作为 DataFrame 或 Spark SQL 临时视图加载.<br>用户可以在数据源选项中指定 JDBC 连接属性.<br>通常作为登录数据源的连接属性提供user.<br>password除了连接属性,Spark 还支持以下不区分大小写的选项:</p>
<img src="/images/fly1368.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1369.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1370.png" style="margin-left: 0px; padding-bottom: 10px;">
<img src="/images/fly1371.png" style="margin-left: 0px; padding-bottom: 10px;">

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></span><br><span class="line"><span class="comment">// Loading data from a JDBC source</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">connectionProperties.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">connectionProperties.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> jdbcDF2 = spark.read</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties)</span><br><span class="line"><span class="comment">// Specifying the custom data types of the read schema</span></span><br><span class="line">connectionProperties.put(<span class="string">&quot;customSchema&quot;</span>, <span class="string">&quot;id DECIMAL(38, 0), name STRING&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> jdbcDF3 = spark.read</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Saving data to a JDBC source</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Specifying create table column data types on write</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">  .option(<span class="string">&quot;createTableColumnTypes&quot;</span>, <span class="string">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span>)</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></span><br><span class="line"><span class="comment">// Loading data from a JDBC source</span></span><br><span class="line">Dataset&lt;Row&gt; jdbcDF = spark.read()</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .load();</span><br><span class="line"></span><br><span class="line">Properties connectionProperties = <span class="keyword">new</span> Properties();</span><br><span class="line">connectionProperties.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>);</span><br><span class="line">connectionProperties.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>);</span><br><span class="line">Dataset&lt;Row&gt; jdbcDF2 = spark.read()</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Saving data to a JDBC source</span></span><br><span class="line">jdbcDF.write()</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .save();</span><br><span class="line"></span><br><span class="line">jdbcDF2.write()</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Specifying create table column data types on write</span></span><br><span class="line">jdbcDF.write()</span><br><span class="line">  .option(<span class="string">&quot;createTableColumnTypes&quot;</span>, <span class="string">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span>)</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>, connectionProperties);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--sql</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> jdbcTable</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:postgresql:dbserver&quot;,</span><br><span class="line">  dbtable &quot;schema.tablename&quot;,</span><br><span class="line">  <span class="keyword">user</span> <span class="string">&#x27;username&#x27;</span>,</span><br><span class="line">  password <span class="string">&#x27;password&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> jdbcTable</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> resultTable</span><br></pre></td></tr></table></figure>

<p>在 Spark 存储库的&quot;examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala&quot;中找到完整的示例代码.</p>
<h2 id="Avro-Files"><a href="#Avro-Files" class="headerlink" title="Avro Files"></a>Avro Files</h2><p>自 Spark 2.4 发布以来,Spark SQL提供了对读取和写入 Apache Avro 数据的内置支持.</p>
<h3 id="部署中"><a href="#部署中" class="headerlink" title="部署中"></a>部署中</h3><p>该spark-avro模块是外部的,不包含在默认情况下spark-submit/spark-shell.</p>
<p>与任何 Spark 应用程序一样,spark-submit用于启动您的应用程序.<br>spark-avro_2.12 并且它的依赖可以直接添加到spark-submitusing 中--packages,比如,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-submit --packages org.apache.spark:spark-avro_2.12:2.4.8 ...</span><br></pre></td></tr></table></figure>

<p>为了在spark-shell上进行实验,您还可以直接使用--packages添加org.apache.spark:spark-avro_2.12及其依赖项,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-shell --packages org.apache.spark:spark-avro_2.12:2.4.8 ...</span><br></pre></td></tr></table></figure>

<p>有关提交具有外部依赖项的应用程序的更多详细信息,请参阅应用程序提交指南.</p>
<h3 id="加载和保存功能"><a href="#加载和保存功能" class="headerlink" title="加载和保存功能"></a>加载和保存功能</h3><p>要以 Avro 格式加载/保存数据,您需要将数据源选项指定format为avro(或<code>org.apache.spark.sql.avro</code>).</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> usersDF = spark.read.format(<span class="string">&quot;avro&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/users.avro&quot;</span>)</span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.format(<span class="string">&quot;avro&quot;</span>).save(<span class="string">&quot;namesAndFavColors.avro&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line">Dataset&lt;Row&gt; usersDF = spark.read().format(<span class="string">&quot;avro&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/users.avro&quot;</span>);</span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write().format(<span class="string">&quot;avro&quot;</span>).save(<span class="string">&quot;namesAndFavColors.avro&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="to-avro-from-avro"><a href="#to-avro-from-avro" class="headerlink" title="to_avro()/from_avro()"></a>to_avro()/from_avro()</h3><p>Avro 包提供to_avro了将列编码为 Avro 格式的二进制数据以及from_avro()将 Avro 二进制数据解码为列的功能.<br>这两个函数都将一列转换为另一列,输入/输出 SQL 数据类型可以是复杂类型或原始类型.</p>
<p>在读取或写入 Kafka 等流式源时,使用 Avro 记录作为列非常有用.<br>每个 Kafka 键值记录都将增加一些元数据,例如 Kafka 中的摄取时间戳/Kafka 中的偏移量等.</p>
<ol>
<li>如果包含您的数据的&quot;值&quot;字段在 Avro 中,您可以使用from_avro()它来提取数据/丰富数据/清理数据,然后再次将其推向下游到 Kafka 或将其写出到文件中.</li>
<li>to_avro()可用于将结构转换为 Avro 记录.<br>当您希望在将数据写入 Kafka 时将多个列重新编码为单个列时,此方法特别有用.</li>
</ol>
<p>这两个函数目前仅在 Scala/Java 中可用.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.avro._</span><br><span class="line"></span><br><span class="line"><span class="comment">// `from_avro` requires Avro schema in JSON string format.</span></span><br><span class="line"><span class="keyword">val</span> jsonFormatSchema = <span class="keyword">new</span> <span class="type">String</span>(<span class="type">Files</span>.readAllBytes(<span class="type">Paths</span>.get(<span class="string">&quot;./examples/src/main/resources/user.avsc&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark</span><br><span class="line">  .readStream</span><br><span class="line">  .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;topic1&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. Decode the Avro data into a struct;</span></span><br><span class="line"><span class="comment">// 2. Filter by column `favorite_color`;</span></span><br><span class="line"><span class="comment">// 3. Encode the column `name` in Avro format.</span></span><br><span class="line"><span class="keyword">val</span> output = df</span><br><span class="line">  .select(from_avro(<span class="symbol">&#x27;value</span>, jsonFormatSchema) as <span class="symbol">&#x27;user</span>)</span><br><span class="line">  .where(<span class="string">&quot;user.favorite_color == \&quot;red\&quot;&quot;</span>)</span><br><span class="line">  .select(to_avro($<span class="string">&quot;user.name&quot;</span>) as <span class="symbol">&#x27;value</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> query = output</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;topic&quot;</span>, <span class="string">&quot;topic2&quot;</span>)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//java</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.avro.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">// `from_avro` requires Avro schema in JSON string format.</span></span><br><span class="line">String jsonFormatSchema = <span class="keyword">new</span> String(Files.readAllBytes(Paths.get(<span class="string">&quot;./examples/src/main/resources/user.avsc&quot;</span>)));</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark</span><br><span class="line">  .readStream()</span><br><span class="line">  .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;topic1&quot;</span>)</span><br><span class="line">  .load();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. Decode the Avro data into a struct;</span></span><br><span class="line"><span class="comment">// 2. Filter by column `favorite_color`;</span></span><br><span class="line"><span class="comment">// 3. Encode the column `name` in Avro format.</span></span><br><span class="line">Dataset&lt;Row&gt; output = df</span><br><span class="line">  .select(from_avro(col(<span class="string">&quot;value&quot;</span>), jsonFormatSchema).as(<span class="string">&quot;user&quot;</span>))</span><br><span class="line">  .where(<span class="string">&quot;user.favorite_color == \&quot;red\&quot;&quot;</span>)</span><br><span class="line">  .select(to_avro(col(<span class="string">&quot;user.name&quot;</span>)).as(<span class="string">&quot;value&quot;</span>));</span><br><span class="line"></span><br><span class="line">StreamingQuery query = output</span><br><span class="line">  .writeStream()</span><br><span class="line">  .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;topic&quot;</span>, <span class="string">&quot;topic2&quot;</span>)</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure>

<h3 id="数据源选项"><a href="#数据源选项" class="headerlink" title="数据源选项"></a>数据源选项</h3><p>Avro 的数据源选项可以使用DataFrameReader/DataFrameWriter的.option的方法设置.</p>
<img src="/images/fly1372.png" style="margin-left: 0px; padding-bottom: 10px;">

<h3 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h3><p>可以使用SparkSession 上的setConf方法或使用 SQL 运行SET key=value命令来完成 Avro 的配置.</p>
<p>spark.sql.legacy.replaceDatabricksSparkAvro.enabled<br>true<br>如果设置为 true,则数据源提供程序<code>com.databricks.spark.avro</code>将映射到内置但外部的 Avro 数据源模块以实现向后兼容.</p>
<p>spark.sql.avro.compression.codec<br>snappy<br>用于编写 AVRO 文件的压缩编解码器.支持的编解码器:uncompressed/deflate/snappy/bzip2/xz.默认编解码器snappy.</p>
<p>spark.sql.avro.deflate.level<br>-1<br>用于编写 AVRO 文件的 deflate 编解码器的压缩级别.有效值必须在 1 到 9(含)或 -1 的范围内.默认值为 -1,对应于当前实现中的 6 级.</p>
<h3 id="与-Databricks-spark-avro-的兼容性"><a href="#与-Databricks-spark-avro-的兼容性" class="headerlink" title="与 Databricks spark-avro 的兼容性"></a>与 Databricks spark-avro 的兼容性</h3><p>此 Avro 数据源模块最初来自 Databricks 的开源存储库 spark-avro并与之兼容.</p>
<p>默认情况下spark.sql.legacy.replaceDatabricksSparkAvro.enabled启用 SQL 配置,数据源提供程序com.databricks.spark.avro映射到这个内置的 Avro 模块.<br>对于使用目录元存储中的Provider属性创建的 Spark 表com.databricks.spark.avro,如果您使用此内置 Avro 模块,则映射对于加载这些表至关重要.</p>
<p>注意在 Databricks 的spark-avro中,隐式类 AvroDataFrameWriter是AvroDataFrameReader为快捷函数创建的.avro().<br>在这个内置但外部的模块中,两个隐式类都被删除了.<br>请使用.format(&quot;avro&quot;)in DataFrameWriter或DataFrameReaderinstead,它应该干净且足够好.</p>
<p>如果您更喜欢使用自己构建的spark-avrojar 文件,您可以简单地禁用配置 spark.sql.legacy.replaceDatabricksSparkAvro.enabled,并使用--jars部署应用程序的选项.<br>阅读申请提交指南中的高级依赖管理部分了解更多详细信息.</p>
<h3 id="Avro-支持的类型-gt-Spark-SQL-转换"><a href="#Avro-支持的类型-gt-Spark-SQL-转换" class="headerlink" title="Avro 支持的类型 -&gt; Spark SQL 转换"></a>Avro 支持的类型 -&gt; Spark SQL 转换</h3><p>目前Spark支持读取Avro记录下的所有原始类型和复杂类型.</p>
<img src="/images/fly1373.png" width="600" style="margin-left: 0px; padding-bottom: 10px;">

<p>除了上面列出的类型,它还支持阅读union类型.<br>以下三种类型被认为是基本union类型:</p>
<ol>
<li>union(int, long)将映射到 LongType.</li>
<li>union(float, double)将映射到 DoubleType.</li>
<li>union(something, null),其中 something 是任何受支持的 Avro 类型.<br>这将映射到与某物相同的 Spark SQL 类型,并将 nullable 设置为 true.<br>所有其他联合类型都被认为是复杂的.<br>它们将根据联合成员映射到其中字段名称为 member0/member1 等的 StructType.<br>这与在 Avro/Parquet 之间转换时的行为一致.</li>
</ol>
<p>它还支持读取以下 Avro逻辑类型:</p>
<img src="/images/fly1374.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>目前,它会忽略 Avro 文件中存在的文档/别名和其他属性.</p>
<h3 id="Spark-SQL-gt-Avro-转换支持的类型"><a href="#Spark-SQL-gt-Avro-转换支持的类型" class="headerlink" title="Spark SQL -&gt; Avro 转换支持的类型"></a>Spark SQL -&gt; Avro 转换支持的类型</h3><p>Spark 支持将所有 Spark SQL 类型写入 Avro.<br>对于大多数类型,从 Spark 类型到 Avro 类型的映射很简单(例如 IntegerType 被转换为 int).但是,下面列出了一些特殊情况:</p>
<img src="/images/fly1375.png" style="margin-left: 0px; padding-bottom: 10px;">

<p>您还可以使用选项指定整个输出 Avro 模式avroSchema,以便 Spark SQL 类型可以转换为其他 Avro 类型.<br>默认情况下不应用以下转换,需要用户指定 Avro 模式:</p>
<img src="/images/fly1376.png" style="margin-left: 0px; padding-bottom: 10px;">

<h2 id="故障排除"><a href="#故障排除" class="headerlink" title="故障排除"></a>故障排除</h2><p>JDBC 驱动程序类必须对客户端会话和所有执行程序上的原始类加载器可见.<br>这是因为 Java 的 DriverManager 类执行安全检查,导致它在打开连接时忽略所有对原始类加载器不可见的驱动程序.<br>一种方便的方法是修改所有工作节点上的 compute_classpath.sh 以包含您的驱动程序 JAR.</p>
<p>一些数据库,例如 H2,将所有名称转换为大写.<br>您需要使用大写字母来引用 Spark SQL 中的这些名称.</p>
<p>用户可以在数据源选项中指定特定于供应商的 JDBC 连接属性以进行特殊处理.<br>例如,spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, oracleJdbcUrl).option(&quot;oracle.jdbc.mapDateToTimestamp&quot;, &quot;false&quot;).<br>oracle.jdbc.mapDateToTimestamp默认为 true,用户通常需要禁用此标志以避免 Oracle 日期被解析为时间戳.</p>
<h1 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h1><p>对于某些工作负载,可以通过在内存中缓存数据或打开一些实验性选项来提高性能.</p>
<h2 id="在内存中缓存数据"><a href="#在内存中缓存数据" class="headerlink" title="在内存中缓存数据"></a>在内存中缓存数据</h2><p>Spark SQL 可以通过调用<font color="blue">spark.catalog.cacheTable(&quot;tableName&quot;)</font> / <font color="blue">dataFrame.cache()</font>使用内存中的列格式缓存表.<br>然后 Spark SQL 将只扫描需要的列,并自动调整压缩以最小化内存使用和 GC 压力.<br>您可以调用<code>spark.catalog.uncacheTable(&quot;tableName&quot;)</code>从内存中删除表.</p>
<p>可以使用SparkSession的setConf方法或使用 SQL运行<code>SET key=value</code>命令来完成内存缓存的配置.</p>
<p>spark.sql.inMemoryColumnarStorage.compressed<br>true<br>当设置为 true 时,Spark SQL 将根据数据的统计信息自动为每一列选择一个压缩编解码器.</p>
<p>spark.sql.inMemoryColumnarStorage.batchSize<br>10000<br>控制列缓存的批次大小.较大的批大小可以提高内存利用率和压缩,但在缓存数据时有 OOM 的风险.</p>
<h2 id="其他配置选项"><a href="#其他配置选项" class="headerlink" title="其他配置选项"></a>其他配置选项</h2><p>以下选项也可用于调整查询执行的性能.<br>随着自动执行更多优化,这些选项可能会在未来的版本中被弃用.</p>
<p>spark.sql.files.maxPartitionBytes<br>134217728 (128 MB)<br>读取文件时打包到单个分区中的最大字节数.</p>
<p>spark.sql.files.openCostInBytes<br>4194304 (4 MB)<br>打开文件的估计成本,以同时扫描的字节数衡量.将多个文件放入分区时使用.最好高估,小文件的分区会比大文件的分区(先调度)快.</p>
<p>spark.sql.broadcastTimeout<br>300<br>广播连接中广播等待时间的超时秒数</p>
<p>spark.sql.autoBroadcastJoinThreshold<br>10485760 (10 MB)<br>配置将在执行连接时广播到所有工作节点的表的最大大小(以字节为单位).通过将此值设置为 -1 可以禁用广播.请注意,目前仅支持 <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code>运行命令的 Hive Metastore 表的统计信息.</p>
<p>spark.sql.shuffle.partitions<br>200<br>配置在shuffling data for joins/aggregations时要使用的分区数.</p>
<p>spark.sql.sources.parallelPartitionDiscovery.threshold<br>32<br>配置阈值以启用作业输入路径的并行列表.如果输入路径的数量大于这个阈值,Spark 将使用 Spark 分布式作业列出文件.否则,它将回退到顺序上市.此配置仅在使用基于文件的数据源(例如 Parquet/ORC/JSON)时有效.</p>
<p>spark.sql.sources.parallelPartitionDiscovery.parallelism<br>10000<br>配置作业输入路径的最大列表并行度.如果输入路径的数量大于此值,则会限制使用此值.同上,该配置仅在使用Parquet/ORC/JSON等基于文件的数据源时有效.</p>
<h2 id="SQL-查询的广播提示"><a href="#SQL-查询的广播提示" class="headerlink" title="SQL 查询的广播提示"></a>SQL 查询的广播提示</h2><p>当BROADCAST将它们与另一个表或视图连接时,提示会引导 Spark 广播每个指定的表.<br>当 Spark 决定连接方法时,广播散列连接(即 BHJ)是首选,即使统计信息高于配置<code>spark.sql.autoBroadcastJoinThreshold</code>.<br>当指定连接的两侧时,Spark 广播具有较低统计信息的一侧.<br>注意 Spark 不保证始终选择 BHJ,因为并非所有情况(例如完全外连接)都支持 BHJ.</p>
<h1 id="分布式-SQL-引擎"><a href="#分布式-SQL-引擎" class="headerlink" title="分布式 SQL 引擎"></a>分布式 SQL 引擎</h1><p>Spark SQL 还可以使用其 JDBC/ODBC 或命令行界面充当分布式查询引擎.<br>在这种模式下,最终用户或应用程序可以直接与 Spark SQL 交互以运行 SQL 查询,而无需编写任何代码.</p>
<h2 id="运行-Thrift-JDBC-ODBC-服务器"><a href="#运行-Thrift-JDBC-ODBC-服务器" class="headerlink" title="运行 Thrift JDBC/ODBC 服务器"></a>运行 Thrift JDBC/ODBC 服务器</h2><p>这里实现的Thrift JDBC/ODBC server对应HiveServer2 于Hive 1.2.1中的.<br>您可以使用 Spark/Hive 1.2.1 附带的beeline 脚本测试 JDBC 服务器.</p>
<p>要启动 JDBC/ODBC 服务器,请在 Spark 目录中运行以下命令:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure>

<p>此脚本接受所有bin/spark-submit命令行选项,以及<font color="blue">--hiveconf</font>用于指定 Hive 属性的选项.<br>您可以运行<code>./sbin/start-thriftserver.sh --help</code>所有可用选项的完整列表.<br>默认情况下,服务器侦听 localhost:10000.<br>您可以通过任一环境变量覆盖此行为,即:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</span><br><span class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</span><br><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">  --master &lt;master-uri&gt; \</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>或系统属性:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</span><br><span class="line">  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</span><br><span class="line">  --master &lt;master-uri&gt;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>现在您可以使用beeline 测试 Thrift JDBC/ODBC 服务器:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/beeline</span><br></pre></td></tr></table></figure>

<p>直接连接到 JDBC/ODBC 服务器:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;localhost:10000</span><br></pre></td></tr></table></figure>

<p>Beeline 会要求您提供用户名和密码.<br>在非安全模式下,只需在您的计算机上输入用户名和空白密码即可.<br>对于安全模式,请按照 beeline 文档中给出的说明进行操作.</p>
<p>Hive 的配置是通过将您的hive-site.xml/hdfs-site.xml/core-site.xml文件放在conf/.<br>您也可以使用 Hive 附带的beeline 脚本.</p>
<p>Thrift JDBC 服务器还支持通过 HTTP 传输发送 Thrift RPC 消息.<br>使用以下设置启用 HTTP 模式作为系统属性或在hive-site.xml文件中conf/:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive.server2.transport.mode &#x2F;&#x2F;Set this to value: http</span><br><span class="line">hive.server2.thrift.http.port &#x2F;&#x2F;HTTP port number to listen on; default is 10001</span><br><span class="line">hive.server2.http.endpoint &#x2F;&#x2F;HTTP endpoint; default is cliservice</span><br></pre></td></tr></table></figure>

<p>要进行测试,请使用 beeline 以 http 模式连接到 JDBC/ODBC 服务器:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;&lt;host&gt;:&lt;port&gt;&#x2F;&lt;database&gt;?hive.server2.transport.mode&#x3D;http;hive.server2.thrift.http.path&#x3D;&lt;http_endpoint&gt;</span><br></pre></td></tr></table></figure>

<h2 id="运行-Spark-SQL-CLI"><a href="#运行-Spark-SQL-CLI" class="headerlink" title="运行 Spark SQL CLI"></a>运行 Spark SQL CLI</h2><p>Spark SQL CLI 是一个方便的工具,可以在本地模式下运行 Hive Metastore 服务并执行从命令行输入的查询.<br>请注意,Spark SQL CLI 无法与 Thrift JDBC 服务器通信.</p>
<p>要启动 Spark SQL CLI,请在 Spark 目录中运行以下命令:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure>

<p>Hive 的配置是通过将您的hive-site.xml/hdfs-site.xml/core-site.xml文件放在conf/.<br>您可以运行<code>./bin/spark-sql --help</code>所有可用选项的完整列表.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/23/spark-kafka%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97/" rel="prev" title="spark-kafka集成指南">
                  <i class="fa fa-chevron-left"></i> spark-kafka集成指南
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/30/hbase%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" rel="next" title="hbase配置参数优化">
                  hbase配置参数优化 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">maoeryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  





</body>
</html>
